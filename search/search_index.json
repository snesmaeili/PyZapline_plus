{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"PyZaplinePlus","text":"<p>Advanced Python library for automatic and adaptive removal of line noise from EEG data</p> <p> </p>"},{"location":"#what-is-pyzaplineplus","title":"What is PyZaplinePlus?","text":"<p>PyZaplinePlus is a Python adaptation of the Zapline-plus library, designed to automatically remove spectral peaks like line noise from EEG data while preserving the integrity of the non-noise spectrum and maintaining the data rank. </p> <p>Unlike traditional notch filters that can distort your data, PyZaplinePlus uses sophisticated spectral detection and Denoising Source Separation (DSS) to identify and remove line noise components adaptively, providing clean EEG signals without unnecessary loss of important neural information.</p>"},{"location":"#key-features","title":"\u2728 Key Features","text":"<ul> <li>\ud83c\udfaf Automatic Line Noise Detection: Detects 50 Hz, 60 Hz, or user-specified frequencies automatically</li> <li>\ud83e\udde0 Adaptive Processing: Dynamically adjusts cleaning strength to minimize negative impacts</li> <li>\ud83d\udcca Advanced Algorithms: Uses DSS combined with PCA for precise noise component identification</li> <li>\ud83d\udd27 Flexible Segmentation: Support for both fixed-length and adaptive chunk segmentation</li> <li>\ud83d\udcc8 Comprehensive Visualization: Built-in plotting for evaluating cleaning effectiveness</li> <li>\ud83d\udc0d Easy Integration: Works seamlessly with NumPy arrays and MNE-Python</li> <li>\u26a1 Professional Quality: Thoroughly tested, well-documented, and production-ready</li> </ul>"},{"location":"#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"#installation","title":"Installation","text":"<pre><code>pip install pyzaplineplus\n</code></pre>"},{"location":"#basic-usage","title":"Basic Usage","text":"<pre><code>import numpy as np\nfrom pyzaplineplus import zapline_plus\n\n# Your EEG data (time \u00d7 channels)\ndata = np.random.randn(10000, 64)  # Example: 10s of 64-channel data at 1000 Hz\nsampling_rate = 1000\n\n# Clean the data - it's that simple!\ncleaned_data = zapline_plus(data, sampling_rate)\n</code></pre>"},{"location":"#with-mne-python","title":"With MNE-Python","text":"<pre><code>import mne\nfrom pyzaplineplus import zapline_plus\n\n# Load your MNE data\nraw = mne.io.read_raw_fif('your_data.fif', preload=True)\ndata = raw.get_data().T  # Transpose to time \u00d7 channels\nsampling_rate = raw.info['sfreq']\n\n# Clean the data\ncleaned_data = zapline_plus(data, sampling_rate)\n\n# Update your MNE object\nraw._data = cleaned_data.T\n</code></pre>"},{"location":"#why-choose-pyzaplineplus","title":"\ud83c\udfaf Why Choose PyZaplinePlus?","text":""},{"location":"#traditional-notch-filters-vs-pyzaplineplus","title":"Traditional Notch Filters vs. PyZaplinePlus","text":"Feature Notch Filters PyZaplinePlus Spectral Distortion \u274c Can distort nearby frequencies \u2705 Preserves non-noise spectrum Adaptivity \u274c Fixed filtering \u2705 Adaptive to data characteristics Rank Preservation \u274c May reduce data rank \u2705 Maintains data rank Multiple Frequencies \u274c Requires multiple filters \u2705 Handles multiple frequencies simultaneously Automatic Detection \u274c Manual frequency specification \u2705 Automatic noise detection"},{"location":"#scientific-foundation","title":"Scientific Foundation","text":"<p>PyZaplinePlus is based on peer-reviewed research:</p> <ul> <li>Zapline-plus: Klug &amp; Kloosterman (2022) - Human Brain Mapping</li> <li>Original ZapLine: de Cheveign\u00e9 (2020) - NeuroImage</li> </ul>"},{"location":"#use-cases","title":"\ud83d\udd2c Use Cases","text":"<ul> <li>EEG Signal Processing: Remove power line noise from EEG recordings</li> <li>BCI Research: Clean data for brain-computer interface applications  </li> <li>Clinical Neuroscience: Preprocess data for clinical EEG analysis</li> <li>Research Studies: Ensure high-quality data for scientific publications</li> <li>Real-time Applications: Suitable for both offline and online processing</li> </ul>"},{"location":"#documentation","title":"\ud83d\udcd6 Documentation","text":"<ul> <li>Installation Guide: Detailed installation instructions</li> <li>Quick Start: Get up and running in minutes</li> <li>Examples: Comprehensive usage examples</li> <li>API Reference: Complete function documentation</li> <li>MNE Integration: Working with MNE-Python</li> </ul>"},{"location":"#contributing","title":"\ud83e\udd1d Contributing","text":"<p>We welcome contributions! See our Contributing Guide for details.</p>"},{"location":"#license","title":"\ud83d\udcc4 License","text":"<p>This project is licensed under the MIT License - see the LICENSE file for details.</p>"},{"location":"#citation","title":"\ud83d\udcda Citation","text":"<p>If you use PyZaplinePlus in your research, please cite:</p> <pre><code>@software{esmaeili2024pyzaplineplus,\n  title = {PyZaplinePlus: Advanced Python library for automatic and adaptive removal of line noise from EEG data},\n  author = {Esmaeili, Sina},\n  year = {2024},\n  url = {https://github.com/snesmaeili/PyZapline_plus}\n}\n</code></pre> <p>And the original Zapline-plus paper:</p> <pre><code>@article{klug2022zapline,\n  title={Zapline-plus: A Zapline extension for automatic and adaptive removal of frequency-specific noise artifacts in M/EEG},\n  author={Klug, Marius and Kloosterman, Niels A},\n  journal={Human Brain Mapping},\n  year={2022},\n  doi={10.1002/hbm.25832}\n}\n</code></pre>"},{"location":"#support","title":"\ud83d\udcac Support","text":"<ul> <li>Documentation: https://snesmaeili.github.io/PyZapline_plus/</li> <li>Issues: https://github.com/snesmaeili/PyZapline_plus/issues</li> <li>Discussions: https://github.com/snesmaeili/PyZapline_plus/discussions</li> <li>Email: sina.esmaeili@umontreal.ca</li> </ul>"},{"location":"about/changelog/","title":"Changelog","text":"<p>See the project\u2019s <code>CHANGELOG.md</code> for detailed release notes.</p>"},{"location":"about/citation/","title":"Citation","text":"<p>If you use PyZaplinePlus in your research, please cite:</p> <pre><code>@software{esmaeili2024pyzaplineplus,\n  title = {PyZaplinePlus: Advanced Python library for automatic and adaptive removal of line noise from EEG data},\n  author = {Esmaeili, Sina},\n  year = {2024},\n  url = {https://github.com/SinaEsmaeili/PyZaplinePlus}\n}\n</code></pre> <p>And the original Zapline-plus paper:</p> <pre><code>@article{klug2022zapline,\n  title={Zapline-plus: A Zapline extension for automatic and adaptive removal of frequency-specific noise artifacts in M/EEG},\n  author={Klug, Marius and Kloosterman, Niels A},\n  journal={Human Brain Mapping},\n  year={2022},\n  doi={10.1002/hbm.25832}\n}\n</code></pre>"},{"location":"about/license/","title":"License","text":"<p>PyZaplinePlus is released under the MIT License.</p> <p>See the root <code>LICENSE</code> file for the full text.</p>"},{"location":"api/core/","title":"Core API","text":""},{"location":"api/core/#pyzaplineplus.core.PyZaplinePlus","title":"<code>PyZaplinePlus(data, sampling_rate, **kwargs)</code>","text":"Source code in <code>pyzaplineplus/core.py</code> <pre><code>def __init__(self, data, sampling_rate, **kwargs):\n    # Validate inputs\n    if not isinstance(data, np.ndarray):\n        data = np.array(data)\n\n    # Check for empty data\n    if data.size == 0:\n        raise ValueError(\"Data array cannot be empty\")\n\n    # Validate sampling rate\n    if not isinstance(sampling_rate, (int, float)) or sampling_rate &lt;= 0:\n        raise ValueError(\"Sampling rate must be a positive number\")\n\n    # Ensure data is 2D (samples x channels)\n    if data.ndim == 1:\n        data = data.reshape(-1, 1)  # Convert to column vector\n    elif data.ndim &gt; 2:\n        raise ValueError(\"Data must be 1D or 2D array\")\n\n    self.data = data\n    self.sampling_rate = sampling_rate\n    self._warned_nyquist = False\n    self.config = {\n        'noisefreqs': kwargs.get('noisefreqs', []),\n        'minfreq': kwargs.get('minfreq', 17),\n        'maxfreq': kwargs.get('maxfreq', 99),\n        'adaptiveNremove': kwargs.get('adaptiveNremove', True),\n        'fixedNremove': kwargs.get('fixedNremove', 1),\n        'detectionWinsize': kwargs.get('detectionWinsize', 6),\n        # Match MATLAB default: 4 (2.5x power over mean in dB scale)\n        'coarseFreqDetectPowerDiff': kwargs.get('coarseFreqDetectPowerDiff', 4),\n        'coarseFreqDetectLowerPowerDiff': kwargs.get('coarseFreqDetectLowerPowerDiff', 1.76091259055681),\n        'searchIndividualNoise': kwargs.get('searchIndividualNoise', True),\n        'freqDetectMultFine': kwargs.get('freqDetectMultFine', 2.0),\n        'detailedFreqBoundsUpper': kwargs.get('detailedFreqBoundsUpper', [-0.05, 0.05]),\n        'detailedFreqBoundsLower': kwargs.get('detailedFreqBoundsLower', [-0.4, 0.1]),\n        'maxProportionAboveUpper': kwargs.get('maxProportionAboveUpper', 0.005),\n        'maxProportionBelowLower': kwargs.get('maxProportionBelowLower', 0.005),\n        'noiseCompDetectSigma': kwargs.get('noiseCompDetectSigma', 3.0),\n        'adaptiveSigma': kwargs.get('adaptiveSigma', True),\n        'minsigma': kwargs.get('minsigma', 2.5),\n        'maxsigma': kwargs.get('maxsigma', 5.0),  # Changed to 5\n        'chunkLength': kwargs.get('chunkLength', 0),\n        'minChunkLength': kwargs.get('minChunkLength', 30),\n        'winSizeCompleteSpectrum': kwargs.get('winSizeCompleteSpectrum', 300),\n        'nkeep': kwargs.get('nkeep', 0),\n        'plotResults': kwargs.get('plotResults', True),\n        'segmentLength': kwargs.get('segmentLength', 1),\n        'prominenceQuantile': kwargs.get('prominenceQuantile', 0.95),\n        'overwritePlot': kwargs.get('overwritePlot', False),\n        'figBase': kwargs.get('figBase', 100),\n        'figPos': kwargs.get('figPos', None),\n        'saveSpectra': kwargs.get('saveSpectra', False)\n        ,\n        # DSS parity/debug controls (dev)\n        # MATLAB parity defaults for DSS internals: use symmetric (+/-) bins\n        'dss_positive_only': kwargs.get('dss_positive_only', False),\n        'dss_divide_by_sumw2': kwargs.get('dss_divide_by_sumw2', False),\n        'dss_strict_frames': kwargs.get('dss_strict_frames', False),\n        'snapDssToFftBin': kwargs.get('snapDssToFftBin', False),\n        'saveDssDebug': kwargs.get('saveDssDebug', False),\n        'debugOutDir': kwargs.get('debugOutDir', None),\n    }\n</code></pre>"},{"location":"api/core/#pyzaplineplus.core.PyZaplinePlus.adaptive_chunk_detection","title":"<code>adaptive_chunk_detection(noise_freq)</code>","text":"<p>Use covariance matrices to adaptively segment data into chunks.</p> Source code in <code>pyzaplineplus/core.py</code> <pre><code>def adaptive_chunk_detection(self,noise_freq):\n    \"\"\"\n    Use covariance matrices to adaptively segment data into chunks.\n    \"\"\"\n    from scipy.signal import find_peaks\n    from scipy.spatial.distance import pdist\n    # 1. Bandpass Filter the Data\n    narrow_band_filtered = self.bandpass_filter(\n        self.data,\n        noise_freq - self.config['detectionWinsize'] / 2,\n        noise_freq + self.config['detectionWinsize'] / 2,\n        self.sampling_rate,\n    )\n\n    # 2. Determine Segment Length and Number of Segments\n    segment_length_samples = int(self.config['segmentLength'] * self.sampling_rate)\n    n_segments = max(len(narrow_band_filtered) // segment_length_samples, 1)\n\n    # 3. Compute Covariance Matrices for Each Segment\n    covariance_matrices = []\n    for i in range(n_segments):\n        start_idx = i * segment_length_samples\n        end_idx = (i + 1) * segment_length_samples if i != n_segments - 1 else len(narrow_band_filtered)\n        segment = narrow_band_filtered[start_idx:end_idx, :]\n        cov_matrix = np.cov(segment, rowvar=False)\n        covariance_matrices.append(cov_matrix)\n\n    # 4. Compute Distances Between Consecutive Covariance Matrices (match MATLAB)\n    # distances(i-1) = sum(pdist(C_i - C_{i-1})) / 2\n    distances = []\n    for i in range(1, len(covariance_matrices)):\n        cov_diff = covariance_matrices[i] - covariance_matrices[i - 1]\n        # Sum of pairwise distances across rows\n        d = pdist(cov_diff)\n        distance = np.sum(d) / 2.0\n        distances.append(distance)\n    distances = np.array(distances)\n\n    # 5. First Find Peaks to Obtain Prominences\n    initial_peaks, properties = find_peaks(distances,prominence=0)\n    prominences = properties['prominences']\n\n    # 6. Determine Prominence Threshold Based on Quantile\n    if len(prominences) == 0:\n        prominence_threshold: float = float('inf')  # No peaks found\n    else:\n        prominence_threshold: float = float(np.quantile(prominences, self.config['prominenceQuantile']))\n\n    # 7. Second Find Peaks Using Prominence Threshold\n    min_peak_distance_segments = int(np.ceil(self.config['minChunkLength'] / self.config['segmentLength']))\n    peaks, _ = find_peaks(\n        distances,\n        prominence=prominence_threshold,\n        distance=min_peak_distance_segments\n    )\n    # 8. Create Final Chunk Indices\n    # Initialize with 0 (start of data)\n    chunk_indices = [0]\n\n    # Calculate the end indices of the peaks in terms of samples\n    for peak in peaks:\n        # peak is the index in 'distances', corresponding to the boundary between segments\n        # So, the peak corresponds to the end of segment 'peak' and start of 'peak+1'\n        end_sample = (peak + 1) * segment_length_samples\n        chunk_indices.append(end_sample)\n\n    # Append the end of data\n    chunk_indices.append(len(self.data))\n\n    # 9. Ensure All Chunks Meet Minimum Length\n    min_length_samples = int(self.config['minChunkLength'] * self.sampling_rate)\n\n    # Check the first and last chunk only if we have at least two boundaries\n    if len(chunk_indices) &gt; 2:\n        if chunk_indices[1] - chunk_indices[0] &lt; min_length_samples:\n            chunk_indices.pop(1)  # Remove the first peak\n    if len(chunk_indices) &gt; 2:\n        if chunk_indices[-1] - chunk_indices[-2] &lt; min_length_samples:\n            chunk_indices.pop(-2)  # Remove the last peak\n\n    # Sort and remove duplicates if any\n    chunk_indices = sorted(set(chunk_indices))\n\n    return chunk_indices\n</code></pre>"},{"location":"api/core/#pyzaplineplus.core.PyZaplinePlus.adaptive_cleaning","title":"<code>adaptive_cleaning(clean_data, raw_data, noise_freq, zapline_config, cleaning_too_strong_once)</code>","text":"<p>Adjust the cleaning process if it was too weak or too strong.</p> Source code in <code>pyzaplineplus/core.py</code> <pre><code>def adaptive_cleaning(self, clean_data, raw_data, noise_freq,zapline_config,cleaning_too_strong_once):\n    \"\"\"\n    Adjust the cleaning process if it was too weak or too strong.\n    \"\"\"\n    # Compute the PSD of the clean data using periodic Hamming + 50% overlap\n    nperseg = int(self.config['winSizeCompleteSpectrum'] * self.sampling_rate)\n    f, pxx_clean = _welch_hamming_periodic(\n        clean_data, fs=self.sampling_rate, nperseg=nperseg, axis=0\n    )\n    pxx_clean_log = 10 * np.log10(pxx_clean)\n\n    # Determine center power by checking lower and upper third around noise frequency\n    detectionWinsize = zapline_config['detectionWinsize']\n    freq_range = (f &gt; noise_freq - (detectionWinsize / 2)) &amp; (f &lt; noise_freq + (detectionWinsize / 2))\n    this_fine_data = np.mean(pxx_clean_log[freq_range, :], axis=1)\n\n    # Calculate thirds of the data\n    third = int(np.round(len(this_fine_data) / 3))\n    if third == 0:\n        third = 1  # Ensure at least one sample\n\n    indices = np.concatenate((np.arange(0, third), np.arange(2 * third, len(this_fine_data))))\n    center_this_data = np.mean(this_fine_data[indices])\n\n    # Measure of variation using lower quantile\n    mean_lower_quantile = np.mean([\n        np.quantile(this_fine_data[0:third], 0.05),\n        np.quantile(this_fine_data[2 * third:], 0.05)\n    ])\n\n    # Compute thresholds\n    freq_detect_mult_fine = zapline_config['freqDetectMultFine']\n    remaining_noise_thresh_upper = center_this_data + freq_detect_mult_fine * (center_this_data - mean_lower_quantile)\n    remaining_noise_thresh_lower = center_this_data - freq_detect_mult_fine * (center_this_data - mean_lower_quantile)\n    zapline_config['remaining_noise_thresh_upper'] = remaining_noise_thresh_upper\n    zapline_config['remaining_noise_thresh_lower'] = remaining_noise_thresh_lower\n\n    # Frequency indices for upper and lower checks\n    freq_idx_upper_check = (f &gt; noise_freq + zapline_config['detailedFreqBoundsUpper'][0]) &amp; \\\n                        (f &lt; noise_freq + zapline_config['detailedFreqBoundsUpper'][1])\n    freq_idx_lower_check = (f &gt; noise_freq + zapline_config['detailedFreqBoundsLower'][0]) &amp; \\\n                        (f &lt; noise_freq + zapline_config['detailedFreqBoundsLower'][1])\n\n    # Store frequency indices for plotting\n    zapline_config['thisFreqidxUppercheck'] = freq_idx_upper_check\n    zapline_config['thisFreqidxLowercheck'] = freq_idx_lower_check\n\n    # Proportions for cleaning assessment\n    numerator_upper = float(np.sum(\n        np.mean(pxx_clean_log[freq_idx_upper_check, :], axis=1) &gt; remaining_noise_thresh_upper\n    ))\n    denominator_upper = float(np.sum(freq_idx_upper_check)) or 1.0\n    proportion_above_upper = numerator_upper / denominator_upper\n    cleaning_too_weak = proportion_above_upper &gt; zapline_config['maxProportionAboveUpper']\n    zapline_config['proportion_above_upper'] = proportion_above_upper\n\n    if cleaning_too_weak:\n        print(\"Cleaning too weak! \")\n    numerator_lower = float(np.sum(\n        np.mean(pxx_clean_log[freq_idx_lower_check, :], axis=1) &lt; remaining_noise_thresh_lower\n    ))\n    denominator_lower = float(np.sum(freq_idx_lower_check)) or 1.0\n    proportion_below_lower = numerator_lower / denominator_lower\n    cleaning_too_strong = proportion_below_lower &gt; zapline_config['maxProportionBelowLower']\n    zapline_config['proportion_below_lower'] = proportion_below_lower\n\n    # Adjust cleaning parameters based on the assessment\n    cleaning_done = True\n    if zapline_config['adaptiveNremove'] and zapline_config['adaptiveSigma']:\n        if cleaning_too_strong and zapline_config['noiseCompDetectSigma'] &lt; zapline_config['maxsigma']:\n            cleaning_too_strong_once = True\n            zapline_config['noiseCompDetectSigma'] = min(\n                zapline_config['noiseCompDetectSigma'] + 0.25,\n                zapline_config['maxsigma']\n            )\n            cleaning_done = False\n            # Decrease current minimum components to remove, but never below the original baseline\n            base_min = int(zapline_config.get('baseFixedNremove', zapline_config.get('fixedNremove', 1)))\n            zapline_config['fixedNremove'] = max(int(zapline_config['fixedNremove']) - 1, base_min)\n            print(f\"Cleaning too strong! Increasing sigma for noise component detection to {zapline_config['noiseCompDetectSigma']} \"\n                f\"and setting minimum number of removed components to {zapline_config['fixedNremove']}.\")\n            return cleaning_done, zapline_config, cleaning_too_strong_once\n\n        elif cleaning_too_weak and not cleaning_too_strong_once and zapline_config['noiseCompDetectSigma'] &gt; zapline_config['minsigma']:\n            zapline_config['noiseCompDetectSigma'] = max(\n                zapline_config['noiseCompDetectSigma'] - 0.25,\n                zapline_config['minsigma']\n            )\n            cleaning_done = False\n            zapline_config['fixedNremove'] += 1\n            print(f\"Cleaning too weak! Reducing sigma for noise component detection to {zapline_config['noiseCompDetectSigma']} \"\n                f\"and setting minimum number of removed components to {zapline_config['fixedNremove']}.\")\n\n    return cleaning_done, zapline_config, cleaning_too_strong_once\n</code></pre>"},{"location":"api/core/#pyzaplineplus.core.PyZaplinePlus.add_back_flat_channels","title":"<code>add_back_flat_channels(clean_data)</code>","text":"<p>Add back flat channels that were removed during preprocessing.</p> <p>Parameters:     clean_data (np.ndarray): Cleaned data with flat channels removed</p> <p>Returns:     np.ndarray: Cleaned data with flat channels added back as zeros</p> Source code in <code>pyzaplineplus/core.py</code> <pre><code>def add_back_flat_channels(self, clean_data):\n    \"\"\"\n    Add back flat channels that were removed during preprocessing.\n\n    Parameters:\n        clean_data (np.ndarray): Cleaned data with flat channels removed\n\n    Returns:\n        np.ndarray: Cleaned data with flat channels added back as zeros\n    \"\"\"\n    if self.flat_channels.size == 0:\n        return clean_data\n\n    # Create output array with original number of channels\n    original_n_channels = clean_data.shape[1] + len(self.flat_channels)\n    n_samples = clean_data.shape[0]\n    output_data = np.zeros((n_samples, original_n_channels))\n\n    # Create mask for non-flat channels\n    all_channels = np.arange(original_n_channels)\n    non_flat_channels = np.setdiff1d(all_channels, self.flat_channels)\n\n    # Insert cleaned data back into non-flat channel positions\n    output_data[:, non_flat_channels] = clean_data\n    # Flat channels remain as zeros\n\n    return output_data\n</code></pre>"},{"location":"api/core/#pyzaplineplus.core.PyZaplinePlus.apply_zapline_to_chunk","title":"<code>apply_zapline_to_chunk(chunk, noise_freq)</code>","text":"<p>Apply noise removal to the chunk using DSS (Denoising Source Separation) based on the provided MATLAB code.</p> Source code in <code>pyzaplineplus/core.py</code> <pre><code>def apply_zapline_to_chunk(self, chunk, noise_freq):\n    \"\"\"\n    Apply noise removal to the chunk using DSS (Denoising Source Separation) based on the provided MATLAB code.\n    \"\"\"\n    chunk = np.asarray(chunk)\n    if chunk.ndim == 1:\n        chunk = chunk[:, np.newaxis]\n\n    # Guard very short chunks: MATLAB requires enough samples for\n    # Welch/DSS; fall back to identity cleaning with a warning.\n    win_sec = float(self.config.get('winSizeCompleteSpectrum', 0) or 0)\n    if win_sec &lt;= 0:\n        win_sec = 1.0\n    guard_sec = min(max(win_sec, 0.5), 2.0)\n    min_chunk_samples = max(int(2 * guard_sec * self.sampling_rate), 2)\n\n    if chunk.shape[0] &lt; min_chunk_samples:\n        warnings.warn(\n            \"chunk too short for stable DSS cleaning; skipping component removal.\",\n            RuntimeWarning,\n        )\n        return chunk.copy(), 0, np.empty(0, dtype=float)\n\n    # Ensure self.config has all necessary default parameters\n    config_defaults = {\n        'nfft': 1024,\n        'nkeep': None,\n        'niterations': 1,\n        'fig1': 100,\n        'fig2': 101,\n        'adaptiveNremove': 1,\n        'noiseCompDetectSigma': 3,\n        'fixedNremove': 1,  # Default nremove is 1\n        'plotflag': False   # Set to True to enable plotting\n    }\n    for key, value in config_defaults.items():\n        if key not in self.config or self.config[key] is None:\n            self.config[key] = value\n\n    # Effective nfft: match MATLAB behavior by reducing nfft if chunk is short\n    nfft_cfg = int(self.config.get('nfft', 1024))\n    nfft = min(nfft_cfg, int(len(chunk)))\n\n    # Step 1: Define line frequency normalized to sampling rate\n    fline = noise_freq / self.sampling_rate\n\n    # Check that fline is less than Nyquist frequency\n    if fline &gt;= 0.5:\n        raise ValueError('fline should be less than Nyquist frequency (sampling_rate / 2)')\n\n    # Step 2: Apply smoothing to remove line frequency and harmonics\n    smoothed_chunk = self.nt_smooth(\n        chunk,\n        T=1 / fline,\n        n_iterations=self.config['niterations']\n    )\n\n    # Step 3: Compute the residual after smoothing\n    residual_chunk = chunk - smoothed_chunk\n\n    # Step 4: PCA to reduce dimensionality and avoid overfitting\n    nkeep_val = self.config.get('nkeep')\n    if nkeep_val in (None, 0) or (isinstance(nkeep_val, (int, float)) and nkeep_val &lt;= 0):\n        nkeep_val = residual_chunk.shape[1]\n        self.config['nkeep'] = nkeep_val\n    else:\n        nkeep_val = int(nkeep_val)\n    truncated_chunk, _ = self.nt_pca(\n        residual_chunk,\n        nkeep=nkeep_val\n    )\n\n    # Step 5: DSS to isolate line components from residual\n    n_harmonics = int(np.floor(0.5 / fline))\n    if self.config.get('snapDssToFftBin', False):\n        # Snap each harmonic to the nearest FFT bin index consistent with MATLAB round: floor(x+0.5)\n        harmonic_freqs = []\n        for k in range(1, n_harmonics + 1):\n            f_norm = fline * k\n            idx = int(np.floor(f_norm * nfft + 0.5))\n            idx = max(0, min(idx, nfft // 2))\n            # Inverse mapping to normalized frequency that re-yields idx in nt_bias_fft\n            f_norm_snap = (idx - 0.5) / nfft if idx &gt; 0 else 0.0\n            harmonic_freqs.append(f_norm_snap)\n        harmonics = np.array(harmonic_freqs, dtype=float)\n    else:\n        harmonics = fline * np.arange(1, n_harmonics + 1)\n    c0, c1 = self.nt_bias_fft(\n        truncated_chunk,\n        freq=harmonics,\n        nfft=nfft\n    )\n    # Optional: save DSS matrices/scores for parity debugging\n    if self.config.get('saveDssDebug', False):\n        try:\n            import os\n            out_dir = self.config.get('debugOutDir') or os.path.join(os.getcwd(), 'comparisons', 'debug')\n            os.makedirs(out_dir, exist_ok=True)\n            # Use noise freq and chunk length as part of the name\n            tag = f\"f{noise_freq:.6f}_n{len(chunk)}\"\n            np.savez(\n                os.path.join(out_dir, f\"dss_mats_{tag}.npz\"),\n                c0=c0, c1=c1,\n            )\n        except Exception:\n            pass\n    todss, pwr0, pwr1 = self.nt_dss0(c0, c1)\n    scores = pwr1 / pwr0\n\n    # Optional plotting of DSS scores\n    if self.config['plotflag']:\n        plt.figure(self.config['fig1'])\n        plt.clf()\n        plt.plot(scores, '.-')\n        plt.xlabel('Component')\n        plt.ylabel('Score')\n        plt.title('DSS to enhance line frequencies')\n        # plt.savefig('dss_scores.png')\n\n    # Step 6: Determine the number of components to remove\n    if scores is None or len(scores) == 0:\n        nremove = 0\n    elif self.config['adaptiveNremove']:\n        adaptive_nremove, _ = self.iterative_outlier_removal(\n            scores,\n            self.config['noiseCompDetectSigma']\n        )\n        if adaptive_nremove &lt; self.config['fixedNremove']:\n            nremove = self.config['fixedNremove']\n            print(\n                f\"Fixed nremove ({self.config['fixedNremove']}) is larger than adaptive nremove, using fixed nremove!\"\n            )\n        else:\n            nremove = adaptive_nremove\n        # Cap removal to at most 1/5 of components, but ensure at least 1 when any components exist\n        if nremove &gt; len(scores) // 5:\n            nremove = max(1, len(scores) // 5)\n            print(\n                f\"nremove is larger than 1/5th of the components, using that ({nremove})!\"\n            )\n    else:\n        nremove = self.config['fixedNremove']\n\n    # Step 7: Project the line-dominated components out of the residual\n    if nremove &gt; 0:\n        # Get line-dominated components\n        line_components = self.nt_mmat(\n            truncated_chunk,\n            todss[:, :nremove]\n        )\n        # Project them out\n        projected_chunk,_,_= self.nt_tsr(\n            residual_chunk,\n            line_components\n        )\n        # Reconstruct the clean signal\n        clean_chunk = smoothed_chunk + projected_chunk\n    else:\n        clean_chunk = chunk\n\n    # Optional plotting of spectra\n    if self.config['plotflag']:\n        # Normalize data for plotting\n        norm_factor = np.sqrt(np.mean(chunk ** 2))\n        chunk_norm = chunk / norm_factor\n        clean_chunk_norm = clean_chunk / norm_factor\n        removed_chunk_norm = (chunk - clean_chunk) / norm_factor\n\n        # Compute spectra\n        pxx_chunk, f = self.nt_spect_plot(\n            chunk_norm,\n            nfft=self.config['nfft'],\n            fs=self.sampling_rate,\n            return_data=True\n        )\n        pxx_clean, _ = self.nt_spect_plot(\n            clean_chunk_norm,\n            nfft=self.config['nfft'],\n            fs=self.sampling_rate,\n            return_data=True\n        )\n        pxx_removed, _ = self.nt_spect_plot(\n            removed_chunk_norm,\n            nfft=self.config['nfft'],\n            fs=self.sampling_rate,\n            return_data=True\n        )\n\n        divisor = np.sum(pxx_chunk, axis=0)\n        # Plot original spectrum\n        plt.figure(self.config['fig2'])\n        plt.clf()\n        plt.subplot(1, 2, 1)\n        plt.semilogy(f, np.abs(pxx_chunk) / divisor, label='Original', color='k')\n        plt.xlabel('Frequency (Hz)')\n        plt.ylabel('Relative Power')\n        plt.legend()\n        plt.grid(True, which='both', axis='both')\n        yl1 = plt.ylim()\n\n        # Plot cleaned and removed spectra\n        plt.subplot(1, 2, 2)\n        plt.semilogy(f, np.abs(pxx_clean) / divisor, label='Clean', color='g')\n        if nremove != 0:\n            plt.semilogy(f, np.abs(pxx_removed) / divisor, label='Removed', color='r')\n            plt.legend()\n        plt.xlabel('Frequency (Hz)')\n        plt.grid(True, which='both', axis='both')\n        yl2 = plt.ylim()\n\n        # Adjust y-limits to be the same\n        yl_min = min(yl1[0], yl2[0])\n        yl_max = max(yl1[1], yl2[1])\n        plt.subplot(1, 2, 1)\n        plt.ylim([yl_min, yl_max])\n        plt.subplot(1, 2, 2)\n        plt.ylim([yl_min, yl_max])\n\n        plt.show()\n\n    return clean_chunk, nremove, scores\n</code></pre>"},{"location":"api/core/#pyzaplineplus.core.PyZaplinePlus.bandpass_filter","title":"<code>bandpass_filter(data, lowcut, highcut, fs, order=5)</code>","text":"<p>Apply a bandpass filter to the data.</p> Source code in <code>pyzaplineplus/core.py</code> <pre><code>def bandpass_filter(self, data, lowcut, highcut, fs, order=5):\n    \"\"\"\n    Apply a bandpass filter to the data.\n    \"\"\"\n    nyq = 0.5 * fs\n    # Normalize with guards against invalid edges and Nyquist issues\n    low_raw = lowcut / nyq\n    high_raw = highcut / nyq\n    low = max(1e-9, min(low_raw, 0.999999))\n    high = max(1e-9, min(high_raw, 0.999999))\n    # If requested band exceeded Nyquist, clip and warn once\n    if (low_raw != low) or (high_raw != high):\n        if not self._warned_nyquist:\n            warnings.warn(\n                \"Requested band exceeded [0, fs/2]; clipping to valid range.\",\n                RuntimeWarning,\n            )\n            self._warned_nyquist = True\n    # If invalid band, return data unchanged (no-op)\n    if high &lt;= low:\n        return data\n    b, a = signal.butter(order, [low, high], btype='band')\n    return signal.filtfilt(b, a, data, axis=0)\n</code></pre>"},{"location":"api/core/#pyzaplineplus.core.PyZaplinePlus.compute_analytics","title":"<code>compute_analytics(pxx_raw_log, pxx_clean_log, f, noise_freq)</code>","text":"<p>Compute analytics to evaluate the cleaning process.</p> Source code in <code>pyzaplineplus/core.py</code> <pre><code>def compute_analytics(self, pxx_raw_log, pxx_clean_log, f, noise_freq):\n    \"\"\"\n    Compute analytics to evaluate the cleaning process.\n    \"\"\"\n    # Overall power removed (in log space)\n    proportion_removed = 1 - 10 ** ((np.mean(pxx_clean_log) - np.mean(pxx_raw_log)) / 10)\n\n    # Frequency range to evaluate below noise frequency\n    freq_idx_below_noise = (f &gt;= max(noise_freq - 11, 0)) &amp; (f &lt;= noise_freq - 1)\n    proportion_removed_below_noise = 1 - 10 ** (\n        (np.mean(pxx_clean_log[freq_idx_below_noise, :]) - np.mean(pxx_raw_log[freq_idx_below_noise, :])) / 10\n    )\n\n    # Frequency range at noise frequency\n    freq_idx_noise = (f &gt; noise_freq + self.config['detailedFreqBoundsUpper'][0]) &amp; \\\n                    (f &lt; noise_freq + self.config['detailedFreqBoundsUpper'][1])\n    proportion_removed_noise = 1 - 10 ** (\n        (np.mean(pxx_clean_log[freq_idx_noise, :]) - np.mean(pxx_raw_log[freq_idx_noise, :])) / 10\n    )\n\n    # Ratio of noise power to surroundings before and after cleaning\n    freq_idx_noise_surrounding = (\n        ((f &gt; noise_freq - (self.config['detectionWinsize'] / 2)) &amp; (f &lt; noise_freq - (self.config['detectionWinsize'] / 6))) |\n        ((f &gt; noise_freq + (self.config['detectionWinsize'] / 6)) &amp; (f &lt; noise_freq + (self.config['detectionWinsize'] / 2)))\n    )\n\n    mean_pxx_raw_noise = np.mean(pxx_raw_log[freq_idx_noise, :])\n    mean_pxx_raw_noise_surrounding = np.mean(pxx_raw_log[freq_idx_noise_surrounding, :])\n    ratio_noise_raw = 10 ** ((mean_pxx_raw_noise - mean_pxx_raw_noise_surrounding) / 10)\n\n    mean_pxx_clean_noise = np.mean(pxx_clean_log[freq_idx_noise, :])\n    mean_pxx_clean_noise_surrounding = np.mean(pxx_clean_log[freq_idx_noise_surrounding, :])\n    ratio_noise_clean = 10 ** ((mean_pxx_clean_noise - mean_pxx_clean_noise_surrounding) / 10)\n\n    return {\n        'proportion_removed': proportion_removed,\n        'proportion_removed_below_noise': proportion_removed_below_noise,\n        'proportion_removed_noise': proportion_removed_noise,\n        'ratio_noise_raw': ratio_noise_raw,\n        'ratio_noise_clean': ratio_noise_clean\n    }\n</code></pre>"},{"location":"api/core/#pyzaplineplus.core.PyZaplinePlus.compute_spectrum","title":"<code>compute_spectrum(data)</code>","text":"<p>Compute the power spectral density of the input data.</p> Notes <p>Uses a Hamming window with 50% overlap to match MATLAB pwelch (periodic form). This ensures parity with MATLAB outputs.</p> Source code in <code>pyzaplineplus/core.py</code> <pre><code>def compute_spectrum(self, data):\n    \"\"\"\n    Compute the power spectral density of the input data.\n\n    Notes\n    -----\n    Uses a Hamming window with 50% overlap to match MATLAB pwelch\n    (periodic form). This ensures parity with MATLAB outputs.\n    \"\"\"\n    # Set window length and overlap to match MATLAB code\n    nperseg = int(self.config['winSizeCompleteSpectrum'] * self.sampling_rate)\n\n    # Ensure nperseg doesn't exceed data length\n    max_nperseg = data.shape[0]\n    if nperseg &gt; max_nperseg:\n        nperseg = max_nperseg\n\n    # Ensure minimum nperseg for meaningful spectrum\n    if nperseg &lt; 8:\n        nperseg = min(8, max_nperseg)\n\n    f, pxx = _welch_hamming_periodic(\n        data, fs=self.sampling_rate, nperseg=nperseg, axis=0\n    )\n\n    # Log transform\n    pxx_log = 10 * np.log10(pxx)\n\n    return pxx_log, f\n</code></pre>"},{"location":"api/core/#pyzaplineplus.core.PyZaplinePlus.detect_chunk_noise","title":"<code>detect_chunk_noise(chunk, noise_freq, return_details=False)</code>","text":"<p>Detect noise frequency in a given chunk.</p> <p>Returns:</p> Type Description <code>float or tuple</code> <p>If <code>return_details</code> is <code>False</code> (default) the function returns the frequency that should be cleaned for the chunk. When <code>return_details</code> is <code>True</code> a tuple <code>(chunk_noise_freq, found_flag, peak_freq)</code> matching the MATLAB bookkeeping is returned.</p> Source code in <code>pyzaplineplus/core.py</code> <pre><code>def detect_chunk_noise(self, chunk, noise_freq, return_details=False):\n    \"\"\"\n    Detect noise frequency in a given chunk.\n\n    Returns\n    -------\n    float or tuple\n        If ``return_details`` is ``False`` (default) the function returns the\n        frequency that should be cleaned for the chunk. When\n        ``return_details`` is ``True`` a tuple ``(chunk_noise_freq,\n        found_flag, peak_freq)`` matching the MATLAB bookkeeping is\n        returned.\n    \"\"\"\n    # Use Hann periodic + 50% overlap to match MATLAB parity\n    nperseg = len(chunk)\n    f, pxx_chunk = _welch_hamming_periodic(\n        chunk, fs=self.sampling_rate, nperseg=nperseg, axis=0\n    )\n    pxx_log = 10 * np.log10(pxx_chunk)\n\n    freq_idx = (f &gt; noise_freq - self.config['detectionWinsize'] / 2) &amp; (f &lt; noise_freq + self.config['detectionWinsize'] / 2)\n    detailed_freq_idx = (f &gt; noise_freq + self.config['detailedFreqBoundsUpper'][0]) &amp; \\\n                        (f &lt; noise_freq + self.config['detailedFreqBoundsUpper'][1])\n    detailed_freqs = f[detailed_freq_idx]\n\n    # Guard small-bin and empty-slice cases to avoid crashes on short chunks\n    n_bins_fine = int(np.sum(freq_idx))\n    if n_bins_fine &lt; 3:\n        # Not enough resolution: report no detection and keep global frequency\n        chunk_noise_freq = float(noise_freq)\n        found_flag = 0\n        peak_freq = float(noise_freq)\n        return (\n            (chunk_noise_freq, found_flag, peak_freq)\n            if return_details\n            else chunk_noise_freq\n        )\n\n    fine_data = np.mean(pxx_log[freq_idx, :], axis=1)\n    third = max(1, len(fine_data) // 3)\n    # Avoid quantiles on empty slices\n    center_data = float(np.mean([fine_data[:third], fine_data[-third:]]))\n    lower_quantile = float(np.mean([\n        np.quantile(fine_data[:third], 0.05),\n        np.quantile(fine_data[-third:], 0.05),\n    ]))\n    detailed_thresh = center_data + self.config['freqDetectMultFine'] * (center_data - lower_quantile)\n\n    # If detailed freq slice is empty, keep the original frequency\n    if detailed_freqs.size &lt; 1:\n        chunk_noise_freq = float(noise_freq)\n        found_flag = 0\n        peak_freq = float(noise_freq)\n        return (\n            (chunk_noise_freq, found_flag, peak_freq)\n            if return_details\n            else chunk_noise_freq\n        )\n\n    # Compute detailed-band max and peak\n    detailed_band_mean = np.mean(pxx_log[detailed_freq_idx, :], axis=1)\n    max_fine_power: float = float(np.max(detailed_band_mean))\n    peak_idx = int(np.argmax(detailed_band_mean))\n    peak_freq = float(detailed_freqs[peak_idx])\n    if max_fine_power &gt; detailed_thresh:\n        chunk_noise_freq = peak_freq\n        found_flag = 1\n        return (\n            (chunk_noise_freq, found_flag, peak_freq)\n            if return_details\n            else chunk_noise_freq\n        )\n    # No strong local peak found: clean with global noise_freq but report peak\n    chunk_noise_freq = float(noise_freq)\n    found_flag = 0\n    return (\n        (chunk_noise_freq, found_flag, peak_freq)\n        if return_details\n        else chunk_noise_freq\n    )\n</code></pre>"},{"location":"api/core/#pyzaplineplus.core.PyZaplinePlus.detect_flat_channels","title":"<code>detect_flat_channels()</code>","text":"<p>Detect and return indices of flat channels in the data.</p> Source code in <code>pyzaplineplus/core.py</code> <pre><code>def detect_flat_channels(self):\n    \"\"\"\n    Detect and return indices of flat channels in the data.\n    \"\"\"\n    diff_data = np.diff(self.data, axis=0)\n    flat_channels = np.where(np.all(diff_data == 0, axis=0))[0]\n    if len(flat_channels) &gt; 0:\n        print(f\"Flat channels detected (will be ignored and added back in after Zapline-plus processing): {flat_channels}\")\n        self.data = np.delete(self.data, flat_channels, axis=1)\n    return flat_channels\n</code></pre>"},{"location":"api/core/#pyzaplineplus.core.PyZaplinePlus.detect_line_noise","title":"<code>detect_line_noise()</code>","text":"<p>Detect line noise (50 Hz or 60 Hz) in the data.</p> <p>Notes: - The MATLAB implementation searches within narrow bands around 50/60 Hz.   To robustly capture the true peak (which might fall exactly on-bin), we   use a slightly wider 2 Hz window: 49\u201351 Hz and 59\u201361 Hz.</p> Source code in <code>pyzaplineplus/core.py</code> <pre><code>def detect_line_noise(self):\n    \"\"\"\n    Detect line noise (50 Hz or 60 Hz) in the data.\n\n    Notes:\n    - The MATLAB implementation searches within narrow bands around 50/60 Hz.\n      To robustly capture the true peak (which might fall exactly on-bin), we\n      use a slightly wider 2 Hz window: 49\u201351 Hz and 59\u201361 Hz.\n    \"\"\"\n    if self.config['noisefreqs'] != 'line':\n        return\n    # Use a robust 2 Hz window around 50 Hz and 60 Hz\n    idx = ((self.f &gt; 49) &amp; (self.f &lt; 51)) | ((self.f &gt; 59) &amp; (self.f &lt; 61))\n    if not np.any(idx):\n        # As a fallback, try the original narrow bounds\n        idx = ((self.f &gt; 49) &amp; (self.f &lt; 50)) | ((self.f &gt; 59) &amp; (self.f &lt; 60))\n    if not np.any(idx):\n        # Still nothing: warn and skip narrowing; allow automatic detection later\n        print(\"Warning: No bins found around 50/60 Hz for 'line' detection; falling back to automatic detection.\")\n        self.config['noisefreqs'] = []\n        return None\n    spectra_chunk = self.pxx_raw_log[idx, :]\n    # Flatten across channels and pick global maximum within the search band\n    flat_idx = np.argmax(spectra_chunk)\n    row_idx = np.unravel_index(flat_idx, spectra_chunk.shape)[0]\n    noise_freq = self.f[idx][row_idx]\n    print(f\"'noisefreqs' parameter was set to 'line', found line noise candidate at {noise_freq:.2f} Hz!\")\n    self.config['noisefreqs'] = []\n    self.config['minfreq'] = noise_freq - self.config['detectionWinsize'] / 2\n    self.config['maxfreq'] = noise_freq + self.config['detectionWinsize'] / 2\n    return noise_freq\n</code></pre>"},{"location":"api/core/#pyzaplineplus.core.PyZaplinePlus.detect_noise_frequencies","title":"<code>detect_noise_frequencies()</code>","text":"<p>Automatically detect noise frequencies in the data.</p> Source code in <code>pyzaplineplus/core.py</code> <pre><code>def detect_noise_frequencies(self):\n    \"\"\"\n    Automatically detect noise frequencies in the data.\n    \"\"\"\n    noise_freqs = []\n    threshs=[]\n    current_minfreq = self.config['minfreq']\n    self.config['automaticFreqDetection'] = True\n    while True:\n        noisefreq, _, _, thresh = find_next_noisefreq(\n            self.pxx_raw_log,\n            self.f,\n            current_minfreq,\n            self.config['coarseFreqDetectPowerDiff'],\n            self.config['detectionWinsize'],\n            self.config['maxfreq'],\n            self.config['coarseFreqDetectLowerPowerDiff'],\n            verbose=False\n        )\n\n        if noisefreq is None:\n            break\n\n        noise_freqs.append(noisefreq)\n        threshs.append(thresh)\n        current_minfreq = noisefreq + self.config['detectionWinsize'] / 2\n\n        if current_minfreq &gt;= self.config['maxfreq']:\n            break\n    self.config['thresh'] = threshs\n    return noise_freqs\n</code></pre>"},{"location":"api/core/#pyzaplineplus.core.PyZaplinePlus.finalize_inputs","title":"<code>finalize_inputs()</code>","text":"<p>Finalize and prepare inputs for the Zapline-plus algorithm.</p> Source code in <code>pyzaplineplus/core.py</code> <pre><code>def finalize_inputs(self):\n    \"\"\"\n    Finalize and prepare inputs for the Zapline-plus algorithm.\n    \"\"\"\n    # Check and adjust sampling rate\n    if self.sampling_rate &gt; 500:\n        print(\"WARNING: It is recommended to downsample the data to around 250Hz to 500Hz before applying Zapline-plus!\")\n        print(f\"Current sampling rate is {self.sampling_rate}. Results may be suboptimal!\")\n\n    # Transpose data if necessary\n    self.transpose_data = self.data.shape[1] &gt; self.data.shape[0]\n    if self.transpose_data:\n        self.data = self.data.T\n\n    # Adjust window size for spectrum calculation to mirror MATLAB behavior\n    # MATLAB ensures at least 8 segments for pwelch by setting the window to ~1/8 of data length\n    if self.config['winSizeCompleteSpectrum'] * self.sampling_rate &gt; (self.data.shape[0] / 8):\n        new_win = int(np.floor((self.data.shape[0] / self.sampling_rate) / 8))\n        self.config['winSizeCompleteSpectrum'] = max(new_win, 1)\n        print('Data set is short. Adjusted window size for whole data set spectrum calculation to be 1/8 of the length!')\n\n    # Set nkeep\n    if self.config['nkeep'] == 0:\n        self.config['nkeep'] = self.data.shape[1]\n\n    # Track the minimum allowed fixedNremove to support adaptive updates\n    if 'baseFixedNremove' not in self.config:\n        self.config['baseFixedNremove'] = int(self.config.get('fixedNremove', 1))\n\n    # Detect flat channels\n    self.flat_channels = self.detect_flat_channels()\n\n    # Compute initial spectrum\n    self.pxx_raw_log, self.f = self.compute_spectrum(self.data)\n\n    # Handle 'line' noise frequency\n    if self.config['noisefreqs'] == 'line':\n        self.detect_line_noise()\n\n\n    # Automatic noise frequency detection\n    if not self.config['noisefreqs']:\n        self.config['noisefreqs'] = self.detect_noise_frequencies()\n</code></pre>"},{"location":"api/core/#pyzaplineplus.core.PyZaplinePlus.fixed_chunk_detection","title":"<code>fixed_chunk_detection()</code>","text":"<p>Split the data into fixed-length chunks.</p> Source code in <code>pyzaplineplus/core.py</code> <pre><code>def fixed_chunk_detection(self):\n    \"\"\"\n    Split the data into fixed-length chunks.\n    \"\"\"\n    chunk_length_samples = int(self.config['chunkLength'] * self.sampling_rate)\n    chunk_indices = [0]\n    while chunk_indices[-1] + chunk_length_samples &lt; len(self.data):\n        chunk_indices.append(chunk_indices[-1] + chunk_length_samples)\n    chunk_indices.append(len(self.data))\n    return chunk_indices\n</code></pre>"},{"location":"api/core/#pyzaplineplus.core.PyZaplinePlus.generate_output_figures","title":"<code>generate_output_figures(data, clean_data, noise_freq, zapline_config, pxx_raw_log, pxx_clean_log, pxx_removed_log, f, analytics, NremoveFinal)</code>","text":"<p>Generate figures to visualize the results, replicating the MATLAB figures with the same colors.</p> Source code in <code>pyzaplineplus/core.py</code> <pre><code>def generate_output_figures(self, data, clean_data, noise_freq, zapline_config, pxx_raw_log, pxx_clean_log, pxx_removed_log, f, analytics, NremoveFinal):\n    \"\"\"\n    Generate figures to visualize the results, replicating the MATLAB figures with the same colors.\n    \"\"\"\n    import numpy as np\n    import matplotlib.pyplot as plt\n    from matplotlib.patches import Rectangle\n    import matplotlib.gridspec as gridspec\n    import os\n\n    # Colors\n    red = np.array([230, 100, 50]) / 256\n    green = np.array([0, 97, 100]) / 256\n    grey = np.array([0.2, 0.2, 0.2])\n\n    # Prepare chunk indices for plotting\n    chunk_indices = zapline_config.get('chunkIndices', None)\n    if chunk_indices is None:\n        print(\"Error: 'chunkIndices' not provided in 'zapline_config'.\")\n        return\n    chunk_indices = np.array(chunk_indices)\n    chunk_indices_plot = chunk_indices / self.sampling_rate / 60  # Convert to minutes\n\n    # Compute chunk_indices_plot_individual\n    chunk_indices_plot_individual = np.array([\n        np.mean([chunk_indices_plot[i], chunk_indices_plot[i+1]]) for i in range(len(chunk_indices_plot)-1)\n    ])\n\n    # Frequency index for plotting around the noise frequency\n    this_freq_idx_plot = (f &gt;= noise_freq - 1.1) &amp; (f &lt;= noise_freq + 1.1)\n\n    # Create figure and GridSpec\n    fig = plt.figure(figsize=(20, 15))\n    gs = gridspec.GridSpec(nrows=5, ncols=4, figure=fig, height_ratios=[1, 1, 1, 1, 1])\n\n    # First row: ax1, ax4, ax5\n    ax1 = fig.add_subplot(gs[0, 0])\n    ax4 = fig.add_subplot(gs[0, 1])\n    ax5 = fig.add_subplot(gs[0, 2:4])\n\n    # Second row: ax2 (span all columns)\n    ax2 = fig.add_subplot(gs[1, :])\n\n    # Third row: ax3 (span all columns)\n    ax3 = fig.add_subplot(gs[2, :])\n\n    # Fourth row: ax6 and ax7\n    ax6 = fig.add_subplot(gs[3, 0:2])\n    ax7 = fig.add_subplot(gs[3, 2:4])\n\n    # Fifth row: ax8 (span all columns)\n    ax8 = fig.add_subplot(gs[4, :])\n\n    # Plot original power on ax1\n    ax1.plot(f[this_freq_idx_plot], np.mean(pxx_raw_log[this_freq_idx_plot, :], axis=1), color=grey)\n    ax1.set_xlim((float(f[this_freq_idx_plot][0] - 0.01), float(f[this_freq_idx_plot][-1])))\n\n    # Y-axis limits\n    remaining_noise_thresh_lower = zapline_config.get('remaining_noise_thresh_lower', None)\n    remaining_noise_thresh_upper = zapline_config.get('remaining_noise_thresh_upper', None)\n    coarse_freq_detect_power_diff = zapline_config.get('coarseFreqDetectPowerDiff', None)\n    if remaining_noise_thresh_lower is not None and remaining_noise_thresh_upper is not None and coarse_freq_detect_power_diff is not None:\n        ylim_lower = remaining_noise_thresh_lower - 0.25 * (remaining_noise_thresh_upper - remaining_noise_thresh_lower)\n        ylim_upper = np.min(np.mean(pxx_raw_log[this_freq_idx_plot, :], axis=1)) + coarse_freq_detect_power_diff * 2\n        ax1.set_ylim((float(ylim_lower), float(ylim_upper)))\n    else:\n        y_min: float = float(np.min(np.mean(pxx_raw_log[this_freq_idx_plot, :], axis=1)))\n        y_max: float = float(np.max(np.mean(pxx_raw_log[this_freq_idx_plot, :], axis=1)))\n        ax1.set_ylim((float(y_min - 0.25 * (y_max - y_min)), float(y_max + 0.25 * (y_max - y_min))))\n\n    ax1.spines['top'].set_visible(False)\n    ax1.spines['right'].set_visible(False)\n    ax1.tick_params(labelsize=12)\n    ax1.set_xlabel('Frequency (Hz)')\n    ax1.set_ylabel('Power [10*log10 \u03bcV^2/Hz]')\n\n    # Automatic frequency detection\n    automatic_freq_detection = zapline_config.get('automaticFreqDetection', False)\n    thresh = zapline_config.get('thresh', None)\n    if automatic_freq_detection:\n        if thresh is not None:\n            ax1.plot(ax1.get_xlim(), [thresh[0], thresh[0]], color=red)\n        ax1.set_title(f'Detected frequency: {noise_freq} Hz')\n    else:\n        ax1.set_title(f'Predefined frequency: {noise_freq} Hz')\n\n    # Plot number of removed components on ax2\n    ax2.cla()\n\n    if NremoveFinal is None:\n        print(\"Error: 'NremoveFinal' not provided.\")\n        return\n\n    search_individual_noise = zapline_config.get('searchIndividualNoise', False)\n    found_noise = zapline_config.get('foundNoise', [True]*len(NremoveFinal))\n    nonoisehandle = None\n\n    for i_chunk in range(len(chunk_indices_plot)-1):\n        if (not search_individual_noise) or found_noise[i_chunk]:\n            # Plot grey fill\n            ax2.fill_between(\n                [chunk_indices_plot[i_chunk], chunk_indices_plot[i_chunk+1]],\n                0, NremoveFinal[i_chunk],\n                color=grey,\n                alpha=0.5\n            )\n        else:\n            # Plot green fill\n            nonoisehandle = ax2.fill_between(\n                [chunk_indices_plot[i_chunk], chunk_indices_plot[i_chunk+1]],\n                0, NremoveFinal[i_chunk],\n                color=green,\n                alpha=0.5\n            )\n\n    ax2.set_xlim((float(chunk_indices_plot[0]), float(chunk_indices_plot[-1])))\n    ax2.set_ylim((0.0, float(max(NremoveFinal) + 1)))\n    n_chunks = len(NremoveFinal)\n    ax2.set_title(f'# removed comps in {n_chunks} chunks, \u03bc = {round(np.mean(NremoveFinal), 2)}')\n    ax2.tick_params(labelsize=12)\n    ax2.spines['top'].set_visible(False)\n    ax2.spines['right'].set_visible(False)\n    ax2.set_xlabel('Time [minutes]')\n    ax2.set_ylabel('Number of Components Removed')\n\n    # Plot noise peaks on ax3\n    noise_peaks = zapline_config.get('noisePeaks', None)\n    if noise_peaks is None:\n        print(\"Error: 'noisePeaks' not provided in 'zapline_config'.\")\n        return\n\n    for i_chunk in range(len(chunk_indices_plot)-2):\n        ax3.plot(\n            [chunk_indices_plot[i_chunk+1], chunk_indices_plot[i_chunk+1]],\n            [0, 1000],\n            color=grey * 3\n        )\n\n    ax3.plot(chunk_indices_plot_individual, noise_peaks, color=grey)\n    ax3.set_xlim((float(chunk_indices_plot[0]), float(chunk_indices_plot[-1])))\n    max_diff = max([(max(noise_peaks)) - noise_freq, noise_freq - (min(noise_peaks))])\n    if max_diff == 0:\n        max_diff = 0.01\n    ax3.set_ylim((float(noise_freq - max_diff * 1.5), float(noise_freq + max_diff * 1.5)))\n    ax3.set_xlabel('Time [minutes]')\n    ax3.set_title('Individual noise frequencies [Hz]')\n    ax3.tick_params(labelsize=12)\n    ax3.spines['top'].set_visible(False)\n    ax3.spines['right'].set_visible(False)\n\n    if search_individual_noise:\n        found_noise_plot = np.array(found_noise, dtype=float)\n        found_noise_plot[found_noise_plot == 1] = np.nan\n        found_noise_plot[~np.isnan(found_noise_plot)] = noise_peaks[~np.isnan(found_noise_plot)]\n        ax3.plot(chunk_indices_plot_individual, found_noise_plot, 'o', color=green)\n\n        if nonoisehandle is not None:\n            ax3.legend([nonoisehandle], ['No clear noise peak found'], edgecolor=[0.8, 0.8, 0.8])\n\n    # Plot scores on ax4\n    scores = zapline_config.get('scores', None)\n    if scores is None:\n        print(\"Error: 'scores' not provided in 'zapline_config'.\")\n        return\n\n    ax4.plot(np.nanmean(scores, axis=0), color=grey)\n\n    mean_Nremove = np.mean(NremoveFinal) + 1\n    ax4.plot([mean_Nremove, mean_Nremove], ax4.get_ylim(), color=red)\n    ax4.set_xlim((0.7, float(round(scores.shape[1] / 3))))\n    adaptive_nremove = zapline_config.get('adaptiveNremove', False)\n    noise_comp_detect_sigma = zapline_config.get('noiseCompDetectSigma', None)\n    if adaptive_nremove and noise_comp_detect_sigma is not None:\n        ax4.set_title(f'Mean artifact scores [a.u.], \u03c3 for detection = {noise_comp_detect_sigma}')\n    else:\n        ax4.set_title('Mean artifact scores [a.u.]')\n    ax4.set_xlabel('Component')\n    ax4.tick_params(labelsize=12)\n    ax4.spines['top'].set_visible(False)\n    ax4.spines['right'].set_visible(False)\n    ax4.legend(['Mean removed'], edgecolor=[0.8, 0.8, 0.8])\n\n    # Plot new power on ax5\n    ax5.plot(f[this_freq_idx_plot], np.mean(pxx_clean_log[this_freq_idx_plot, :], axis=1), color=green)\n    ax5.set_xlim((float(f[this_freq_idx_plot][0] - 0.01), float(f[this_freq_idx_plot][-1])))\n\n    # Plot thresholds\n    this_freq_idx_upper_check = zapline_config.get('thisFreqidxUppercheck', None)\n    this_freq_idx_lower_check = zapline_config.get('thisFreqidxLowercheck', None)\n    remaining_noise_thresh_upper = zapline_config.get('remaining_noise_thresh_upper', None)\n    remaining_noise_thresh_lower = zapline_config.get('remaining_noise_thresh_lower', None)\n    proportion_above_upper = zapline_config.get('proportion_above_upper', None)\n    proportion_below_lower = zapline_config.get('proportion_below_lower', None)\n\n    try:\n        if this_freq_idx_upper_check is not None and this_freq_idx_lower_check is not None:\n            upper_freqs = f[this_freq_idx_upper_check]\n            lower_freqs = f[this_freq_idx_lower_check]\n            l1, = ax5.plot(\n                [upper_freqs[0], upper_freqs[-1]],\n                [remaining_noise_thresh_upper, remaining_noise_thresh_upper],\n                color=grey\n            )\n            l2, = ax5.plot(\n                [lower_freqs[0], lower_freqs[-1]],\n                [remaining_noise_thresh_lower, remaining_noise_thresh_lower],\n                color=red\n            )\n            legend_labels = [\n                f\"{round(proportion_above_upper * 100, 2)}% above\",\n                f\"{round(proportion_below_lower * 100, 2)}% below\"\n            ]\n            ax5.legend([l1, l2], legend_labels, loc='upper center', edgecolor=[0.8, 0.8, 0.8])\n    except Exception as e:\n        print(\"Could not plot thresholds:\", e)\n\n    # Y-axis limits for ax5\n    if remaining_noise_thresh_lower is not None and remaining_noise_thresh_upper is not None and coarse_freq_detect_power_diff is not None:\n        ylim_lower = remaining_noise_thresh_lower - 0.25 * (remaining_noise_thresh_upper - remaining_noise_thresh_lower)\n        ylim_upper = np.min(np.mean(pxx_raw_log[this_freq_idx_plot, :], axis=1)) + coarse_freq_detect_power_diff * 2\n        ax5.set_ylim((float(ylim_lower), float(ylim_upper)))\n    else:\n        y_min = np.min(np.mean(pxx_clean_log[this_freq_idx_plot, :], axis=1))\n        y_max = np.max(np.mean(pxx_clean_log[this_freq_idx_plot, :], axis=1))\n        ax5.set_ylim((float(y_min - 0.25 * (y_max - y_min)), float(y_max + 0.25 * (y_max - y_min))))\n\n    ax5.set_xlabel('Frequency (Hz)')\n    ax5.set_ylabel('Power [10*log10 \u03bcV^2/Hz]')\n    ax5.set_title('Cleaned spectrum')\n    ax5.tick_params(labelsize=12)\n    ax5.spines['top'].set_visible(False)\n    ax5.spines['right'].set_visible(False)\n\n    # Plot starting spectrum on ax6\n    ax6.cla()\n    meanhandles, = ax6.plot(f, np.mean(pxx_raw_log, axis=1), color=grey, linewidth=1.5)\n    ax6.grid(True, which='both', linestyle='--', linewidth=0.5)\n    ax6.tick_params(labelsize=12)\n    ax6.set_xlabel('Frequency')\n    ax6.set_ylabel('Power [10*log10 \u03bcV^2/Hz]')\n    ylimits1 = ax6.get_ylim()\n    ratio_noise_raw = analytics.get('ratio_noise_raw', None)\n    ax6.set_title(f'Noise frequency: {noise_freq} Hz\\nRatio of noise to surroundings: {ratio_noise_raw:.2f}')\n    ax6.spines['top'].set_visible(False)\n    ax6.spines['right'].set_visible(False)\n\n    # Plot removed and clean spectrum on ax7\n    removedhandle, = ax7.plot(f / noise_freq, np.mean(pxx_removed_log, axis=1), color=red, linewidth=1.5)\n    cleanhandle, = ax7.plot(f / noise_freq, np.mean(pxx_clean_log, axis=1), color=green, linewidth=1.5)\n    ax7.grid(True, which='both', linestyle='--', linewidth=0.5)\n    ax7.tick_params(labelsize=12)\n    ax7.set_xlabel('Frequency (relative to noise)')\n    ax7.set_ylabel('')\n    ylimits2 = ax7.get_ylim()\n    ylimits = [min(ylimits1[0], ylimits2[0]), max(ylimits1[1], ylimits2[1])]\n    ax6.set_ylim(tuple(map(float, ylimits)))\n    ax7.set_ylim(tuple(map(float, ylimits)))\n    ax6.set_xlim((float(np.min(f) - np.max(f) * 0.0032), float(np.max(f))))\n    ax7.set_xlim((float(np.min(f / noise_freq) - np.max(f / noise_freq) * 0.003), float(np.max(f / noise_freq))))\n    proportion_removed_noise = analytics.get('proportion_removed_noise', None)\n    ratio_noise_clean = analytics.get('ratio_noise_clean', None)\n    ax7.set_title(f'Removed power at noise frequency: {proportion_removed_noise * 100:.2f}%\\nRatio of noise to surroundings: {ratio_noise_clean:.2f}')\n    ax7.spines['top'].set_visible(False)\n    ax7.spines['right'].set_visible(False)\n\n    # Plot shaded min/max frequency areas\n    minfreq = self.config.get('minfreq', None)\n    maxfreq = self.config.get('maxfreq', None)\n    if minfreq is not None and maxfreq is not None:\n        # Shade areas in ax6\n        ax6.add_patch(Rectangle((0, ylimits[0]), minfreq, ylimits[1] - ylimits[0], color='black', alpha=0.1))\n        ax6.add_patch(Rectangle((maxfreq, ylimits[0]), f[-1] - maxfreq, ylimits[1] - ylimits[0], color='black', alpha=0.1))\n        # Shade areas in ax7\n        ax7.add_patch(Rectangle((0, ylimits[0]), minfreq / noise_freq, ylimits[1] - ylimits[0], color='black', alpha=0.1))\n        ax7.add_patch(Rectangle((maxfreq / noise_freq, ylimits[0]), (f[-1] / noise_freq) - (maxfreq / noise_freq),\n                                ylimits[1] - ylimits[0], color='black', alpha=0.1))\n    ax6.legend([meanhandles], ['Raw data'], edgecolor=[0.8, 0.8, 0.8])\n    ax7.legend([cleanhandle, removedhandle], ['Clean data', 'Removed data'], edgecolor=[0.8, 0.8, 0.8])\n\n    # Plot below noise on ax8\n    this_freq_idx_belownoise = (f &gt;= max(noise_freq - 11, 0)) &amp; (f &lt;= noise_freq - 1)\n    ax8.plot(f[this_freq_idx_belownoise], np.mean(pxx_raw_log[this_freq_idx_belownoise, :], axis=1),\n            color=grey, linewidth=1.5)\n    ax8.plot(f[this_freq_idx_belownoise], np.mean(pxx_clean_log[this_freq_idx_belownoise, :], axis=1),\n            color=green, linewidth=1.5)\n    ax8.legend(['Raw data', 'Clean data'], edgecolor=[0.8, 0.8, 0.8])\n    ax8.grid(True, which='both', linestyle='--', linewidth=0.5)\n    ax8.tick_params(labelsize=12)\n    ax8.set_xlabel('Frequency')\n    ax8.spines['top'].set_visible(False)\n    ax8.spines['right'].set_visible(False)\n    ax8.set_xlim((float(np.min(f[this_freq_idx_belownoise])), float(np.max(f[this_freq_idx_belownoise]))))\n    proportion_removed = analytics.get('proportion_removed', None)\n    proportion_removed_below_noise = analytics.get('proportion_removed_below_noise', None)\n    ax8.set_title(f'Removed of full spectrum: {proportion_removed * 100:.2f}%\\nRemoved below noise: {proportion_removed_below_noise * 100:.2f}%')\n\n    plt.tight_layout()\n    plt.draw()\n    # Save figure to a standard location (figures/zapline_results.png)\n    out_dir = os.path.join(os.getcwd(), 'figures')\n    try:\n        os.makedirs(out_dir, exist_ok=True)\n    except Exception:\n        pass\n    plt.savefig(os.path.join(out_dir, 'zapline_results.png'))\n    return fig\n</code></pre>"},{"location":"api/core/#pyzaplineplus.core.PyZaplinePlus.iterative_outlier_removal","title":"<code>iterative_outlier_removal(data_vector, sd_level=3)</code>","text":"<p>Remove outliers in a vector based on an iterative sigma threshold approach.</p> Source code in <code>pyzaplineplus/core.py</code> <pre><code>def iterative_outlier_removal(self, data_vector, sd_level=3):\n    \"\"\"\n    Remove outliers in a vector based on an iterative sigma threshold approach.\n    \"\"\"\n    threshold_old: float = float(np.max(data_vector))\n    threshold: float = float(np.mean(data_vector) + sd_level * np.std(data_vector))\n    n_remove: int = 0\n\n    while threshold &lt; threshold_old:\n        flagged_points = data_vector &gt; threshold\n        data_vector = data_vector[~flagged_points]\n        n_remove += np.sum(flagged_points)\n        threshold_old = threshold\n        threshold = float(np.mean(data_vector) + sd_level * np.std(data_vector))\n\n    return n_remove, threshold\n</code></pre>"},{"location":"api/core/#pyzaplineplus.core.PyZaplinePlus.normalize_topcs","title":"<code>normalize_topcs(eigenvalues, topcs, threshold=None)</code>","text":"<p>Normalize and select top principal components based on a threshold.</p> <p>Parameters:     eigenvalues (np.ndarray): 1D array of eigenvalues.     topcs (np.ndarray): 2D array of top principal components (channels x PCs).     threshold (float, optional): Threshold value for selecting eigenvalues.</p> <p>Returns:     topcs_normalized (np.ndarray): Normalized top principal components.     eigenvalues_selected (np.ndarray): Selected eigenvalues after thresholding.</p> Source code in <code>pyzaplineplus/core.py</code> <pre><code>def normalize_topcs(self, eigenvalues, topcs, threshold=None):\n    \"\"\"\n    Normalize and select top principal components based on a threshold.\n\n    Parameters:\n        eigenvalues (np.ndarray): 1D array of eigenvalues.\n        topcs (np.ndarray): 2D array of top principal components (channels x PCs).\n        threshold (float, optional): Threshold value for selecting eigenvalues.\n\n    Returns:\n        topcs_normalized (np.ndarray): Normalized top principal components.\n        eigenvalues_selected (np.ndarray): Selected eigenvalues after thresholding.\n    \"\"\"\n    if threshold is not None and threshold &gt; 0:\n        # Ensure eigenvalues is 1D\n        if eigenvalues.ndim &gt; 1:\n            eigenvalues = np.diag(eigenvalues)\n\n        # Compute the ratio and create a boolean mask\n        ratio = eigenvalues / np.max(eigenvalues)\n        mask = ratio &gt; threshold  # Boolean array\n\n        # Debug statements (optional)\n        # print(f\"Eigenvalues: {eigenvalues}\")\n        # print(f\"Max Eigenvalue: {np.max(eigenvalues)}\")\n        # print(f\"Threshold: {threshold}\")\n        # print(f\"Ratio: {ratio}\")\n        # print(f\"Mask: {mask}\")\n\n        # Select indices where the condition is True\n        idx_thresh = np.where(mask)[0]\n\n        # Debug statement (optional)\n        # print(f\"Selected indices: {idx_thresh}\")\n\n        # Select columns in topcs based on idx_thresh\n        topcs_selected = topcs[:,idx_thresh]\n\n        # Select corresponding eigenvalues\n        eigenvalues_selected = eigenvalues[idx_thresh]\n\n        return topcs_selected, eigenvalues_selected\n    else:\n        # If no threshold is provided, return the original arrays\n        return topcs, eigenvalues\n</code></pre>"},{"location":"api/core/#pyzaplineplus.core.PyZaplinePlus.nt_bias_fft","title":"<code>nt_bias_fft(x, freq, nfft)</code>","text":"<p>Compute covariance matrices with and without filter bias using FFT.</p> <p>Parameters: - x (np.ndarray): Data matrix.     - 2D: (n_samples, n_channels)     - 3D: (n_samples, n_channels, n_trials) - freq (np.ndarray): Normalized frequencies to retain.     - 1D array: Individual frequencies.     - 2D array: Frequency bands with two rows (start and end frequencies). - nfft (int): FFT size.</p> <p>Returns: - c0 (np.ndarray): Unbiased covariance matrix. - c1 (np.ndarray): Biased covariance matrix after applying the frequency filter.</p> Source code in <code>pyzaplineplus/core.py</code> <pre><code>def nt_bias_fft(self, x: np.ndarray, freq: np.ndarray, nfft: int) -&gt; tuple:\n    \"\"\"\n    Compute covariance matrices with and without filter bias using FFT.\n\n    Parameters:\n    - x (np.ndarray): Data matrix.\n        - 2D: (n_samples, n_channels)\n        - 3D: (n_samples, n_channels, n_trials)\n    - freq (np.ndarray): Normalized frequencies to retain.\n        - 1D array: Individual frequencies.\n        - 2D array: Frequency bands with two rows (start and end frequencies).\n    - nfft (int): FFT size.\n\n    Returns:\n    - c0 (np.ndarray): Unbiased covariance matrix.\n    - c1 (np.ndarray): Biased covariance matrix after applying the frequency filter.\n    \"\"\"\n    from scipy.fft import fft\n    from scipy.signal import windows\n\n\n    # Input Validation\n    if np.max(freq) &gt; 0.5:\n        raise ValueError(\"Frequencies should be &lt;= 0.5\")\n    if nfft &gt; x.shape[0]:\n        raise ValueError(\"nfft too large\")\n\n    # Initialize Filter\n    filt: np.ndarray = np.zeros(nfft // 2 + 1)\n\n    if freq.ndim == 1:\n        for k in range(freq.shape[0]):\n            idx = int(np.floor(freq[k] * nfft + 0.5))\n            if idx &gt;= len(filt):\n                raise ValueError(f\"Frequency index {idx} out of bounds for filter of length {len(filt)}.\")\n            filt[idx] = 1\n    elif freq.shape[0] == 2:\n        for k in range(freq.shape[1]):\n            start_idx = int(np.floor(freq[0, k] * nfft + 0.5))\n            end_idx = int(np.floor(freq[1, k] * nfft + 0.5)) + 1\n            if start_idx &gt;= len(filt) or end_idx &gt; len(filt):\n                raise ValueError(f\"Frequency slice [{start_idx}:{end_idx}] out of bounds for filter of length {len(filt)}.\")\n            filt[start_idx:end_idx] = 1\n    else:\n        raise ValueError(\"freq should have one or two rows\")\n\n    # Symmetrize the Filter\n    # Build full-spectrum filter. Optionally force positive-bin-only for dev parity experiments.\n    if self.config.get('dss_positive_only', False) and freq.ndim == 1 and freq.size &gt; 0:\n        # Positive-bin-only mask: keep only exact positive bin indices, no mirrored negative bins\n        filt_full = np.zeros(nfft, dtype=float)\n        for k in range(freq.shape[0]):\n            idx = int(np.floor(freq[k] * nfft + 0.5))\n            filt_full[idx] = 1.0\n    else:\n        filt_full = np.concatenate([filt, np.flip(filt[1:-1])])\n\n    # Hann Window (symmetric to match MATLAB hanning(nfft))\n    w = windows.hann(nfft, sym=True)\n\n    # Handle 2D and 3D Data\n    if x.ndim == 2:\n        n_samples, n_channels = x.shape\n        n_trials = 1\n        x = x[:, :, np.newaxis]  # Convert to 3D for uniform processing\n    elif x.ndim == 3:\n        n_samples, n_channels, n_trials = x.shape\n    else:\n        raise ValueError(\"Input data `x` must be 2D or 3D.\")\n\n    # Compute c0: Unbiased Covariance Matrix\n    if n_trials &gt; 1:\n        x_reshaped = x.reshape(n_samples, n_channels * n_trials)\n    else:\n        x_reshaped = x.reshape(n_samples, n_channels)\n    c0,_= self.nt_cov(x_reshaped)\n\n    # Ensure c0 is 2D\n    if c0.ndim == 0:\n        c0 = np.array([[c0]])\n    elif c0.ndim == 1:\n        c0 = np.atleast_2d(c0)\n\n    # Initialize c1: Biased Covariance Matrix\n    c1 = np.zeros_like(c0)\n\n    # Calculate frame start indices (50% overlap). For MATLAB parity, restrict to\n    # full windows excluding the trailing partial pair so that frames = 2*floor(m/nfft)-3\n    # which matches observed behavior in zapline-plus' nt_bias_fft usage.\n    if self.config.get('dss_strict_frames', True):\n        windows_full = max(0, int(np.floor(n_samples / nfft) - 1))\n        max_start = int(max(0, windows_full * nfft))\n        starts = list(range(0, max_start + 1, nfft // 2))\n    else:\n        nframes = int(np.ceil((n_samples - nfft / 2) / (nfft / 2)))\n        starts = [int(min(k * (nfft // 2), n_samples - nfft)) for k in range(nframes)]\n\n    for trial in range(n_trials):\n        for idx in starts:\n            z = x[idx:idx + nfft, :, trial]  # (nfft, n_channels)\n            z = self.nt_vecmult(z,w)  # Apply Hann window\n            Z = fft(z, axis=0)\n            Z = self.nt_vecmult(Z, filt_full)  # Apply filter\n            cov_matrix = np.real(np.dot(Z.conj().T, Z))  # (n_channels, n_channels)\n            c1 += cov_matrix\n\n    # Optional window-energy normalization (dev parity control)\n    if self.config.get('dss_divide_by_sumw2', False):\n        from numpy import sum as npsum\n        denom = float(npsum(w**2))\n        if denom &gt; 0:\n            c1 = c1 / denom\n\n    return c0, c1\n</code></pre>"},{"location":"api/core/#pyzaplineplus.core.PyZaplinePlus.nt_cov","title":"<code>nt_cov(x, shifts=None, w=None)</code>","text":"<p>Calculate time-shifted covariance of the data.</p> <p>Parameters:     x (Union[np.ndarray, List[np.ndarray]]): Data matrix or list of matrices.         - If numeric:             - 1D: (n_samples,)             - 2D: (n_samples, n_channels)             - 3D: (n_samples, n_channels, n_trials)         - If list: list of 2D numpy arrays (cell array equivalent).     shifts (Union[List[int], np.ndarray]): Array-like of non-negative integer shifts.     w (Optional[Union[np.ndarray, List[np.ndarray]]]): Weights (optional).         - If numeric:             - 1D array for 1D or 2D <code>x</code>.             - 2D array for 3D <code>x</code>.         - If list: list of weight matrices corresponding to each cell.</p> <p>Returns:     Tuple[np.ndarray, float]: Covariance matrix and total weight.         - c: covariance matrix (numpy.ndarray) with shape (n_channels * nshifts, n_channels * nshifts).         - tw: total weight (float).</p> Source code in <code>pyzaplineplus/core.py</code> <pre><code>def nt_cov(\n    self,\n    x: Union[np.ndarray, List[np.ndarray]],\n    shifts: Optional[Union[List[int], np.ndarray]] = None,\n    w: Optional[Union[np.ndarray, List[np.ndarray]]] = None\n) -&gt; Tuple[np.ndarray, float]:\n    \"\"\"\n    Calculate time-shifted covariance of the data.\n\n    Parameters:\n        x (Union[np.ndarray, List[np.ndarray]]): Data matrix or list of matrices.\n            - If numeric:\n                - 1D: (n_samples,)\n                - 2D: (n_samples, n_channels)\n                - 3D: (n_samples, n_channels, n_trials)\n            - If list: list of 2D numpy arrays (cell array equivalent).\n        shifts (Union[List[int], np.ndarray]): Array-like of non-negative integer shifts.\n        w (Optional[Union[np.ndarray, List[np.ndarray]]]): Weights (optional).\n            - If numeric:\n                - 1D array for 1D or 2D `x`.\n                - 2D array for 3D `x`.\n            - If list: list of weight matrices corresponding to each cell.\n\n    Returns:\n        Tuple[np.ndarray, float]: Covariance matrix and total weight.\n            - c: covariance matrix (numpy.ndarray) with shape (n_channels * nshifts, n_channels * nshifts).\n            - tw: total weight (float).\n    \"\"\"\n    # Convert shifts to a NumPy array and flatten to 1D\n    if shifts is None:\n        shifts = np.array([0])\n    else:\n        shifts = np.asarray(shifts).flatten()\n    shifts = cast(np.ndarray, shifts)\n    if np.any(shifts &lt; 0):\n        raise ValueError(\"Shifts must be non-negative integers.\")\n    nshifts = len(shifts)\n\n    # Validate input data is not empty\n    if x is None or (isinstance(x, np.ndarray) and x.size == 0) or (isinstance(x, list) and len(x) == 0):\n        raise ValueError(\"Input data `x` is empty.\")\n\n    # Initialize covariance matrix and total weight\n    c = None\n    tw = 0.0\n\n    # Determine if input is a list (cell array) or numpy array\n    if isinstance(x, list):\n        # Handle list input (cell array equivalent)\n        if w is not None and not isinstance(w, list):\n            raise ValueError(\"Weights `w` must be a list if `x` is a list (cell array).\")\n\n        # Number of cells/trials not required explicitly here\n        # Determine number of channels\n        if len(x) == 0:\n            raise ValueError(\"Input list `x` is empty.\")\n        first_shape = x[0].shape\n        if first_shape == ():\n            n_channels = 1\n        elif x[0].ndim == 1:\n            n_channels = 1\n        elif x[0].ndim == 2:\n            n_channels = x[0].shape[1]\n        else:\n            raise ValueError(f\"Data in cell {0} has unsupported number of dimensions: {x[0].ndim}\")\n\n        c = np.zeros((n_channels * nshifts, n_channels * nshifts), dtype=np.float64)\n\n        for idx, data in enumerate(x):\n            # Validate data dimensions\n            if not isinstance(data, np.ndarray):\n                raise TypeError(f\"Element {idx} of input list `x` is not a numpy.ndarray.\")\n            if data.ndim != 2:\n                raise ValueError(f\"Data in cell {idx} must be 2D, got {data.ndim}D.\")\n            n_samples, n_channels_current = data.shape\n            if n_channels_current != n_channels:\n                raise ValueError(f\"All cells must have the same number of channels. Cell {idx} has {n_channels_current} channels, expected {n_channels}.\")\n\n            # Handle weights\n            if w is not None:\n                weight = w[idx]\n                if not isinstance(weight, np.ndarray):\n                    raise TypeError(f\"Weight for cell {idx} must be a numpy.ndarray.\")\n                if weight.size == 0:\n                    raise ValueError(f\"Weight for cell {idx} is empty.\")\n                if weight.ndim == 1:\n                    weight = weight[:, np.newaxis]  # Shape: (n_samples, 1)\n                elif weight.ndim == 2 and weight.shape[1] == 1:\n                    pass  # Shape is already (n_samples, 1)\n                else:\n                    raise ValueError(f\"Weight for cell {idx} must be 1D or 2D with a single column. Got shape {weight.shape}.\")\n            # Apply shifts\n            if not np.all(shifts == 0):\n                xx = self.nt_multishift(data, shifts)  # Shape: (n_samples * nshifts, n_channels)\n                if w is not None:\n                    ww = self.nt_multishift(weight, shifts)  # Shape: (n_samples * nshifts, 1)\n                    # Take the minimum weight across shifts for each sample\n                    ww_min = np.min(ww, axis=1, keepdims=True)  # Shape: (n_samples * nshifts, 1)\n                else:\n                    ww_min = np.ones((xx.shape[0], 1), dtype=np.float64)\n            else:\n                xx = data.copy()  # Shape: (n_samples, n_channels)\n                if w is not None:\n                    ww_min = weight.copy()  # Shape: (n_samples, 1)\n                else:\n                    ww_min = np.ones((xx.shape[0], 1), dtype=np.float64)\n\n            # Multiply each row by its corresponding weight\n            if w is not None:\n                xx = self.nt_vecmult(xx, ww_min)  # Shape: (time_shifted x channels)\n\n            # Accumulate covariance\n            c += np.dot(xx.T, xx)  # Shape: (n_channels * nshifts, n_channels * nshifts)\n\n            # Accumulate total weight\n            if w is not None:\n                if not np.all(shifts == 0):\n                    tw += np.sum(ww_min)\n                else:\n                    tw += np.sum(weight)\n            else:\n                tw += xx.shape[0]  # Number of samples\n\n    elif isinstance(x, np.ndarray):\n        # Handle NumPy array input\n        data = x.copy()\n        # original_shape not used; keep minimal state\n\n        # Determine data dimensionality\n        if data.ndim == 1:\n            data = data[:, np.newaxis]  # Shape: (n_samples, 1)\n            n_channels = 1\n            n_trials = 1\n        elif data.ndim == 2:\n            n_channels = data.shape[1]\n            n_trials = 1\n        elif data.ndim == 3:\n            n_channels = data.shape[1]\n            n_trials = data.shape[2]\n        else:\n            raise ValueError(f\"Input data has unsupported number of dimensions: {data.ndim}\")\n\n        # Preallocate covariance matrix\n        c = np.zeros((n_channels * nshifts, n_channels * nshifts), dtype=np.float64)\n\n        # Iterate over trials\n        for trial in range(n_trials):\n            if data.ndim == 3:\n                trial_data = data[:, :, trial]  # Shape: (n_samples, n_channels)\n                if w is not None:\n                    if not isinstance(w, np.ndarray):\n                        raise TypeError(\"Weights `w` must be a numpy.ndarray when `x` is a numpy.ndarray.\")\n                    trial_weight = w[:, trial]  # Shape: (n_samples,)\n            else:\n                trial_data = data.copy()  # Shape: (n_samples, n_channels)\n                if w is not None:\n                    if not isinstance(w, np.ndarray):\n                        raise TypeError(\"Weights `w` must be a numpy.ndarray when `x` is a numpy.ndarray.\")\n                    trial_weight = w.copy()  # Shape: (n_samples, 1)\n\n            # Apply shifts\n            if not np.all(shifts == 0):\n                xx = self.nt_multishift(trial_data, shifts)  # Shape: (n_samples * nshifts, n_channels)\n                if w is not None:\n                    ww = self.nt_multishift(trial_weight, shifts)  # Shape: (n_samples * nshifts, 1)\n                    # Take the minimum weight across shifts for each sample\n                    ww_min = np.min(ww, axis=1, keepdims=True)  # Shape: (n_samples * nshifts, 1)\n                else:\n                    ww_min = np.ones((xx.shape[0], 1), dtype=np.float64)\n            else:\n                xx = trial_data.copy()  # Shape: (n_samples, n_channels)\n                if w is not None:\n                    ww_min = trial_weight.copy()  # Shape: (n_samples, 1)\n                else:\n                    ww_min = np.ones((xx.shape[0], 1), dtype=np.float64)\n\n            # Multiply each row by its corresponding weight\n            if w is not None:\n                xx = self.nt_vecmult(xx, ww_min)  # Shape: (time_shifted x channels)\n\n            # Accumulate covariance\n            c += np.dot(xx.T, xx)  # Shape: (n_channels * nshifts, n_channels * nshifts)\n\n            # Accumulate total weight\n            if w is not None:\n                if not np.all(shifts == 0):\n                    tw += np.sum(ww_min)\n                else:\n                    tw += np.sum(trial_weight)\n            else:\n                tw += xx.shape[0]  # Number of samples\n\n    else:\n        raise TypeError(\"Input `x` must be a numpy.ndarray or a list of numpy.ndarray (cell array).\")\n\n    return c, tw\n</code></pre>"},{"location":"api/core/#pyzaplineplus.core.PyZaplinePlus.nt_demean","title":"<code>nt_demean(x, w=None)</code>","text":"<p>Remove weighted mean over columns.</p> <p>Parameters:     x (np.ndarray): Data array (time x channels x trials).     w (np.ndarray): Optional weights array (time x 1 x trials) or (time x channels x trials).</p> <p>Returns:     x_demeaned (np.ndarray): Demeaned data array.     mn (np.ndarray): Mean values that were removed.</p> Source code in <code>pyzaplineplus/core.py</code> <pre><code>def nt_demean(self, x, w=None):\n    \"\"\"\n    Remove weighted mean over columns.\n\n    Parameters:\n        x (np.ndarray): Data array (time x channels x trials).\n        w (np.ndarray): Optional weights array (time x 1 x trials) or (time x channels x trials).\n\n    Returns:\n        x_demeaned (np.ndarray): Demeaned data array.\n        mn (np.ndarray): Mean values that were removed.\n    \"\"\"\n    added_trial_dim = False\n    if x.ndim == 2:\n        x = x[:, :, np.newaxis]\n        added_trial_dim = True\n        # Only expand w if provided\n        if w is not None and w.ndim == 2:\n            w = w[:, :, np.newaxis]\n\n    # Interpret index-form weights if provided and smaller than time dimension\n    if w is not None and w.size &lt; x.shape[0]:\n        w_indices = w.flatten()\n        if np.min(w_indices) &lt; 0 or np.max(w_indices) &gt;= x.shape[0]:\n            raise ValueError('w interpreted as indices but values are out of range')\n        w_full = np.zeros((x.shape[0], 1, x.shape[2]))\n        w_full[w_indices, :, :] = 1\n        w = w_full\n\n    # Broadcast weights across trials if needed\n    if w is not None and w.ndim == 3 and w.shape[2] != x.shape[2]:\n        if w.shape[2] == 1 and x.shape[2] != 1:\n            w = np.tile(w, (1, 1, x.shape[2]))\n        else:\n            raise ValueError('w should have same number of trials as x, or be singleton in that dimension')\n\n    m, n, o = x.shape\n    x_unfolded = x.reshape(m, -1)  # Unfold x to 2D array (time x (channels*trials))\n\n    if w is None:\n        # Unweighted mean\n        mn = np.mean(x_unfolded, axis=0, keepdims=True)\n        x_demeaned = x_unfolded - mn\n    else:\n        # Ensure weights have shape (time, channels*trials)\n        if w.ndim == 3:\n            w_unfolded = w.reshape(m, -1)\n        elif w.ndim == 2:\n            # If provided as (time, 1), tile across channels*trials\n            if w.shape[1] == 1:\n                w_unfolded = np.tile(w, (1, x_unfolded.shape[1]))\n            else:\n                if w.shape[1] != x_unfolded.shape[1]:\n                    raise ValueError('Weight matrix should match unfolded data width')\n                w_unfolded = w\n        else:\n            raise ValueError('Weight matrix has invalid dimensions')\n\n        if w_unfolded.shape[0] != x_unfolded.shape[0]:\n            raise ValueError('x and w should have the same number of time samples')\n\n        sum_w = np.sum(w_unfolded, axis=0, keepdims=True) + np.finfo(float).eps\n        mn = np.sum(x_unfolded * w_unfolded, axis=0, keepdims=True) / sum_w\n        x_demeaned = x_unfolded - mn\n\n    x_demeaned = x_demeaned.reshape(m, n, o)\n    mn = mn.reshape(1, n, o)\n    if added_trial_dim:\n        x_demeaned = x_demeaned.squeeze(-1)  # Remove the last dimension if singleton\n        mn = mn.squeeze(-1)\n    return x_demeaned, mn\n</code></pre>"},{"location":"api/core/#pyzaplineplus.core.PyZaplinePlus.nt_dss0","title":"<code>nt_dss0(c0, c1, keep1=None, keep2=10 ** -9)</code>","text":"<p>Compute DSS from covariance matrices.</p> <p>Parameters: - c0: baseline covariance - c1: biased covariance - keep1: number of PCs to retain (default: all) - keep2: ignore PCs smaller than this threshold (default: 10^-9)</p> <p>Returns: - todss: matrix to convert data to normalized DSS components - pwr0: power per component (baseline) - pwr1: power per component (biased)</p> Source code in <code>pyzaplineplus/core.py</code> <pre><code>def nt_dss0(self, c0, c1, keep1=None, keep2=10**-9):\n    \"\"\"\n    Compute DSS from covariance matrices.\n\n    Parameters:\n    - c0: baseline covariance\n    - c1: biased covariance\n    - keep1: number of PCs to retain (default: all)\n    - keep2: ignore PCs smaller than this threshold (default: 10^-9)\n\n    Returns:\n    - todss: matrix to convert data to normalized DSS components\n    - pwr0: power per component (baseline)\n    - pwr1: power per component (biased)\n    \"\"\"\n    if c0.shape != c1.shape:\n        raise ValueError(\"c0 and c1 should have the same size\")\n    if c0.shape[0] != c0.shape[1]:\n        raise ValueError(\"c0 should be square\")\n    if np.any(np.isnan(c0)) or np.any(np.isnan(c1)):\n        raise ValueError(\"NaN in covariance matrices\")\n    if np.any(np.isinf(c0)) or np.any(np.isinf(c1)):\n        raise ValueError(\"INF in covariance matrices\")\n\n    # PCA and whitening matrix from the unbiased covariance\n    topcs1, evs1 = self.nt_pcarot(c0, keep1, keep2)\n    # Match MATLAB nt_dss0.m: take absolute eigenvalues before whitening\n    evs1 = np.abs(evs1)\n\n    # Truncate PCA series if needed (thresholding already handled inside nt_pcarot)\n    if keep1 is not None:\n        topcs1 = topcs1[:, :keep1]\n        evs1 = evs1[:keep1]\n\n    # Apply PCA and whitening to the biased covariance\n    evs1_sqrt=1.0 / np.sqrt(evs1)\n    N = np.diag(evs1_sqrt)\n    c2 = N.T @ topcs1.T @ c1 @ topcs1 @ N\n\n    # Matrix to convert PCA-whitened data to DSS\n    topcs2, evs2 = self.nt_pcarot(c2, keep1, keep2)\n\n    # DSS matrix (raw data to normalized DSS)\n    todss = topcs1 @ N @ topcs2\n    N2 = np.diag(todss.T @ c0 @ todss)\n    todss = todss @ np.diag(1.0 / np.sqrt(N2))  # Adjust so that components are normalized\n\n    # Power per DSS component (match MATLAB's nt_dss0.m formulation)\n    # pwr0 = sqrt(sum((c0' * todss).^2)), pwr1 = sqrt(sum((c1' * todss).^2))\n    P0 = c0.T @ todss\n    P1 = c1.T @ todss\n    pwr0 = np.sqrt(np.sum(P0 ** 2, axis=0))\n    pwr1 = np.sqrt(np.sum(P1 ** 2, axis=0))\n\n    return todss, pwr0, pwr1\n</code></pre>"},{"location":"api/core/#pyzaplineplus.core.PyZaplinePlus.nt_fold","title":"<code>nt_fold(x, epochsize)</code>","text":"<p>Converts a 2D matrix (time*trial x channel) back into a 3D matrix (time x channel x trial).</p> <p>Parameters:     x (np.ndarray): Input data. Should be 2D.     epochsize (int): Number of samples per trial.</p> <p>Returns:     y (np.ndarray): Folded data.</p> Source code in <code>pyzaplineplus/core.py</code> <pre><code>def nt_fold(self,x, epochsize):\n    \"\"\"\n    Converts a 2D matrix (time*trial x channel) back into a 3D matrix (time x channel x trial).\n\n    Parameters:\n        x (np.ndarray): Input data. Should be 2D.\n        epochsize (int): Number of samples per trial.\n\n    Returns:\n        y (np.ndarray): Folded data.\n    \"\"\"\n    if x.size == 0:\n        return np.array([])\n    else:\n        if x.shape[0] / epochsize &gt; 1:\n            trials = int(x.shape[0] / epochsize)\n            y = np.transpose(np.reshape(x, (epochsize, trials, x.shape[1])), (0, 2, 1))\n        else:\n            y = x\n    return y\n</code></pre>"},{"location":"api/core/#pyzaplineplus.core.PyZaplinePlus.nt_mmat","title":"<code>nt_mmat(x, m)</code>","text":"<p>Matrix multiplication (with convolution).</p> <p>Parameters:     x (np.ndarray): Input data (can be 2D or multi-dimensional).     m (np.ndarray): Matrix to apply.                     - If 2D: Right multiply x by m.                     - If 3D: Perform convolution-like operation along time.</p> <p>Returns:     y (np.ndarray): Result after applying m to x.</p> Source code in <code>pyzaplineplus/core.py</code> <pre><code>def nt_mmat(self, x, m):\n    \"\"\"\n    Matrix multiplication (with convolution).\n\n    Parameters:\n        x (np.ndarray): Input data (can be 2D or multi-dimensional).\n        m (np.ndarray): Matrix to apply.\n                        - If 2D: Right multiply x by m.\n                        - If 3D: Perform convolution-like operation along time.\n\n    Returns:\n        y (np.ndarray): Result after applying m to x.\n    \"\"\"\n    # Handle the case where x is a list (similar to cell arrays in MATLAB)\n    if isinstance(x, list):\n        return [self.nt_mmat(xi, m) for xi in x]\n\n    # Handle multi-dimensional x beyond 3D\n    if x.ndim &gt; 3:\n        # Reshape x to 3D (time x channels x combined other dimensions)\n        original_shape = x.shape\n        time_dim = original_shape[0]\n        chan_dim = original_shape[1]\n        other_dims = original_shape[2:]\n        x = x.reshape(time_dim, chan_dim, -1)\n        y = self.nt_mmat(x, m)\n        # Reshape y back to original dimensions\n        y_shape = (y.shape[0], y.shape[1]) + other_dims\n        y = y.reshape(y_shape)\n        return y\n\n    # If m is 2D, perform simple matrix multiplication using\n    if m.ndim == 2:\n        y = self.nt_mmat0(x, m)\n\n        # Ensure y has the correct shape by removing any singleton dimensions\n        if y.ndim == 3 and y.shape[2] == 1:\n            y = y.squeeze(-1)\n        elif y.ndim == 2:\n            pass  # Already correct\n        else:\n            # Handle unexpected dimensions\n            y = np.squeeze(y)\n\n        return y\n\n    else:\n        # Convolution-like operation when m is 3D\n        n_rows, n_cols, n_lags = m.shape\n\n        if x.ndim == 2:\n            x = x[:, :, np.newaxis]  # Add trial dimension\n        n_samples, n_chans, n_trials = x.shape\n\n        if n_chans != n_rows:\n            raise ValueError(\"Number of channels in x must match number of rows in m.\")\n\n        # Initialize output array\n        y = np.zeros((n_samples + n_lags - 1, n_cols, n_trials))\n\n        # Perform convolution-like operation\n        for trial in range(n_trials):\n            x_trial = x[:, :, trial]  # Shape: (n_samples, n_chans)\n            y_trial = np.zeros((n_samples + n_lags - 1, n_cols))\n\n            # Unfold x_trial to 2D\n            x_unfolded = x_trial  # Shape: (n_samples, n_chans)\n\n            for lag in range(n_lags):\n                m_lag = m[:, :, lag]  # Shape: (n_rows, n_cols)\n                # Shift x_trial by lag\n                x_shifted = np.zeros((n_samples + n_lags - 1, n_chans))\n                x_shifted[lag:lag + n_samples, :] = x_unfolded\n\n                # Multiply and accumulate\n                y_partial = x_shifted @ m_lag  # Shape: (n_samples + n_lags - 1, n_cols)\n                y_trial += y_partial\n\n            y[:, :, trial] = y_trial\n\n        # If trials dimension was added artificially, remove it\n        if n_trials == 1:\n            y = y.squeeze(-1)  # Remove the last dimension if singleton\n\n        return y\n</code></pre>"},{"location":"api/core/#pyzaplineplus.core.PyZaplinePlus.nt_mmat0","title":"<code>nt_mmat0(x, m)</code>","text":"<p>Performs matrix multiplication after unfolding x, then folds the result back.</p> <p>Parameters:     x (np.ndarray): Input data. Can be 2D or 3D.     m (np.ndarray): Matrix to multiply with. Should be 2D.</p> <p>Returns:     y (np.ndarray): Result after multiplication and folding.</p> Source code in <code>pyzaplineplus/core.py</code> <pre><code>def nt_mmat0(self,x, m):\n    \"\"\"\n    Performs matrix multiplication after unfolding x, then folds the result back.\n\n    Parameters:\n        x (np.ndarray): Input data. Can be 2D or 3D.\n        m (np.ndarray): Matrix to multiply with. Should be 2D.\n\n    Returns:\n        y (np.ndarray): Result after multiplication and folding.\n    \"\"\"\n    unfolded_x = self.nt_unfold(x)\n    multiplied = unfolded_x @ m\n    epochsize = x.shape[0]   # Assuming epochsize is the first dimension\n    folded_y = self.nt_fold(multiplied, epochsize)\n    return folded_y\n</code></pre>"},{"location":"api/core/#pyzaplineplus.core.PyZaplinePlus.nt_multishift","title":"<code>nt_multishift(x, shifts)</code>","text":"<p>Apply multiple shifts to a matrix.</p> <p>Parameters: x (numpy.ndarray): Input data to shift. Shape can be 1D, 2D, or 3D.                     - 1D: (samples,)                     - 2D: (samples, channels)                     - 3D: (samples, channels, trials) shifts (array-like): Array of non-negative integer shifts.</p> <p>Returns: numpy.ndarray: Shifted data with increased channel dimension.             - 1D input becomes 2D: (samples_shifted, shifts.size)             - 2D input becomes 3D: (samples_shifted, channels * shifts.size, trials)</p> Source code in <code>pyzaplineplus/core.py</code> <pre><code>def nt_multishift(self,x, shifts):\n    \"\"\"\n    Apply multiple shifts to a matrix.\n\n    Parameters:\n    x (numpy.ndarray): Input data to shift. Shape can be 1D, 2D, or 3D.\n                        - 1D: (samples,)\n                        - 2D: (samples, channels)\n                        - 3D: (samples, channels, trials)\n    shifts (array-like): Array of non-negative integer shifts.\n\n    Returns:\n    numpy.ndarray: Shifted data with increased channel dimension.\n                - 1D input becomes 2D: (samples_shifted, shifts.size)\n                - 2D input becomes 3D: (samples_shifted, channels * shifts.size, trials)\n    \"\"\"\n    x = np.asarray(x)\n    shifts = np.asarray(shifts).flatten()\n    nshifts = shifts.size\n\n    # Input validation\n    if np.any(shifts &lt; 0):\n        raise ValueError('Shifts should be non-negative')\n    if x.shape[0] &lt; np.max(shifts):\n        raise ValueError('Shifts should be no larger than the number of time samples in x')\n\n    # Handle different input dimensions by expanding to 3D\n    if x.ndim == 1:\n        x = x[:, np.newaxis, np.newaxis]  # (samples, 1, 1)\n    elif x.ndim == 2:\n        x = x[:, :, np.newaxis]  # (samples, channels, 1)\n    elif x.ndim &gt; 3:\n        raise ValueError('Input data has more than 3 dimensions, which is not supported.')\n\n    m, n, o = x.shape  # samples x channels x trials\n\n    # If only one shift and it's zero, return the original data\n    if nshifts == 1 and shifts[0] == 0:\n        return x.squeeze(axis=-1)\n\n    max_shift: int = int(np.max(shifts))\n    N: int = m - max_shift  # Number of samples after shifting\n\n    # Initialize output array\n    z = np.empty((N, n * nshifts, o), dtype=x.dtype)\n\n    for trial in range(o):\n        for channel in range(n):\n            y = x[:, channel, trial]  # (samples,)\n            for s_idx, shift in enumerate(shifts):\n                if shift == 0:\n                    shifted_y = y[:N]\n                else:\n                    shifted_y = y[shift:shift + N]\n                # Place the shifted data in the correct position\n                z[:, channel * nshifts + s_idx, trial] = shifted_y\n\n    return z.squeeze(axis=-1)\n</code></pre>"},{"location":"api/core/#pyzaplineplus.core.PyZaplinePlus.nt_normcol","title":"<code>nt_normcol(x, w=None)</code>","text":"<p>Normalize each column so its weighted mean square is 1.</p> <p>Parameters:     x (np.ndarray): Data array (time x channels x trials).     w (np.ndarray): Optional weights array with same dimensions as x or (time x 1 x trials).</p> <p>Returns:     y (np.ndarray): Normalized data array.     norms (np.ndarray): Vector of norms used for normalization.</p> Source code in <code>pyzaplineplus/core.py</code> <pre><code>def nt_normcol(self, x, w=None):\n    \"\"\"\n    Normalize each column so its weighted mean square is 1.\n\n    Parameters:\n        x (np.ndarray): Data array (time x channels x trials).\n        w (np.ndarray): Optional weights array with same dimensions as x or (time x 1 x trials).\n\n    Returns:\n        y (np.ndarray): Normalized data array.\n        norms (np.ndarray): Vector of norms used for normalization.\n    \"\"\"\n    if x.size == 0:\n        raise ValueError('Empty x')\n\n    if isinstance(x, list):\n        raise NotImplementedError('Weights not supported for list inputs')\n\n    if x.ndim == 4:\n        # Apply normcol to each \"book\" (4th dimension)\n        m, n, o, p = x.shape\n        y = np.zeros_like(x)\n        N = np.zeros(n)\n        for k in range(p):\n            y[:, :, :, k], NN = self.nt_normcol(x[:, :, :, k])\n            N += NN ** 2\n        return y, np.sqrt(N)\n\n    if x.ndim == 3:\n        # Unfold data to 2D\n        m, n, o = x.shape\n        x_unfolded = x.reshape(m * o, n)\n        if w is None:\n            y_unfolded, N = self.nt_normcol(x_unfolded)\n        else:\n            if w.shape[0] != m:\n                raise ValueError('Weight matrix should have same number of time samples as data')\n            if w.ndim == 2 and w.shape[1] == 1:\n                w = np.tile(w, (1, n, o))\n            w_unfolded = w.reshape(m * o, n)\n            y_unfolded, N = self.nt_normcol(x_unfolded, w_unfolded)\n        y = y_unfolded.reshape(m, n, o)\n\n        norms = np.sqrt(N)\n        return y, norms\n\n    elif x.ndim == 2:\n        # 2D data\n        m, n = x.shape\n        if w is None:\n            # No weight\n            N = np.sum(x ** 2, axis=0) / m\n            N_inv_sqrt = np.where(N == 0, 0, 1.0 / np.sqrt(N))\n            y = x * N_inv_sqrt\n        else:\n            if w.shape[0] != x.shape[0]:\n                raise ValueError('Weight matrix should have same number of time samples as data')\n            if w.ndim == 1 or (w.ndim == 2 and w.shape[1] == 1):\n                w = np.tile(w.reshape(-1, 1), (1, n))\n            if w.shape != x.shape:\n                raise ValueError('Weight should have same shape as data')\n            sum_w = np.sum(w, axis=0)\n            N = np.sum((x ** 2) * w, axis=0) / (sum_w + np.finfo(float).eps)\n            N_inv_sqrt = np.where(N == 0, 0, 1.0 / np.sqrt(N))\n            y = x * N_inv_sqrt\n        norms = np.sqrt(N)\n        return y, norms\n    else:\n        raise ValueError('Input data must be 2D or 3D')\n</code></pre>"},{"location":"api/core/#pyzaplineplus.core.PyZaplinePlus.nt_pca","title":"<code>nt_pca(x, shifts=None, nkeep=None, threshold=0, w=None)</code>","text":"<p>Apply PCA with time shifts and retain a specified number of components.</p> <p>Parameters: - x: data matrix (n_samples, n_channels) or list of arrays for cell-like data - shifts: array of shifts to apply (default: [0]) - nkeep: number of components to keep (default: all) - threshold: discard PCs with eigenvalues below this (default: 0) - w: weights (optional)     - If x is numeric: w can be 1D (n_samples,) or 2D (n_samples, n_channels)     - If x is a list: w should be a list of arrays matching x's structure</p> <p>Returns: - z: principal components     - If x is numeric: numpy.ndarray of shape (numel(idx), PCs, trials)     - If x is a list: list of numpy.ndarrays, each of shape (numel(idx), PCs) - idx: indices of x that map to z</p> Source code in <code>pyzaplineplus/core.py</code> <pre><code>def nt_pca(self, x, shifts=None, nkeep=None, threshold=0, w=None):\n    \"\"\"\n    Apply PCA with time shifts and retain a specified number of components.\n\n    Parameters:\n    - x: data matrix (n_samples, n_channels) or list of arrays for cell-like data\n    - shifts: array of shifts to apply (default: [0])\n    - nkeep: number of components to keep (default: all)\n    - threshold: discard PCs with eigenvalues below this (default: 0)\n    - w: weights (optional)\n        - If x is numeric: w can be 1D (n_samples,) or 2D (n_samples, n_channels)\n        - If x is a list: w should be a list of arrays matching x's structure\n\n    Returns:\n    - z: principal components\n        - If x is numeric: numpy.ndarray of shape (numel(idx), PCs, trials)\n        - If x is a list: list of numpy.ndarrays, each of shape (numel(idx), PCs)\n    - idx: indices of x that map to z\n    \"\"\"\n\n    # Ensure shifts is a numpy array\n    if shifts is None:\n        shifts = np.array([0])\n    else:\n        shifts = np.array(shifts).flatten()\n        if len(shifts) == 0:\n            shifts = np.array([0])\n    if np.any(shifts &lt; 0):\n        raise ValueError(\"All shifts must be non-negative.\")\n\n    # Adjust shifts to make them non-negative\n    min_shift: int = int(np.min(shifts))\n    offset = max(0, -min_shift)\n    shifts = shifts + offset\n    idx = offset + np.arange(x.shape[0] - max(shifts))  # x[idx] maps to z\n    # Determine if x is numeric or list (cell-like)\n    if isinstance(x, list):\n        o = len(x)\n        if o == 0:\n            raise ValueError(\"Input list 'x' is empty.\")\n        m, n = x[0].shape\n        if w is not None and not isinstance(w, list):\n            raise ValueError(\"Weights 'w' must be a list when 'x' is a list.\")\n        tw = 0\n        # Compute covariance\n        c, tw = self.nt_cov(x, shifts, w)\n    elif isinstance(x, np.ndarray):\n        if x.ndim not in [2, 3]:\n            raise ValueError(\"Input 'x' must be a 2D or 3D numpy.ndarray or a list of 2D arrays.\")\n        m, n = x.shape[:2]\n        o = x.shape[2] if x.ndim == 3 else 1\n        c, tw = self.nt_cov(x, shifts, w)\n    else:\n        raise TypeError(\"Input 'x' must be a numpy.ndarray or a list of numpy.ndarrays.\")\n\n    # Perform PCA\n    topcs, evs = self.nt_pcarot(c, nkeep, threshold)\n\n    # Apply PCA matrix to time-shifted data\n    if isinstance(x, list):\n        z = []\n        for k in range(o):\n            shifted = self.nt_multishift(x[k], shifts)  # Shape: (numel(idx), n * nshifts)\n            # Project onto PCA components\n            z_k = np.dot(shifted, topcs)\n            z.append(z_k)\n    elif isinstance(x, np.ndarray):\n        if x.ndim == 2:\n            shifted = self.nt_multishift(x, shifts)  # Shape: (numel(idx), n * nshifts)\n            z = shifted @ topcs # Shape: (numel(idx), PCs)\n        elif x.ndim == 3:\n            z = np.zeros((len(idx), topcs.shape[1], o))\n            for k in range(o):\n                shifted = self.nt_multishift(x[:, :, k], shifts)  # Shape: (numel(idx), n * nshifts)\n                shifted = shifted.reshape(-1, 1)  # Shape becomes (18000, 1)\n                z[:, :, k] = np.dot(shifted, topcs)  # Shape: (numel(idx), PCs)\n    else:\n        # This case should have been handled earlier\n        z = None\n\n    return z, idx\n</code></pre>"},{"location":"api/core/#pyzaplineplus.core.PyZaplinePlus.nt_pcarot","title":"<code>nt_pcarot(cov, nkeep=None, threshold=None, N=None)</code>","text":"<p>Calculate PCA rotation matrix from covariance matrix.</p> <p>Parameters: - cov (numpy.ndarray): Covariance matrix (symmetric, positive semi-definite). - nkeep (int, optional): Number of principal components to keep. - threshold (float, optional): Discard components with eigenvalues below this fraction of the largest eigenvalue. - N (int, optional): Number of top eigenvalues and eigenvectors to compute.</p> <p>Returns: - topcs (numpy.ndarray): PCA rotation matrix (eigenvectors), shape (n_features, n_components). - eigenvalues (numpy.ndarray): PCA eigenvalues, shape (n_components,).</p> Source code in <code>pyzaplineplus/core.py</code> <pre><code>def nt_pcarot(self, cov, nkeep=None, threshold=None, N=None):\n    \"\"\"\n    Calculate PCA rotation matrix from covariance matrix.\n\n    Parameters:\n    - cov (numpy.ndarray): Covariance matrix (symmetric, positive semi-definite).\n    - nkeep (int, optional): Number of principal components to keep.\n    - threshold (float, optional): Discard components with eigenvalues below this fraction of the largest eigenvalue.\n    - N (int, optional): Number of top eigenvalues and eigenvectors to compute.\n\n    Returns:\n    - topcs (numpy.ndarray): PCA rotation matrix (eigenvectors), shape (n_features, n_components).\n    - eigenvalues (numpy.ndarray): PCA eigenvalues, shape (n_components,).\n    \"\"\"\n    from scipy.sparse.linalg import eigsh\n    from scipy.linalg import eigh\n\n    # Validate covariance matrix\n    if not isinstance(cov, np.ndarray):\n        raise TypeError(\"Covariance matrix 'cov' must be a numpy.ndarray.\")\n    if cov.ndim != 2 or cov.shape[0] != cov.shape[1]:\n        raise ValueError(\"Covariance matrix 'cov' must be a square (2D) array.\")\n\n    n_features = cov.shape[0]\n\n    # Compute eigenvalues and eigenvectors\n    if N is not None:\n        if not isinstance(N, int) or N &lt;= 0:\n            raise ValueError(\"'N' must be a positive integer.\")\n        N = min(N, n_features)\n        eigenvalues_all, eigenvectors_all = eigsh(cov, k=N, which='LM')  # 'LM' selects largest magnitude eigenvalues\n        # Ensure real parts\n        eigenvalues_all = np.real(eigenvalues_all)\n        eigenvectors_all = np.real(eigenvectors_all)\n    else:\n        eigenvalues_all, eigenvectors_all = eigh(cov)\n        # Ensure real parts\n        eigenvalues_all = np.real(eigenvalues_all)\n        eigenvectors_all = np.real(eigenvectors_all)\n        # Reverse to descending order\n        eigenvalues_all = eigenvalues_all[::-1]\n        eigenvectors_all = eigenvectors_all[:, ::-1]\n        # Covariance is positive semidefinite; keep as-is for true MATLAB parity\n        # positive_idx = eigenvalues_all &gt; 0\n        # eigenvalues_all = eigenvalues_all[positive_idx]\n        # eigenvectors_all = eigenvectors_all[:, positive_idx]\n\n    # Define a small tolerance to handle numerical precision issues (optional)\n\n\n    # Select top N eigenvalues and eigenvectors if N is specified and not already done\n    if N is None:\n        eigenvalues = eigenvalues_all\n        eigenvectors = eigenvectors_all\n    else:\n        eigenvalues = eigenvalues_all\n        eigenvectors = eigenvectors_all\n\n    # Apply threshold\n    if threshold is not None:\n        if eigenvalues[0] == 0:\n            raise ValueError(\"The largest eigenvalue is zero; cannot apply threshold.\")\n        valid_indices = np.where(eigenvalues / eigenvalues[0] &gt; threshold)[0]\n        eigenvalues = eigenvalues[valid_indices]\n        eigenvectors = eigenvectors[:, valid_indices]\n\n    # Apply nkeep\n    if nkeep is not None:\n        if not isinstance(nkeep, int) or nkeep &lt;= 0:\n            raise ValueError(\"'nkeep' must be a positive integer.\")\n        nkeep = min(nkeep, eigenvectors.shape[1])\n        eigenvalues = eigenvalues[:nkeep]\n        eigenvectors = eigenvectors[:, :nkeep]\n\n    topcs = eigenvectors\n    return topcs, eigenvalues\n</code></pre>"},{"location":"api/core/#pyzaplineplus.core.PyZaplinePlus.nt_regcov","title":"<code>nt_regcov(cxy, cyy, keep=None, threshold=0)</code>","text":"<p>Compute regression matrix from cross-covariance matrices.</p> <p>Parameters:     cxy (np.ndarray): Cross-covariance matrix between data and regressor.     cyy (np.ndarray): Covariance matrix of regressor.     keep (int): Number of regressor PCs to keep (default: all).     threshold (float): Eigenvalue threshold for discarding regressor PCs (default: 0).</p> <p>Returns:     r (np.ndarray): Regression matrix to apply to regressor to best model data.</p> Source code in <code>pyzaplineplus/core.py</code> <pre><code>def nt_regcov(self, cxy, cyy, keep=None, threshold=0):\n    \"\"\"\n    Compute regression matrix from cross-covariance matrices.\n\n    Parameters:\n        cxy (np.ndarray): Cross-covariance matrix between data and regressor.\n        cyy (np.ndarray): Covariance matrix of regressor.\n        keep (int): Number of regressor PCs to keep (default: all).\n        threshold (float): Eigenvalue threshold for discarding regressor PCs (default: 0).\n\n    Returns:\n        r (np.ndarray): Regression matrix to apply to regressor to best model data.\n    \"\"\"\n    # PCA of regressor covariance matrix\n    topcs,eigenvalues = self.nt_pcarot(cyy)\n\n\n    # Discard negligible regressor PCs\n    if keep is not None:\n        # Keep at most `keep` components, not more\n        keep = min(keep, topcs.shape[1])\n        topcs = topcs[:, :keep]\n        eigenvalues = eigenvalues[:keep]\n\n    # if threshold is not None and threshold &gt; 0:\n    #     idx_thresh = np.where(eigenvalues / np.max(eigenvalues) &gt; threshold)[0]\n    #     topcs = topcs[idx_thresh]\n    #     eigenvalues = eigenvalues[idx_thresh]\n    topcs, eigenvalues= self.normalize_topcs(eigenvalues, topcs, threshold)\n\n    # Cross-covariance between data and regressor PCs\n    cxy = cxy.T  # Transpose cxy to match dimensions\n    r = topcs.T @ cxy\n\n    # Projection matrix from regressor PCs\n    r = self.nt_vecmult(r,1 / eigenvalues)\n\n    # Projection matrix from regressors\n    r = topcs @ r\n\n    return r\n</code></pre>"},{"location":"api/core/#pyzaplineplus.core.PyZaplinePlus.nt_smooth","title":"<code>nt_smooth(x, T, n_iterations=1, nodelayflag=False)</code>","text":"<p>Smooth the data by convolution with a square window.</p> <p>Parameters: x (numpy.ndarray): The input data to smooth. Shape: (samples, channels) or (samples, channels, ...) T (float): The window size (can be fractional). n_iterations (int): Number of iterations of smoothing (default is 1). nodelayflag (bool): If True, compensate for delay introduced by filtering.</p> <p>Returns: numpy.ndarray: Smoothed data with the same shape as input.</p> Source code in <code>pyzaplineplus/core.py</code> <pre><code>def nt_smooth(self, x, T, n_iterations=1, nodelayflag=False):\n    \"\"\"\n    Smooth the data by convolution with a square window.\n\n    Parameters:\n    x (numpy.ndarray): The input data to smooth. Shape: (samples, channels) or (samples, channels, ...)\n    T (float): The window size (can be fractional).\n    n_iterations (int): Number of iterations of smoothing (default is 1).\n    nodelayflag (bool): If True, compensate for delay introduced by filtering.\n\n    Returns:\n    numpy.ndarray: Smoothed data with the same shape as input.\n    \"\"\"\n    from scipy.signal import lfilter\n\n    # Ensure x is at least 2D\n    if x.ndim &lt; 2:\n        x = x[:, np.newaxis]\n\n    # Split T into integer and fractional parts\n    integ = int(np.floor(T))\n    frac = T - integ\n\n    # If the window size exceeds data length, replace data with mean\n    if integ &gt;= x.shape[0]:\n        x = np.tile(np.mean(x, axis=0), (x.shape[0], 1))\n        return x\n\n    # Remove onset step (similar to MATLAB code)\n    mn = np.mean(x[:integ + 1, :], axis=0)\n    x = x - mn\n\n    if n_iterations == 1 and frac == 0:\n        # Faster convolution using cumulative sum (similar to MATLAB)\n        cumsum = np.cumsum(x, axis=0)\n        x[integ:, :] = (cumsum[integ:, :] - cumsum[:-integ, :]) / T\n    else:\n        # Construct the initial filter kernel B\n        B = np.concatenate((np.ones(integ), [frac])) / T\n\n        # Iteratively convolve B with [ones(integ), frac] / T for n_iterations-1 times\n        for _ in range(n_iterations - 1):\n            B = np.convolve(B, np.concatenate((np.ones(integ), [frac]))) / T\n\n        # Apply the filter using causal filtering (similar to MATLAB's filter)\n        # For multi-dimensional data, apply filter along the first axis (samples)\n        for channel in range(x.shape[1]):\n            x[:, channel] = lfilter(B, 1, x[:, channel])\n\n    # Restore the mean value that was subtracted earlier\n    x = x + mn\n\n    # Delay compensation if nodelayflag is set to True\n    if nodelayflag:\n        shift = int(round(T / 2 * n_iterations))\n        if shift &gt; 0:\n            # Shift the data forward and pad the end with zeros\n            padding = np.zeros((shift, x.shape[1]))\n            x = np.vstack((x[shift:, :], padding))\n        else:\n            pass  # No shift needed\n\n    return x\n</code></pre>"},{"location":"api/core/#pyzaplineplus.core.PyZaplinePlus.nt_spect_plot","title":"<code>nt_spect_plot(data, nfft, fs, return_data=False)</code>","text":"<p>Compute and optionally plot the power spectrum of the data.</p> Source code in <code>pyzaplineplus/core.py</code> <pre><code>def nt_spect_plot(self, data, nfft, fs, return_data=False):\n    \"\"\"\n    Compute and optionally plot the power spectrum of the data.\n    \"\"\"\n    f, pxx = _welch_hamming_periodic(data, fs=fs, nperseg=nfft, axis=0)\n    if return_data:\n        return pxx, f\n    else:\n        plt.figure()\n        plt.semilogy(f, np.mean(pxx, axis=1))\n        plt.xlabel('Frequency (Hz)')\n        plt.ylabel('Power Spectral Density')\n        plt.title('Power Spectrum')\n        plt.grid(True)\n        plt.show()\n</code></pre>"},{"location":"api/core/#pyzaplineplus.core.PyZaplinePlus.nt_tsr","title":"<code>nt_tsr(x, ref, shifts=None, wx=None, wref=None, keep=None, thresh=1e-20)</code>","text":"<p>Perform time-shift regression (TSPCA) to denoise data.</p> <p>Parameters:     x (np.ndarray): Data to denoise (time x channels x trials).     ref (np.ndarray): Reference data (time x channels x trials).     shifts (np.ndarray): Array of shifts to apply to ref (default: [0]).     wx (np.ndarray): Weights to apply to x (time x 1 x trials).     wref (np.ndarray): Weights to apply to ref (time x 1 x trials).     keep (int): Number of shifted-ref PCs to retain (default: all).     thresh (float): Threshold to ignore small shifted-ref PCs (default: 1e-20).</p> <p>Returns:     y (np.ndarray): Denoised data.     idx (np.ndarray): Indices where x(idx) is aligned with y.     w (np.ndarray): Weights applied by tsr.</p> Source code in <code>pyzaplineplus/core.py</code> <pre><code>def nt_tsr(self, x, ref, shifts=None, wx=None, wref=None, keep=None, thresh=1e-20):\n    \"\"\"\n    Perform time-shift regression (TSPCA) to denoise data.\n\n    Parameters:\n        x (np.ndarray): Data to denoise (time x channels x trials).\n        ref (np.ndarray): Reference data (time x channels x trials).\n        shifts (np.ndarray): Array of shifts to apply to ref (default: [0]).\n        wx (np.ndarray): Weights to apply to x (time x 1 x trials).\n        wref (np.ndarray): Weights to apply to ref (time x 1 x trials).\n        keep (int): Number of shifted-ref PCs to retain (default: all).\n        thresh (float): Threshold to ignore small shifted-ref PCs (default: 1e-20).\n\n    Returns:\n        y (np.ndarray): Denoised data.\n        idx (np.ndarray): Indices where x(idx) is aligned with y.\n        w (np.ndarray): Weights applied by tsr.\n    \"\"\"\n    # Handle default arguments\n    if shifts is None:\n        shifts = np.array([0])\n    else:\n        shifts = np.asarray(shifts)\n\n    # x = np.atleast_3d(x)  # Shape: (time, channels, trials) or (time, channels, 1)\n    # ref = np.atleast_3d(ref)\n\n    if wx is not None and wx.ndim == 2:\n        wx = wx[:, np.newaxis, :]\n        wx = np.atleast_3d(wx)\n    if wref is not None and wref.ndim == 2:\n        wref = wref[:, np.newaxis, :]\n        wref = np.atleast_3d(wref)\n\n    # Ensure x and ref are at least 3D\n\n    # Check argument values for sanity\n    if x.shape[0] != ref.shape[0]:\n        raise ValueError('x and ref should have the same number of time samples')\n    if x.ndim&gt;=3:\n        if x.shape[2] != ref.shape[2]:\n            raise ValueError('x and ref should have the same number of trials')\n    if wx is not None and (x.shape[0] != wx.shape[0] or x.shape[2] != wx.shape[2]):\n        raise ValueError('x and wx should have matching dimensions')\n    if wref is not None and (ref.shape[0] != wref.shape[0] or ref.shape[2] != wref.shape[2]):\n        raise ValueError('ref and wref should have matching dimensions')\n    if max(shifts) - min(0, min(shifts)) &gt;= x.shape[0]:\n        raise ValueError('x has too few samples to support the given shifts')\n    if wx is not None and wx.shape[1] != 1:\n        raise ValueError('wx should have shape (time, 1, trials)')\n    if wref is not None and wref.shape[1] != 1:\n        raise ValueError('wref should have shape (time, 1, trials)')\n    if wx is not None and np.sum(wx) == 0:\n        raise ValueError('weights on x are all zero!')\n    if wref is not None and np.sum(wref) == 0:\n        raise ValueError('weights on ref are all zero!')\n    if shifts.size &gt; 1000:\n        raise ValueError(f'Number of shifts ({shifts.size}) is too large (if OK, adjust the code to allow it)')\n\n    # Adjust x and ref to ensure that shifts are non-negative\n    offset1 = max(0, -min(shifts))\n    idx = np.arange(offset1, x.shape[0])\n    x = x[idx, :]  # Truncate x\n    if wx is not None:\n        wx = wx[idx, :]\n    shifts = shifts + offset1  # Shifts are now non-negative\n\n    # Adjust size of x\n    offset2 = max(0, max(shifts))\n    idx_ref = np.arange(0, ref.shape[0] - offset2)\n    x = x[:len(idx_ref), :]  # Part of x that overlaps with time-shifted refs\n    if wx is not None:\n        wx = wx[:len(idx_ref), :]\n    if x.ndim == 3:\n        mx, nx, ox = x.shape\n        mref, nref, oref = ref.shape\n    elif x.ndim == 2:\n        mx, nx = x.shape\n        mref, nref = ref.shape\n    else:\n        raise ValueError('x should be 2D or 3D')\n\n\n    # Consolidate weights into a single weight matrix\n    w = np.zeros((mx, 1))\n    if wx is None and wref is None:\n        w[:] = 1\n    elif wref is None:\n        w = wx\n    elif wx is None:\n        wr = wref[:, :]\n        wr_shifted = self.nt_multishift(wr, shifts)\n        w[:, :] = np.min(wr_shifted, axis=1, keepdims=True)\n    else:\n\n        wr = wref[:, :]\n        wr_shifted = self.nt_multishift(wr, shifts)\n        w_min = np.min(wr_shifted, axis=1, keepdims=True)\n        w[:, :] = np.minimum(w_min, wx[:w_min.shape[0], :])\n    wx = w\n    wref = np.zeros((mref, 1))\n    wref[idx, :] = w\n\n    # Remove weighted means\n    x_demeaned, _ = self.nt_demean(x, wx)\n    ref_demeaned, _ = self.nt_demean(ref, wref)\n\n    # Equalize power of ref channels, then equalize power of ref PCs\n    ref_normalized, _ = self.nt_normcol(ref_demeaned, wref)\n    ref_pca, _ = self.nt_pca(ref_normalized, threshold=1e-6)\n    ref_normalized_pca, _ = self.nt_normcol(ref_pca, wref)\n    ref = ref_normalized_pca\n\n    # Covariances and cross-covariance with time-shifted refs\n    cref, twcref = self.nt_cov(ref, shifts, wref)\n    cxref, twcxref = self.nt_xcov(x, ref, shifts, wx)\n\n    # Regression matrix of x on time-shifted refs\n    r = self.nt_regcov(cxref / twcxref, cref / twcref, keep=keep, threshold=thresh)\n\n    # Clean x by removing regression on time-shifted refs\n    y = np.zeros_like(x)\n\n    ref_shifted = self.nt_multishift(ref[:, :], shifts)\n    z = ref_shifted @ r\n    y = x[:z.shape[0], :] - z\n\n    y_demeaned, _ = self.nt_demean(y, wx)  # Multishift(ref) is not necessarily zero mean\n\n    # idx for alignment\n    idx_output = np.arange(offset1, offset1 + y.shape[0])\n    w = wref\n\n    # Return outputs\n    return y_demeaned, idx_output, w\n</code></pre>"},{"location":"api/core/#pyzaplineplus.core.PyZaplinePlus.nt_unfold","title":"<code>nt_unfold(x)</code>","text":"<p>Converts a 3D matrix (time x channel x trial) into a 2D matrix (time*trial x channel).</p> <p>Parameters:     x (np.ndarray): Input data. Can be 2D or 3D.</p> <p>Returns:     y (np.ndarray): Unfolded data.</p> Source code in <code>pyzaplineplus/core.py</code> <pre><code>def nt_unfold(self,x):\n    \"\"\"\n    Converts a 3D matrix (time x channel x trial) into a 2D matrix (time*trial x channel).\n\n    Parameters:\n        x (np.ndarray): Input data. Can be 2D or 3D.\n\n    Returns:\n        y (np.ndarray): Unfolded data.\n    \"\"\"\n    if x.size == 0:\n        return np.array([])\n    else:\n        if x.ndim == 3:\n            m, n, p = x.shape\n            if p &gt; 1:\n                y = np.reshape(np.transpose(x, (0, 2, 1)), (m * p, n))\n            else:\n                y = x\n        else:\n            y = x\n    return y\n</code></pre>"},{"location":"api/core/#pyzaplineplus.core.PyZaplinePlus.nt_vecmult","title":"<code>nt_vecmult(xx, ww)</code>","text":"<p>Multiply each row of 'xx' by the corresponding weight in 'ww'.</p> <p>Parameters:     xx (np.ndarray): Data array (time_shifted x channels).     ww (np.ndarray): Weights array (time_shifted x 1).</p> <p>Returns:     weighted_xx (np.ndarray): Weighted data (time_shifted x channels).</p> Source code in <code>pyzaplineplus/core.py</code> <pre><code>def nt_vecmult(self, xx: np.ndarray, ww: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Multiply each row of 'xx' by the corresponding weight in 'ww'.\n\n    Parameters:\n        xx (np.ndarray): Data array (time_shifted x channels).\n        ww (np.ndarray): Weights array (time_shifted x 1).\n\n    Returns:\n        weighted_xx (np.ndarray): Weighted data (time_shifted x channels).\n    \"\"\"\n    # Ensure that 'ww' is a column vector\n    if ww.ndim == 1:\n        ww = ww[:, np.newaxis]\n    elif ww.ndim == 2 and ww.shape[1] != 1:\n        raise ValueError(\"Weights 'ww' must have a single column or be a 1D array.\")\n\n    # Element-wise multiplication with broadcasting\n    weighted_xx = xx * ww  # Shape: (time_shifted x channels)\n    return weighted_xx\n</code></pre>"},{"location":"api/core/#pyzaplineplus.core.PyZaplinePlus.nt_xcov","title":"<code>nt_xcov(x, y, shifts=None, w=None)</code>","text":"<p>Compute the cross-covariance of x and time-shifted y.</p> <p>Parameters:     x (Union[np.ndarray, List[np.ndarray]]): Data array x (time x channels x trials) or list of 2D arrays.     y (Union[np.ndarray, List[np.ndarray]]): Data array y (time x channels x trials) or list of 2D arrays.     shifts (Optional[Union[List[int], np.ndarray]]): Array of non-negative integer time shifts (default: [0]).     w (Optional[Union[np.ndarray, List[np.ndarray]]]): Optional weights array (time x 1 x trials) or (time x channels x trials).</p> <p>Returns:     Tuple[np.ndarray, float]: Cross-covariance matrix and total weight.         - c: cross-covariance matrix.         - tw: total weight.</p> Source code in <code>pyzaplineplus/core.py</code> <pre><code>def nt_xcov(\n    self,\n    x: Union[np.ndarray, List[np.ndarray]],\n    y: Union[np.ndarray, List[np.ndarray]],\n    shifts: Optional[Union[List[int], np.ndarray]] = None,\n    w: Optional[Union[np.ndarray, List[np.ndarray]]] = None\n) -&gt; Tuple[np.ndarray, float]:\n    \"\"\"\n    Compute the cross-covariance of x and time-shifted y.\n\n    Parameters:\n        x (Union[np.ndarray, List[np.ndarray]]): Data array x (time x channels x trials) or list of 2D arrays.\n        y (Union[np.ndarray, List[np.ndarray]]): Data array y (time x channels x trials) or list of 2D arrays.\n        shifts (Optional[Union[List[int], np.ndarray]]): Array of non-negative integer time shifts (default: [0]).\n        w (Optional[Union[np.ndarray, List[np.ndarray]]]): Optional weights array (time x 1 x trials) or (time x channels x trials).\n\n    Returns:\n        Tuple[np.ndarray, float]: Cross-covariance matrix and total weight.\n            - c: cross-covariance matrix.\n            - tw: total weight.\n    \"\"\"\n    # If numpy arrays are 2D, expand to 3D for consistent processing\n    if isinstance(x, np.ndarray) and isinstance(y, np.ndarray):\n        if x.ndim == 2 and y.ndim == 2:\n            x = x[:, :, np.newaxis]\n            y = y[:, :, np.newaxis]\n            if w is not None:\n                if not isinstance(w, np.ndarray):\n                    raise TypeError(\"Weights `w` must be a numpy.ndarray when `x`/`y` are numpy.ndarray types.\")\n                if w.ndim == 2:\n                    w = w[:, :, np.newaxis]\n    if shifts is None:\n        shifts = np.array([0])\n    else:\n        shifts = np.asarray(shifts).flatten()\n    shifts = cast(np.ndarray, shifts)\n\n    if np.any(shifts &lt; 0):\n        raise ValueError('Shifts must be non-negative integers')\n\n    # Validate dimensions\n    if isinstance(x, list) and isinstance(y, list):\n        if len(x) != len(y):\n            raise ValueError(\"Lists `x` and `y` must have the same length.\")\n        if w is not None and not isinstance(w, list):\n            raise ValueError(\"Weights `w` must be a list if `x` and `y` are lists.\")\n    elif isinstance(x, np.ndarray) and isinstance(y, np.ndarray):\n        if x.ndim != y.ndim:\n            raise ValueError(\"Arrays `x` and `y` must have the same number of dimensions.\")\n        if x.shape[0] != y.shape[0]:\n            raise ValueError(\"`x` and `y` must have the same number of time samples.\")\n        if x.ndim &gt; 2:\n            if x.shape[2] != y.shape[2]:\n                raise ValueError(\"`x` and `y` must have the same number of trials.\")\n            if w is not None:\n                if not isinstance(w, np.ndarray):\n                    raise TypeError(\"Weights `w` must be a numpy.ndarray when `x`/`y` are numpy.ndarray types.\")\n                if x.shape[2] != w.shape[2]:\n                    raise ValueError(\"`x` and `w` must have the same number of trials.\")\n    else:\n        raise TypeError(\"`x` and `y` must both be either lists or numpy.ndarray types.\")\n\n    nshifts = shifts.size\n\n    # Initialize covariance matrix and total weight\n    c = None\n    tw = 0.0\n\n    # Handle list inputs (equivalent to cell arrays in MATLAB)\n    if isinstance(x, list):\n        if w is not None and not isinstance(w, list):\n            raise ValueError(\"Weights `w` must be a list if `x` is a list (cell array).\")\n\n        o = len(x)  # Number of cells/trials\n        if o == 0:\n            raise ValueError(\"Input list `x` is empty.\")\n\n        # Determine number of channels from the first cell\n        # shapes of first elements inferred below when needed\n        if x[0].ndim == 1:\n            n_channels_x = 1\n        elif x[0].ndim == 2:\n            n_channels_x = x[0].shape[1]\n        else:\n            raise ValueError(f\"Data in cell 0 of `x` has unsupported number of dimensions: {x[0].ndim}\")\n\n        if y[0].ndim == 1:\n            n_channels_y = 1\n        elif y[0].ndim == 2:\n            n_channels_y = y[0].shape[1]\n        else:\n            raise ValueError(f\"Data in cell 0 of `y` has unsupported number of dimensions: {y[0].ndim}\")\n\n        # Initialize cross-covariance matrix\n        c = np.zeros((n_channels_x, n_channels_y * nshifts), dtype=np.float64)\n\n        for idx in range(o):\n            data_x = x[idx]\n            data_y = y[idx]\n\n            # Validate data dimensions\n            if not isinstance(data_x, np.ndarray) or not isinstance(data_y, np.ndarray):\n                raise TypeError(f\"Elements in lists `x` and `y` must be numpy.ndarray types. Found types: {type(data_x)}, {type(data_y)} at index {idx}.\")\n            if data_x.ndim != 2 or data_y.ndim != 2:\n                raise ValueError(f\"Data in lists `x` and `y` must be 2D arrays. Found dimensions: {data_x.ndim}, {data_y.ndim} at index {idx}.\")\n\n            if data_x.shape[1] != n_channels_x:\n                raise ValueError(f\"All cells in `x` must have {n_channels_x} channels. Found {data_x.shape[1]} at index {idx}.\")\n            if data_y.shape[1] != n_channels_y:\n                raise ValueError(f\"All cells in `y` must have {n_channels_y} channels. Found {data_y.shape[1]} at index {idx}.\")\n\n            # Handle weights\n            if w is not None:\n                weight = w[idx]\n                if not isinstance(weight, np.ndarray):\n                    raise TypeError(f\"Weight for cell {idx} must be a numpy.ndarray.\")\n                if weight.size == 0:\n                    raise ValueError(f\"Weight for cell {idx} is empty.\")\n                if weight.ndim == 1:\n                    weight = weight[:, np.newaxis]  # Shape: (n_samples, 1)\n                elif weight.ndim == 2 and weight.shape[1] == 1:\n                    pass  # Shape is already (n_samples, 1)\n                else:\n                    raise ValueError(f\"Weight for cell {idx} must be 1D or 2D with a single column. Got shape {weight.shape}.\")\n\n            # Apply shifts to y\n            y_shifted = self.nt_multishift(data_y, shifts)  # Shape: (n_samples_shifted, n_channels_y * nshifts)\n\n            # Truncate x to match the shifted y's time dimension\n            if not np.all(shifts == 0):\n                # Apply shifts to x if necessary (though MATLAB does not shift x in cross-covariance)\n                # Assuming x is not shifted, only y is\n                # Thus, truncate x to match the shifted y's number of samples\n                x_truncated = data_x[:y_shifted.shape[0], :]  # Shape: (n_samples_shifted, n_channels_x)\n            else:\n                x_truncated = data_x.copy()\n\n            # Handle weights: multiply x by w\n            if w is not None:\n                # Unfold x and w, multiply, fold back\n                x_weighted = self.nt_fold(\n                    self.nt_vecmult(\n                        self.nt_unfold(x_truncated),\n                        self.nt_unfold(weight[:y_shifted.shape[0], :]),\n                    ),\n                    data_x.shape[0],\n                )[: y_shifted.shape[0], :]  # Ensure matching time dimension\n                x_truncated = x_weighted  # Shape: (n_samples_shifted, n_channels_x)\n\n                # Update total weight\n                if not np.all(shifts == 0):\n                    # Take minimum weight across shifts\n                    ww = self.nt_multishift(weight[:y_shifted.shape[0], :], shifts)  # Shape: (n_samples_shifted * nshifts, 1)\n                    ww_min = np.min(ww, axis=1, keepdims=True)\n                    tw += np.sum(ww_min)\n                else:\n                    tw += np.sum(weight[:y_shifted.shape[0], :])\n            else:\n                tw += x_truncated.shape[0]  # Number of samples\n\n            # Accumulate cross-covariance\n            c += np.dot(x_truncated.T, y_shifted)  # Shape: (n_channels_x, n_channels_y * nshifts)\n\n        # Handle NumPy array inputs\n    elif isinstance(x, np.ndarray) and isinstance(y, np.ndarray):\n        # Validate dimensions\n        if x.ndim != y.ndim:\n            raise ValueError(\"Arrays `x` and `y` must have the same number of dimensions.\")\n        if x.shape[0] != y.shape[0]:\n            raise ValueError(\"`x` and `y` must have the same number of time samples.\")\n        if x.ndim &gt; 2:\n            if x.shape[2] != y.shape[2]:\n                raise ValueError(\"`x` and `y` must have the same number of trials.\")\n            if w is not None:\n                if not isinstance(w, np.ndarray):\n                    raise TypeError(\"Weights `w` must be a numpy.ndarray if `x` and `y` are numpy.ndarray types.\")\n                if w.shape[0] != x.shape[0] or w.shape[2] != x.shape[2]:\n                    raise ValueError(\"`w` must have the same number of time samples and trials as `x`.\")\n\n        mx, nx, ox = x.shape\n        my, ny, oy = y.shape\n\n        # Determine number of channels\n        n_channels_x = nx\n        n_channels_y = ny\n\n        # Initialize cross-covariance matrix\n        c = np.zeros((n_channels_x, n_channels_y * nshifts), dtype=np.float64)\n\n        for trial in range(ox):\n            data_x = x[:, :, trial]  # Shape: (n_samples, n_channels_x)\n            data_y = y[:, :, trial]  # Shape: (n_samples, n_channels_y)\n\n            # Apply shifts to y\n            y_shifted= self.nt_multishift(data_y, shifts)  # Shape: (n_samples_shifted, n_channels_y * nshifts)\n\n            # Truncate x to match the shifted y's time dimension\n            if not np.all(shifts == 0):\n                # Assuming y is shifted, x is not; truncate x accordingly\n                x_truncated = data_x[:y_shifted.shape[0], :]  # Shape: (n_samples_shifted, n_channels_x)\n            else:\n                x_truncated = data_x.copy()\n\n            if w is not None:\n                # Extract weights for this trial and truncate\n                if not isinstance(w, np.ndarray):\n                    raise TypeError(\"Weights `w` must be a numpy.ndarray when `x`/`y` are numpy.ndarray types.\")\n                trial_weight = w[:, :, trial]  # Shape: (n_samples, channels or 1)\n                # if w.ndim == 2:\n                #     # For 3D `x`, weights are 2D (time x trials)\n                #     trial_weight = trial_weight  # Shape: (n_samples, trials)\n                # elif w.ndim == 1:\n                #     # For 1D or 2D `x`, weights are 1D\n                #     trial_weight = trial_weight[:, np.newaxis]  # Shape: (n_samples, 1)\n                # else:\n                #     raise ValueError(f\"Unsupported weight dimensionality: {w.ndim}\")\n\n                # Unfold x and w, multiply, fold back\n                x_weighted = self.nt_fold(\n                    self.nt_vecmult(\n                        self.nt_unfold(x_truncated),\n                        self.nt_unfold(trial_weight[:y_shifted.shape[0], :]),\n                    ),\n                    mx,\n                )[: y_shifted.shape[0], :]  # Ensure matching time dimension\n\n                x_truncated = x_weighted  # Shape: (n_samples_shifted, n_channels_x)\n\n                if not np.all(shifts == 0):\n                    # Take minimum weight across shifts\n                    ww = self.nt_multishift(trial_weight[:y_shifted.shape[0], :], shifts)  # Shape: (n_samples_shifted * nshifts, 1)\n                    ww_min = np.min(ww, axis=1, keepdims=True)\n                    tw += np.sum(ww_min)\n                else:\n                    tw += np.sum(trial_weight[:y_shifted.shape[0], :])\n            else:\n                tw += x_truncated.shape[0]  # Number of samples\n\n            # Accumulate cross-covariance\n            c += np.dot(x_truncated.T, y_shifted)  # Shape: (n_channels_x, n_channels_y * nshifts)\n\n    else:\n        raise TypeError(\"`x` and `y` must both be either lists or numpy.ndarray types.\")\n\n    return c, tw\n</code></pre>"},{"location":"api/core/#pyzaplineplus.core.zapline_plus","title":"<code>zapline_plus(data, sampling_rate, **kwargs)</code>","text":"Source code in <code>pyzaplineplus/core.py</code> <pre><code>def zapline_plus(data, sampling_rate, **kwargs):\n    zp = PyZaplinePlus(data, sampling_rate, **kwargs)\n    return zp.run()\n</code></pre>"},{"location":"api/noise-detection/","title":"Noise Detection","text":""},{"location":"api/noise-detection/#pyzaplineplus.noise_detection.find_next_noisefreq","title":"<code>find_next_noisefreq(pxx, f, minfreq=0, threshdiff=5, winsizeHz=3, maxfreq=None, lower_threshdiff=1.76091259055681, verbose=False)</code>","text":"<p>Find the next noise frequency in the power spectrum.</p> <p>This function searches for noise frequencies by analyzing the power spectral density and identifying peaks that exceed a threshold relative to surrounding frequencies.</p> <p>Parameters:</p> Name Type Description Default <code>pxx</code> <code>ndarray</code> <p>Power spectral density array (frequency x channels).</p> required <code>f</code> <code>ndarray</code> <p>Frequency vector corresponding to pxx.</p> required <code>minfreq</code> <code>float</code> <p>Minimum frequency to search from (default: 0).</p> <code>0</code> <code>threshdiff</code> <code>float</code> <p>Threshold difference for noise detection (default: 5).</p> <code>5</code> <code>winsizeHz</code> <code>float</code> <p>Window size in Hz for analysis (default: 3).</p> <code>3</code> <code>maxfreq</code> <code>float</code> <p>Maximum frequency to search to. If None, uses 85% of max frequency.</p> <code>None</code> <code>lower_threshdiff</code> <code>float</code> <p>Lower threshold difference for continued detection (default: 1.76091259055681).</p> <code>1.76091259055681</code> <code>verbose</code> <code>bool</code> <p>Whether to print verbose output and show plots (default: False).</p> <code>False</code> <p>Returns:</p> Name Type Description <code>noisefreq</code> <code>float or None</code> <p>Detected noise frequency, or None if no noise found.</p> <code>thisfreqs</code> <code>ndarray or None</code> <p>Frequency array of the analyzed window.</p> <code>thisdata</code> <code>ndarray or None</code> <p>Power data of the analyzed window.</p> <code>threshfound</code> <code>float or None</code> <p>The threshold that was used for detection.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from scipy import signal\n&gt;&gt;&gt; # Create sample data with line noise\n&gt;&gt;&gt; fs = 1000\n&gt;&gt;&gt; t = np.arange(0, 10, 1/fs)\n&gt;&gt;&gt; data = np.random.randn(len(t), 32) + 0.5 * np.sin(2*np.pi*50*t)[:, np.newaxis]\n&gt;&gt;&gt; f, pxx = signal.welch(data, fs=fs, axis=0)\n&gt;&gt;&gt; noisefreq, _, _, _ = find_next_noisefreq(pxx, f, minfreq=45, maxfreq=55)\n&gt;&gt;&gt; print(f\"Detected noise at {noisefreq:.2f} Hz\")\n</code></pre> Source code in <code>pyzaplineplus/noise_detection.py</code> <pre><code>def find_next_noisefreq(pxx, f, minfreq=0, threshdiff=5, winsizeHz=3, maxfreq=None,\n                       lower_threshdiff=1.76091259055681, verbose=False):\n    \"\"\"\n    Find the next noise frequency in the power spectrum.\n\n    This function searches for noise frequencies by analyzing the power spectral density\n    and identifying peaks that exceed a threshold relative to surrounding frequencies.\n\n    Parameters\n    ----------\n    pxx : ndarray\n        Power spectral density array (frequency x channels).\n    f : ndarray\n        Frequency vector corresponding to pxx.\n    minfreq : float, optional\n        Minimum frequency to search from (default: 0).\n    threshdiff : float, optional\n        Threshold difference for noise detection (default: 5).\n    winsizeHz : float, optional\n        Window size in Hz for analysis (default: 3).\n    maxfreq : float, optional\n        Maximum frequency to search to. If None, uses 85% of max frequency.\n    lower_threshdiff : float, optional\n        Lower threshold difference for continued detection (default: 1.76091259055681).\n    verbose : bool, optional\n        Whether to print verbose output and show plots (default: False).\n\n    Returns\n    -------\n    noisefreq : float or None\n        Detected noise frequency, or None if no noise found.\n    thisfreqs : ndarray or None\n        Frequency array of the analyzed window.\n    thisdata : ndarray or None\n        Power data of the analyzed window.\n    threshfound : float or None\n        The threshold that was used for detection.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from scipy import signal\n    &gt;&gt;&gt; # Create sample data with line noise\n    &gt;&gt;&gt; fs = 1000\n    &gt;&gt;&gt; t = np.arange(0, 10, 1/fs)\n    &gt;&gt;&gt; data = np.random.randn(len(t), 32) + 0.5 * np.sin(2*np.pi*50*t)[:, np.newaxis]\n    &gt;&gt;&gt; f, pxx = signal.welch(data, fs=fs, axis=0)\n    &gt;&gt;&gt; noisefreq, _, _, _ = find_next_noisefreq(pxx, f, minfreq=45, maxfreq=55)\n    &gt;&gt;&gt; print(f\"Detected noise at {noisefreq:.2f} Hz\")\n    \"\"\"\n    if maxfreq is None:\n        maxfreq = max(f) * 0.85\n\n    if verbose:\n        print(f\"Searching for first noise freq between {minfreq}Hz and {maxfreq}Hz...\")\n\n    noisefreq = None\n    threshfound = None\n    thisfreqs = None\n    thisdata = None\n    winsize = round(pxx.shape[0] / (max(f) - min(f)) * winsizeHz)\n    meandata = np.mean(pxx, axis=1)\n\n    detectionstart = False\n    detected = True\n    i_startdetected = 0\n    i_enddetected = 0\n\n    # Compute i_start\n    indices = np.where(f &gt; minfreq)[0]\n    if indices.size == 0:\n        i_start = round(winsize / 2)\n    else:\n        i_start = max(indices[0] + 1, round(winsize / 2))\n\n    # Compute i_end\n    indices = np.where(f &lt; maxfreq)[0]\n    if indices.size == 0:\n        i_end = len(f) - round(winsize / 2)\n    else:\n        i_end = min(indices[-1], len(f) - round(winsize / 2))\n\n    lastfreq = 0\n    for i in range(int(i_start - round(winsize / 2)), int(i_end - round(winsize / 2) + 1)):\n        thisdata = meandata[i:i + winsize]\n        thisfreqs = f[i:i + winsize]\n\n        # Correct index for zero-based indexing\n        middle_index = int(round(len(thisdata) / 2)) - 1\n        thisfreq = round(thisfreqs[middle_index])\n\n        if verbose and thisfreq &gt; lastfreq:\n            print(f\"{thisfreq},\", end=\"\")\n            lastfreq = thisfreq\n\n        third = round(len(thisdata) / 3)\n        center_thisdata = np.mean(np.concatenate([thisdata[:third], thisdata[2 * third:]]))\n        thresh = center_thisdata + threshdiff\n\n        if not detected:\n            detectednew = thisdata[middle_index] &gt; thresh\n            if detectednew:\n                i_startdetected = round(i + (winsize - 1) / 2)\n                threshfound = thresh\n        else:\n            detectednew = thisdata[middle_index] &gt; center_thisdata + lower_threshdiff\n            i_enddetected = round(i + (winsize - 1) / 2)\n\n        if not detectionstart and detected and not detectednew:\n            detectionstart = True\n        elif detectionstart and detected and not detectednew:\n            # Handle multiple maxima\n            max_value: float = float(np.max(meandata[i_startdetected:i_enddetected + 1]))\n            max_indices = np.where(meandata[i_startdetected:i_enddetected + 1] == max_value)[0]\n            noisefreq = f[max_indices[0] + i_startdetected]\n            if verbose:\n                print(f\"\\nfound {noisefreq}Hz!\")\n                try:\n                    import matplotlib.pyplot as plt\n                    plt.figure()\n                    plt.plot(thisfreqs, thisdata)\n                    plt.axhline(y=float(thresh), color='r', linestyle='-')\n                    plt.axhline(y=float(center_thisdata), color='k', linestyle='-')\n                    plt.title(str(noisefreq))\n                    plt.xlabel('Frequency (Hz)')\n                    plt.ylabel('Power (dB)')\n                    plt.show()\n                except ImportError:\n                    print(\"Matplotlib not available for plotting\")\n            return noisefreq, thisfreqs, thisdata, threshfound\n\n        detected = detectednew\n\n    if verbose:\n        print(\"\\nnone found.\")\n    return noisefreq, thisfreqs, thisdata, threshfound\n</code></pre>"},{"location":"api/utilities/","title":"Utilities","text":"<p>Additional low-level helpers used in the algorithm are implemented within <code>pyzaplineplus.core</code> (e.g., DSS, PCA wrappers, spectral helpers). These are considered internal but documented for reference.</p>"},{"location":"api/utilities/#pyzaplineplus.core","title":"<code>core</code>","text":""},{"location":"api/utilities/#pyzaplineplus.core.PyZaplinePlus","title":"<code>PyZaplinePlus(data, sampling_rate, **kwargs)</code>","text":"Source code in <code>pyzaplineplus/core.py</code> <pre><code>def __init__(self, data, sampling_rate, **kwargs):\n    # Validate inputs\n    if not isinstance(data, np.ndarray):\n        data = np.array(data)\n\n    # Check for empty data\n    if data.size == 0:\n        raise ValueError(\"Data array cannot be empty\")\n\n    # Validate sampling rate\n    if not isinstance(sampling_rate, (int, float)) or sampling_rate &lt;= 0:\n        raise ValueError(\"Sampling rate must be a positive number\")\n\n    # Ensure data is 2D (samples x channels)\n    if data.ndim == 1:\n        data = data.reshape(-1, 1)  # Convert to column vector\n    elif data.ndim &gt; 2:\n        raise ValueError(\"Data must be 1D or 2D array\")\n\n    self.data = data\n    self.sampling_rate = sampling_rate\n    self._warned_nyquist = False\n    self.config = {\n        'noisefreqs': kwargs.get('noisefreqs', []),\n        'minfreq': kwargs.get('minfreq', 17),\n        'maxfreq': kwargs.get('maxfreq', 99),\n        'adaptiveNremove': kwargs.get('adaptiveNremove', True),\n        'fixedNremove': kwargs.get('fixedNremove', 1),\n        'detectionWinsize': kwargs.get('detectionWinsize', 6),\n        # Match MATLAB default: 4 (2.5x power over mean in dB scale)\n        'coarseFreqDetectPowerDiff': kwargs.get('coarseFreqDetectPowerDiff', 4),\n        'coarseFreqDetectLowerPowerDiff': kwargs.get('coarseFreqDetectLowerPowerDiff', 1.76091259055681),\n        'searchIndividualNoise': kwargs.get('searchIndividualNoise', True),\n        'freqDetectMultFine': kwargs.get('freqDetectMultFine', 2.0),\n        'detailedFreqBoundsUpper': kwargs.get('detailedFreqBoundsUpper', [-0.05, 0.05]),\n        'detailedFreqBoundsLower': kwargs.get('detailedFreqBoundsLower', [-0.4, 0.1]),\n        'maxProportionAboveUpper': kwargs.get('maxProportionAboveUpper', 0.005),\n        'maxProportionBelowLower': kwargs.get('maxProportionBelowLower', 0.005),\n        'noiseCompDetectSigma': kwargs.get('noiseCompDetectSigma', 3.0),\n        'adaptiveSigma': kwargs.get('adaptiveSigma', True),\n        'minsigma': kwargs.get('minsigma', 2.5),\n        'maxsigma': kwargs.get('maxsigma', 5.0),  # Changed to 5\n        'chunkLength': kwargs.get('chunkLength', 0),\n        'minChunkLength': kwargs.get('minChunkLength', 30),\n        'winSizeCompleteSpectrum': kwargs.get('winSizeCompleteSpectrum', 300),\n        'nkeep': kwargs.get('nkeep', 0),\n        'plotResults': kwargs.get('plotResults', True),\n        'segmentLength': kwargs.get('segmentLength', 1),\n        'prominenceQuantile': kwargs.get('prominenceQuantile', 0.95),\n        'overwritePlot': kwargs.get('overwritePlot', False),\n        'figBase': kwargs.get('figBase', 100),\n        'figPos': kwargs.get('figPos', None),\n        'saveSpectra': kwargs.get('saveSpectra', False)\n        ,\n        # DSS parity/debug controls (dev)\n        # MATLAB parity defaults for DSS internals: use symmetric (+/-) bins\n        'dss_positive_only': kwargs.get('dss_positive_only', False),\n        'dss_divide_by_sumw2': kwargs.get('dss_divide_by_sumw2', False),\n        'dss_strict_frames': kwargs.get('dss_strict_frames', False),\n        'snapDssToFftBin': kwargs.get('snapDssToFftBin', False),\n        'saveDssDebug': kwargs.get('saveDssDebug', False),\n        'debugOutDir': kwargs.get('debugOutDir', None),\n    }\n</code></pre>"},{"location":"api/utilities/#pyzaplineplus.core.PyZaplinePlus.adaptive_chunk_detection","title":"<code>adaptive_chunk_detection(noise_freq)</code>","text":"<p>Use covariance matrices to adaptively segment data into chunks.</p> Source code in <code>pyzaplineplus/core.py</code> <pre><code>def adaptive_chunk_detection(self,noise_freq):\n    \"\"\"\n    Use covariance matrices to adaptively segment data into chunks.\n    \"\"\"\n    from scipy.signal import find_peaks\n    from scipy.spatial.distance import pdist\n    # 1. Bandpass Filter the Data\n    narrow_band_filtered = self.bandpass_filter(\n        self.data,\n        noise_freq - self.config['detectionWinsize'] / 2,\n        noise_freq + self.config['detectionWinsize'] / 2,\n        self.sampling_rate,\n    )\n\n    # 2. Determine Segment Length and Number of Segments\n    segment_length_samples = int(self.config['segmentLength'] * self.sampling_rate)\n    n_segments = max(len(narrow_band_filtered) // segment_length_samples, 1)\n\n    # 3. Compute Covariance Matrices for Each Segment\n    covariance_matrices = []\n    for i in range(n_segments):\n        start_idx = i * segment_length_samples\n        end_idx = (i + 1) * segment_length_samples if i != n_segments - 1 else len(narrow_band_filtered)\n        segment = narrow_band_filtered[start_idx:end_idx, :]\n        cov_matrix = np.cov(segment, rowvar=False)\n        covariance_matrices.append(cov_matrix)\n\n    # 4. Compute Distances Between Consecutive Covariance Matrices (match MATLAB)\n    # distances(i-1) = sum(pdist(C_i - C_{i-1})) / 2\n    distances = []\n    for i in range(1, len(covariance_matrices)):\n        cov_diff = covariance_matrices[i] - covariance_matrices[i - 1]\n        # Sum of pairwise distances across rows\n        d = pdist(cov_diff)\n        distance = np.sum(d) / 2.0\n        distances.append(distance)\n    distances = np.array(distances)\n\n    # 5. First Find Peaks to Obtain Prominences\n    initial_peaks, properties = find_peaks(distances,prominence=0)\n    prominences = properties['prominences']\n\n    # 6. Determine Prominence Threshold Based on Quantile\n    if len(prominences) == 0:\n        prominence_threshold: float = float('inf')  # No peaks found\n    else:\n        prominence_threshold: float = float(np.quantile(prominences, self.config['prominenceQuantile']))\n\n    # 7. Second Find Peaks Using Prominence Threshold\n    min_peak_distance_segments = int(np.ceil(self.config['minChunkLength'] / self.config['segmentLength']))\n    peaks, _ = find_peaks(\n        distances,\n        prominence=prominence_threshold,\n        distance=min_peak_distance_segments\n    )\n    # 8. Create Final Chunk Indices\n    # Initialize with 0 (start of data)\n    chunk_indices = [0]\n\n    # Calculate the end indices of the peaks in terms of samples\n    for peak in peaks:\n        # peak is the index in 'distances', corresponding to the boundary between segments\n        # So, the peak corresponds to the end of segment 'peak' and start of 'peak+1'\n        end_sample = (peak + 1) * segment_length_samples\n        chunk_indices.append(end_sample)\n\n    # Append the end of data\n    chunk_indices.append(len(self.data))\n\n    # 9. Ensure All Chunks Meet Minimum Length\n    min_length_samples = int(self.config['minChunkLength'] * self.sampling_rate)\n\n    # Check the first and last chunk only if we have at least two boundaries\n    if len(chunk_indices) &gt; 2:\n        if chunk_indices[1] - chunk_indices[0] &lt; min_length_samples:\n            chunk_indices.pop(1)  # Remove the first peak\n    if len(chunk_indices) &gt; 2:\n        if chunk_indices[-1] - chunk_indices[-2] &lt; min_length_samples:\n            chunk_indices.pop(-2)  # Remove the last peak\n\n    # Sort and remove duplicates if any\n    chunk_indices = sorted(set(chunk_indices))\n\n    return chunk_indices\n</code></pre>"},{"location":"api/utilities/#pyzaplineplus.core.PyZaplinePlus.adaptive_cleaning","title":"<code>adaptive_cleaning(clean_data, raw_data, noise_freq, zapline_config, cleaning_too_strong_once)</code>","text":"<p>Adjust the cleaning process if it was too weak or too strong.</p> Source code in <code>pyzaplineplus/core.py</code> <pre><code>def adaptive_cleaning(self, clean_data, raw_data, noise_freq,zapline_config,cleaning_too_strong_once):\n    \"\"\"\n    Adjust the cleaning process if it was too weak or too strong.\n    \"\"\"\n    # Compute the PSD of the clean data using periodic Hamming + 50% overlap\n    nperseg = int(self.config['winSizeCompleteSpectrum'] * self.sampling_rate)\n    f, pxx_clean = _welch_hamming_periodic(\n        clean_data, fs=self.sampling_rate, nperseg=nperseg, axis=0\n    )\n    pxx_clean_log = 10 * np.log10(pxx_clean)\n\n    # Determine center power by checking lower and upper third around noise frequency\n    detectionWinsize = zapline_config['detectionWinsize']\n    freq_range = (f &gt; noise_freq - (detectionWinsize / 2)) &amp; (f &lt; noise_freq + (detectionWinsize / 2))\n    this_fine_data = np.mean(pxx_clean_log[freq_range, :], axis=1)\n\n    # Calculate thirds of the data\n    third = int(np.round(len(this_fine_data) / 3))\n    if third == 0:\n        third = 1  # Ensure at least one sample\n\n    indices = np.concatenate((np.arange(0, third), np.arange(2 * third, len(this_fine_data))))\n    center_this_data = np.mean(this_fine_data[indices])\n\n    # Measure of variation using lower quantile\n    mean_lower_quantile = np.mean([\n        np.quantile(this_fine_data[0:third], 0.05),\n        np.quantile(this_fine_data[2 * third:], 0.05)\n    ])\n\n    # Compute thresholds\n    freq_detect_mult_fine = zapline_config['freqDetectMultFine']\n    remaining_noise_thresh_upper = center_this_data + freq_detect_mult_fine * (center_this_data - mean_lower_quantile)\n    remaining_noise_thresh_lower = center_this_data - freq_detect_mult_fine * (center_this_data - mean_lower_quantile)\n    zapline_config['remaining_noise_thresh_upper'] = remaining_noise_thresh_upper\n    zapline_config['remaining_noise_thresh_lower'] = remaining_noise_thresh_lower\n\n    # Frequency indices for upper and lower checks\n    freq_idx_upper_check = (f &gt; noise_freq + zapline_config['detailedFreqBoundsUpper'][0]) &amp; \\\n                        (f &lt; noise_freq + zapline_config['detailedFreqBoundsUpper'][1])\n    freq_idx_lower_check = (f &gt; noise_freq + zapline_config['detailedFreqBoundsLower'][0]) &amp; \\\n                        (f &lt; noise_freq + zapline_config['detailedFreqBoundsLower'][1])\n\n    # Store frequency indices for plotting\n    zapline_config['thisFreqidxUppercheck'] = freq_idx_upper_check\n    zapline_config['thisFreqidxLowercheck'] = freq_idx_lower_check\n\n    # Proportions for cleaning assessment\n    numerator_upper = float(np.sum(\n        np.mean(pxx_clean_log[freq_idx_upper_check, :], axis=1) &gt; remaining_noise_thresh_upper\n    ))\n    denominator_upper = float(np.sum(freq_idx_upper_check)) or 1.0\n    proportion_above_upper = numerator_upper / denominator_upper\n    cleaning_too_weak = proportion_above_upper &gt; zapline_config['maxProportionAboveUpper']\n    zapline_config['proportion_above_upper'] = proportion_above_upper\n\n    if cleaning_too_weak:\n        print(\"Cleaning too weak! \")\n    numerator_lower = float(np.sum(\n        np.mean(pxx_clean_log[freq_idx_lower_check, :], axis=1) &lt; remaining_noise_thresh_lower\n    ))\n    denominator_lower = float(np.sum(freq_idx_lower_check)) or 1.0\n    proportion_below_lower = numerator_lower / denominator_lower\n    cleaning_too_strong = proportion_below_lower &gt; zapline_config['maxProportionBelowLower']\n    zapline_config['proportion_below_lower'] = proportion_below_lower\n\n    # Adjust cleaning parameters based on the assessment\n    cleaning_done = True\n    if zapline_config['adaptiveNremove'] and zapline_config['adaptiveSigma']:\n        if cleaning_too_strong and zapline_config['noiseCompDetectSigma'] &lt; zapline_config['maxsigma']:\n            cleaning_too_strong_once = True\n            zapline_config['noiseCompDetectSigma'] = min(\n                zapline_config['noiseCompDetectSigma'] + 0.25,\n                zapline_config['maxsigma']\n            )\n            cleaning_done = False\n            # Decrease current minimum components to remove, but never below the original baseline\n            base_min = int(zapline_config.get('baseFixedNremove', zapline_config.get('fixedNremove', 1)))\n            zapline_config['fixedNremove'] = max(int(zapline_config['fixedNremove']) - 1, base_min)\n            print(f\"Cleaning too strong! Increasing sigma for noise component detection to {zapline_config['noiseCompDetectSigma']} \"\n                f\"and setting minimum number of removed components to {zapline_config['fixedNremove']}.\")\n            return cleaning_done, zapline_config, cleaning_too_strong_once\n\n        elif cleaning_too_weak and not cleaning_too_strong_once and zapline_config['noiseCompDetectSigma'] &gt; zapline_config['minsigma']:\n            zapline_config['noiseCompDetectSigma'] = max(\n                zapline_config['noiseCompDetectSigma'] - 0.25,\n                zapline_config['minsigma']\n            )\n            cleaning_done = False\n            zapline_config['fixedNremove'] += 1\n            print(f\"Cleaning too weak! Reducing sigma for noise component detection to {zapline_config['noiseCompDetectSigma']} \"\n                f\"and setting minimum number of removed components to {zapline_config['fixedNremove']}.\")\n\n    return cleaning_done, zapline_config, cleaning_too_strong_once\n</code></pre>"},{"location":"api/utilities/#pyzaplineplus.core.PyZaplinePlus.add_back_flat_channels","title":"<code>add_back_flat_channels(clean_data)</code>","text":"<p>Add back flat channels that were removed during preprocessing.</p> <p>Parameters:     clean_data (np.ndarray): Cleaned data with flat channels removed</p> <p>Returns:     np.ndarray: Cleaned data with flat channels added back as zeros</p> Source code in <code>pyzaplineplus/core.py</code> <pre><code>def add_back_flat_channels(self, clean_data):\n    \"\"\"\n    Add back flat channels that were removed during preprocessing.\n\n    Parameters:\n        clean_data (np.ndarray): Cleaned data with flat channels removed\n\n    Returns:\n        np.ndarray: Cleaned data with flat channels added back as zeros\n    \"\"\"\n    if self.flat_channels.size == 0:\n        return clean_data\n\n    # Create output array with original number of channels\n    original_n_channels = clean_data.shape[1] + len(self.flat_channels)\n    n_samples = clean_data.shape[0]\n    output_data = np.zeros((n_samples, original_n_channels))\n\n    # Create mask for non-flat channels\n    all_channels = np.arange(original_n_channels)\n    non_flat_channels = np.setdiff1d(all_channels, self.flat_channels)\n\n    # Insert cleaned data back into non-flat channel positions\n    output_data[:, non_flat_channels] = clean_data\n    # Flat channels remain as zeros\n\n    return output_data\n</code></pre>"},{"location":"api/utilities/#pyzaplineplus.core.PyZaplinePlus.apply_zapline_to_chunk","title":"<code>apply_zapline_to_chunk(chunk, noise_freq)</code>","text":"<p>Apply noise removal to the chunk using DSS (Denoising Source Separation) based on the provided MATLAB code.</p> Source code in <code>pyzaplineplus/core.py</code> <pre><code>def apply_zapline_to_chunk(self, chunk, noise_freq):\n    \"\"\"\n    Apply noise removal to the chunk using DSS (Denoising Source Separation) based on the provided MATLAB code.\n    \"\"\"\n    chunk = np.asarray(chunk)\n    if chunk.ndim == 1:\n        chunk = chunk[:, np.newaxis]\n\n    # Guard very short chunks: MATLAB requires enough samples for\n    # Welch/DSS; fall back to identity cleaning with a warning.\n    win_sec = float(self.config.get('winSizeCompleteSpectrum', 0) or 0)\n    if win_sec &lt;= 0:\n        win_sec = 1.0\n    guard_sec = min(max(win_sec, 0.5), 2.0)\n    min_chunk_samples = max(int(2 * guard_sec * self.sampling_rate), 2)\n\n    if chunk.shape[0] &lt; min_chunk_samples:\n        warnings.warn(\n            \"chunk too short for stable DSS cleaning; skipping component removal.\",\n            RuntimeWarning,\n        )\n        return chunk.copy(), 0, np.empty(0, dtype=float)\n\n    # Ensure self.config has all necessary default parameters\n    config_defaults = {\n        'nfft': 1024,\n        'nkeep': None,\n        'niterations': 1,\n        'fig1': 100,\n        'fig2': 101,\n        'adaptiveNremove': 1,\n        'noiseCompDetectSigma': 3,\n        'fixedNremove': 1,  # Default nremove is 1\n        'plotflag': False   # Set to True to enable plotting\n    }\n    for key, value in config_defaults.items():\n        if key not in self.config or self.config[key] is None:\n            self.config[key] = value\n\n    # Effective nfft: match MATLAB behavior by reducing nfft if chunk is short\n    nfft_cfg = int(self.config.get('nfft', 1024))\n    nfft = min(nfft_cfg, int(len(chunk)))\n\n    # Step 1: Define line frequency normalized to sampling rate\n    fline = noise_freq / self.sampling_rate\n\n    # Check that fline is less than Nyquist frequency\n    if fline &gt;= 0.5:\n        raise ValueError('fline should be less than Nyquist frequency (sampling_rate / 2)')\n\n    # Step 2: Apply smoothing to remove line frequency and harmonics\n    smoothed_chunk = self.nt_smooth(\n        chunk,\n        T=1 / fline,\n        n_iterations=self.config['niterations']\n    )\n\n    # Step 3: Compute the residual after smoothing\n    residual_chunk = chunk - smoothed_chunk\n\n    # Step 4: PCA to reduce dimensionality and avoid overfitting\n    nkeep_val = self.config.get('nkeep')\n    if nkeep_val in (None, 0) or (isinstance(nkeep_val, (int, float)) and nkeep_val &lt;= 0):\n        nkeep_val = residual_chunk.shape[1]\n        self.config['nkeep'] = nkeep_val\n    else:\n        nkeep_val = int(nkeep_val)\n    truncated_chunk, _ = self.nt_pca(\n        residual_chunk,\n        nkeep=nkeep_val\n    )\n\n    # Step 5: DSS to isolate line components from residual\n    n_harmonics = int(np.floor(0.5 / fline))\n    if self.config.get('snapDssToFftBin', False):\n        # Snap each harmonic to the nearest FFT bin index consistent with MATLAB round: floor(x+0.5)\n        harmonic_freqs = []\n        for k in range(1, n_harmonics + 1):\n            f_norm = fline * k\n            idx = int(np.floor(f_norm * nfft + 0.5))\n            idx = max(0, min(idx, nfft // 2))\n            # Inverse mapping to normalized frequency that re-yields idx in nt_bias_fft\n            f_norm_snap = (idx - 0.5) / nfft if idx &gt; 0 else 0.0\n            harmonic_freqs.append(f_norm_snap)\n        harmonics = np.array(harmonic_freqs, dtype=float)\n    else:\n        harmonics = fline * np.arange(1, n_harmonics + 1)\n    c0, c1 = self.nt_bias_fft(\n        truncated_chunk,\n        freq=harmonics,\n        nfft=nfft\n    )\n    # Optional: save DSS matrices/scores for parity debugging\n    if self.config.get('saveDssDebug', False):\n        try:\n            import os\n            out_dir = self.config.get('debugOutDir') or os.path.join(os.getcwd(), 'comparisons', 'debug')\n            os.makedirs(out_dir, exist_ok=True)\n            # Use noise freq and chunk length as part of the name\n            tag = f\"f{noise_freq:.6f}_n{len(chunk)}\"\n            np.savez(\n                os.path.join(out_dir, f\"dss_mats_{tag}.npz\"),\n                c0=c0, c1=c1,\n            )\n        except Exception:\n            pass\n    todss, pwr0, pwr1 = self.nt_dss0(c0, c1)\n    scores = pwr1 / pwr0\n\n    # Optional plotting of DSS scores\n    if self.config['plotflag']:\n        plt.figure(self.config['fig1'])\n        plt.clf()\n        plt.plot(scores, '.-')\n        plt.xlabel('Component')\n        plt.ylabel('Score')\n        plt.title('DSS to enhance line frequencies')\n        # plt.savefig('dss_scores.png')\n\n    # Step 6: Determine the number of components to remove\n    if scores is None or len(scores) == 0:\n        nremove = 0\n    elif self.config['adaptiveNremove']:\n        adaptive_nremove, _ = self.iterative_outlier_removal(\n            scores,\n            self.config['noiseCompDetectSigma']\n        )\n        if adaptive_nremove &lt; self.config['fixedNremove']:\n            nremove = self.config['fixedNremove']\n            print(\n                f\"Fixed nremove ({self.config['fixedNremove']}) is larger than adaptive nremove, using fixed nremove!\"\n            )\n        else:\n            nremove = adaptive_nremove\n        # Cap removal to at most 1/5 of components, but ensure at least 1 when any components exist\n        if nremove &gt; len(scores) // 5:\n            nremove = max(1, len(scores) // 5)\n            print(\n                f\"nremove is larger than 1/5th of the components, using that ({nremove})!\"\n            )\n    else:\n        nremove = self.config['fixedNremove']\n\n    # Step 7: Project the line-dominated components out of the residual\n    if nremove &gt; 0:\n        # Get line-dominated components\n        line_components = self.nt_mmat(\n            truncated_chunk,\n            todss[:, :nremove]\n        )\n        # Project them out\n        projected_chunk,_,_= self.nt_tsr(\n            residual_chunk,\n            line_components\n        )\n        # Reconstruct the clean signal\n        clean_chunk = smoothed_chunk + projected_chunk\n    else:\n        clean_chunk = chunk\n\n    # Optional plotting of spectra\n    if self.config['plotflag']:\n        # Normalize data for plotting\n        norm_factor = np.sqrt(np.mean(chunk ** 2))\n        chunk_norm = chunk / norm_factor\n        clean_chunk_norm = clean_chunk / norm_factor\n        removed_chunk_norm = (chunk - clean_chunk) / norm_factor\n\n        # Compute spectra\n        pxx_chunk, f = self.nt_spect_plot(\n            chunk_norm,\n            nfft=self.config['nfft'],\n            fs=self.sampling_rate,\n            return_data=True\n        )\n        pxx_clean, _ = self.nt_spect_plot(\n            clean_chunk_norm,\n            nfft=self.config['nfft'],\n            fs=self.sampling_rate,\n            return_data=True\n        )\n        pxx_removed, _ = self.nt_spect_plot(\n            removed_chunk_norm,\n            nfft=self.config['nfft'],\n            fs=self.sampling_rate,\n            return_data=True\n        )\n\n        divisor = np.sum(pxx_chunk, axis=0)\n        # Plot original spectrum\n        plt.figure(self.config['fig2'])\n        plt.clf()\n        plt.subplot(1, 2, 1)\n        plt.semilogy(f, np.abs(pxx_chunk) / divisor, label='Original', color='k')\n        plt.xlabel('Frequency (Hz)')\n        plt.ylabel('Relative Power')\n        plt.legend()\n        plt.grid(True, which='both', axis='both')\n        yl1 = plt.ylim()\n\n        # Plot cleaned and removed spectra\n        plt.subplot(1, 2, 2)\n        plt.semilogy(f, np.abs(pxx_clean) / divisor, label='Clean', color='g')\n        if nremove != 0:\n            plt.semilogy(f, np.abs(pxx_removed) / divisor, label='Removed', color='r')\n            plt.legend()\n        plt.xlabel('Frequency (Hz)')\n        plt.grid(True, which='both', axis='both')\n        yl2 = plt.ylim()\n\n        # Adjust y-limits to be the same\n        yl_min = min(yl1[0], yl2[0])\n        yl_max = max(yl1[1], yl2[1])\n        plt.subplot(1, 2, 1)\n        plt.ylim([yl_min, yl_max])\n        plt.subplot(1, 2, 2)\n        plt.ylim([yl_min, yl_max])\n\n        plt.show()\n\n    return clean_chunk, nremove, scores\n</code></pre>"},{"location":"api/utilities/#pyzaplineplus.core.PyZaplinePlus.bandpass_filter","title":"<code>bandpass_filter(data, lowcut, highcut, fs, order=5)</code>","text":"<p>Apply a bandpass filter to the data.</p> Source code in <code>pyzaplineplus/core.py</code> <pre><code>def bandpass_filter(self, data, lowcut, highcut, fs, order=5):\n    \"\"\"\n    Apply a bandpass filter to the data.\n    \"\"\"\n    nyq = 0.5 * fs\n    # Normalize with guards against invalid edges and Nyquist issues\n    low_raw = lowcut / nyq\n    high_raw = highcut / nyq\n    low = max(1e-9, min(low_raw, 0.999999))\n    high = max(1e-9, min(high_raw, 0.999999))\n    # If requested band exceeded Nyquist, clip and warn once\n    if (low_raw != low) or (high_raw != high):\n        if not self._warned_nyquist:\n            warnings.warn(\n                \"Requested band exceeded [0, fs/2]; clipping to valid range.\",\n                RuntimeWarning,\n            )\n            self._warned_nyquist = True\n    # If invalid band, return data unchanged (no-op)\n    if high &lt;= low:\n        return data\n    b, a = signal.butter(order, [low, high], btype='band')\n    return signal.filtfilt(b, a, data, axis=0)\n</code></pre>"},{"location":"api/utilities/#pyzaplineplus.core.PyZaplinePlus.compute_analytics","title":"<code>compute_analytics(pxx_raw_log, pxx_clean_log, f, noise_freq)</code>","text":"<p>Compute analytics to evaluate the cleaning process.</p> Source code in <code>pyzaplineplus/core.py</code> <pre><code>def compute_analytics(self, pxx_raw_log, pxx_clean_log, f, noise_freq):\n    \"\"\"\n    Compute analytics to evaluate the cleaning process.\n    \"\"\"\n    # Overall power removed (in log space)\n    proportion_removed = 1 - 10 ** ((np.mean(pxx_clean_log) - np.mean(pxx_raw_log)) / 10)\n\n    # Frequency range to evaluate below noise frequency\n    freq_idx_below_noise = (f &gt;= max(noise_freq - 11, 0)) &amp; (f &lt;= noise_freq - 1)\n    proportion_removed_below_noise = 1 - 10 ** (\n        (np.mean(pxx_clean_log[freq_idx_below_noise, :]) - np.mean(pxx_raw_log[freq_idx_below_noise, :])) / 10\n    )\n\n    # Frequency range at noise frequency\n    freq_idx_noise = (f &gt; noise_freq + self.config['detailedFreqBoundsUpper'][0]) &amp; \\\n                    (f &lt; noise_freq + self.config['detailedFreqBoundsUpper'][1])\n    proportion_removed_noise = 1 - 10 ** (\n        (np.mean(pxx_clean_log[freq_idx_noise, :]) - np.mean(pxx_raw_log[freq_idx_noise, :])) / 10\n    )\n\n    # Ratio of noise power to surroundings before and after cleaning\n    freq_idx_noise_surrounding = (\n        ((f &gt; noise_freq - (self.config['detectionWinsize'] / 2)) &amp; (f &lt; noise_freq - (self.config['detectionWinsize'] / 6))) |\n        ((f &gt; noise_freq + (self.config['detectionWinsize'] / 6)) &amp; (f &lt; noise_freq + (self.config['detectionWinsize'] / 2)))\n    )\n\n    mean_pxx_raw_noise = np.mean(pxx_raw_log[freq_idx_noise, :])\n    mean_pxx_raw_noise_surrounding = np.mean(pxx_raw_log[freq_idx_noise_surrounding, :])\n    ratio_noise_raw = 10 ** ((mean_pxx_raw_noise - mean_pxx_raw_noise_surrounding) / 10)\n\n    mean_pxx_clean_noise = np.mean(pxx_clean_log[freq_idx_noise, :])\n    mean_pxx_clean_noise_surrounding = np.mean(pxx_clean_log[freq_idx_noise_surrounding, :])\n    ratio_noise_clean = 10 ** ((mean_pxx_clean_noise - mean_pxx_clean_noise_surrounding) / 10)\n\n    return {\n        'proportion_removed': proportion_removed,\n        'proportion_removed_below_noise': proportion_removed_below_noise,\n        'proportion_removed_noise': proportion_removed_noise,\n        'ratio_noise_raw': ratio_noise_raw,\n        'ratio_noise_clean': ratio_noise_clean\n    }\n</code></pre>"},{"location":"api/utilities/#pyzaplineplus.core.PyZaplinePlus.compute_spectrum","title":"<code>compute_spectrum(data)</code>","text":"<p>Compute the power spectral density of the input data.</p> Notes <p>Uses a Hamming window with 50% overlap to match MATLAB pwelch (periodic form). This ensures parity with MATLAB outputs.</p> Source code in <code>pyzaplineplus/core.py</code> <pre><code>def compute_spectrum(self, data):\n    \"\"\"\n    Compute the power spectral density of the input data.\n\n    Notes\n    -----\n    Uses a Hamming window with 50% overlap to match MATLAB pwelch\n    (periodic form). This ensures parity with MATLAB outputs.\n    \"\"\"\n    # Set window length and overlap to match MATLAB code\n    nperseg = int(self.config['winSizeCompleteSpectrum'] * self.sampling_rate)\n\n    # Ensure nperseg doesn't exceed data length\n    max_nperseg = data.shape[0]\n    if nperseg &gt; max_nperseg:\n        nperseg = max_nperseg\n\n    # Ensure minimum nperseg for meaningful spectrum\n    if nperseg &lt; 8:\n        nperseg = min(8, max_nperseg)\n\n    f, pxx = _welch_hamming_periodic(\n        data, fs=self.sampling_rate, nperseg=nperseg, axis=0\n    )\n\n    # Log transform\n    pxx_log = 10 * np.log10(pxx)\n\n    return pxx_log, f\n</code></pre>"},{"location":"api/utilities/#pyzaplineplus.core.PyZaplinePlus.detect_chunk_noise","title":"<code>detect_chunk_noise(chunk, noise_freq, return_details=False)</code>","text":"<p>Detect noise frequency in a given chunk.</p> <p>Returns:</p> Type Description <code>float or tuple</code> <p>If <code>return_details</code> is <code>False</code> (default) the function returns the frequency that should be cleaned for the chunk. When <code>return_details</code> is <code>True</code> a tuple <code>(chunk_noise_freq, found_flag, peak_freq)</code> matching the MATLAB bookkeeping is returned.</p> Source code in <code>pyzaplineplus/core.py</code> <pre><code>def detect_chunk_noise(self, chunk, noise_freq, return_details=False):\n    \"\"\"\n    Detect noise frequency in a given chunk.\n\n    Returns\n    -------\n    float or tuple\n        If ``return_details`` is ``False`` (default) the function returns the\n        frequency that should be cleaned for the chunk. When\n        ``return_details`` is ``True`` a tuple ``(chunk_noise_freq,\n        found_flag, peak_freq)`` matching the MATLAB bookkeeping is\n        returned.\n    \"\"\"\n    # Use Hann periodic + 50% overlap to match MATLAB parity\n    nperseg = len(chunk)\n    f, pxx_chunk = _welch_hamming_periodic(\n        chunk, fs=self.sampling_rate, nperseg=nperseg, axis=0\n    )\n    pxx_log = 10 * np.log10(pxx_chunk)\n\n    freq_idx = (f &gt; noise_freq - self.config['detectionWinsize'] / 2) &amp; (f &lt; noise_freq + self.config['detectionWinsize'] / 2)\n    detailed_freq_idx = (f &gt; noise_freq + self.config['detailedFreqBoundsUpper'][0]) &amp; \\\n                        (f &lt; noise_freq + self.config['detailedFreqBoundsUpper'][1])\n    detailed_freqs = f[detailed_freq_idx]\n\n    # Guard small-bin and empty-slice cases to avoid crashes on short chunks\n    n_bins_fine = int(np.sum(freq_idx))\n    if n_bins_fine &lt; 3:\n        # Not enough resolution: report no detection and keep global frequency\n        chunk_noise_freq = float(noise_freq)\n        found_flag = 0\n        peak_freq = float(noise_freq)\n        return (\n            (chunk_noise_freq, found_flag, peak_freq)\n            if return_details\n            else chunk_noise_freq\n        )\n\n    fine_data = np.mean(pxx_log[freq_idx, :], axis=1)\n    third = max(1, len(fine_data) // 3)\n    # Avoid quantiles on empty slices\n    center_data = float(np.mean([fine_data[:third], fine_data[-third:]]))\n    lower_quantile = float(np.mean([\n        np.quantile(fine_data[:third], 0.05),\n        np.quantile(fine_data[-third:], 0.05),\n    ]))\n    detailed_thresh = center_data + self.config['freqDetectMultFine'] * (center_data - lower_quantile)\n\n    # If detailed freq slice is empty, keep the original frequency\n    if detailed_freqs.size &lt; 1:\n        chunk_noise_freq = float(noise_freq)\n        found_flag = 0\n        peak_freq = float(noise_freq)\n        return (\n            (chunk_noise_freq, found_flag, peak_freq)\n            if return_details\n            else chunk_noise_freq\n        )\n\n    # Compute detailed-band max and peak\n    detailed_band_mean = np.mean(pxx_log[detailed_freq_idx, :], axis=1)\n    max_fine_power: float = float(np.max(detailed_band_mean))\n    peak_idx = int(np.argmax(detailed_band_mean))\n    peak_freq = float(detailed_freqs[peak_idx])\n    if max_fine_power &gt; detailed_thresh:\n        chunk_noise_freq = peak_freq\n        found_flag = 1\n        return (\n            (chunk_noise_freq, found_flag, peak_freq)\n            if return_details\n            else chunk_noise_freq\n        )\n    # No strong local peak found: clean with global noise_freq but report peak\n    chunk_noise_freq = float(noise_freq)\n    found_flag = 0\n    return (\n        (chunk_noise_freq, found_flag, peak_freq)\n        if return_details\n        else chunk_noise_freq\n    )\n</code></pre>"},{"location":"api/utilities/#pyzaplineplus.core.PyZaplinePlus.detect_flat_channels","title":"<code>detect_flat_channels()</code>","text":"<p>Detect and return indices of flat channels in the data.</p> Source code in <code>pyzaplineplus/core.py</code> <pre><code>def detect_flat_channels(self):\n    \"\"\"\n    Detect and return indices of flat channels in the data.\n    \"\"\"\n    diff_data = np.diff(self.data, axis=0)\n    flat_channels = np.where(np.all(diff_data == 0, axis=0))[0]\n    if len(flat_channels) &gt; 0:\n        print(f\"Flat channels detected (will be ignored and added back in after Zapline-plus processing): {flat_channels}\")\n        self.data = np.delete(self.data, flat_channels, axis=1)\n    return flat_channels\n</code></pre>"},{"location":"api/utilities/#pyzaplineplus.core.PyZaplinePlus.detect_line_noise","title":"<code>detect_line_noise()</code>","text":"<p>Detect line noise (50 Hz or 60 Hz) in the data.</p> <p>Notes: - The MATLAB implementation searches within narrow bands around 50/60 Hz.   To robustly capture the true peak (which might fall exactly on-bin), we   use a slightly wider 2 Hz window: 49\u201351 Hz and 59\u201361 Hz.</p> Source code in <code>pyzaplineplus/core.py</code> <pre><code>def detect_line_noise(self):\n    \"\"\"\n    Detect line noise (50 Hz or 60 Hz) in the data.\n\n    Notes:\n    - The MATLAB implementation searches within narrow bands around 50/60 Hz.\n      To robustly capture the true peak (which might fall exactly on-bin), we\n      use a slightly wider 2 Hz window: 49\u201351 Hz and 59\u201361 Hz.\n    \"\"\"\n    if self.config['noisefreqs'] != 'line':\n        return\n    # Use a robust 2 Hz window around 50 Hz and 60 Hz\n    idx = ((self.f &gt; 49) &amp; (self.f &lt; 51)) | ((self.f &gt; 59) &amp; (self.f &lt; 61))\n    if not np.any(idx):\n        # As a fallback, try the original narrow bounds\n        idx = ((self.f &gt; 49) &amp; (self.f &lt; 50)) | ((self.f &gt; 59) &amp; (self.f &lt; 60))\n    if not np.any(idx):\n        # Still nothing: warn and skip narrowing; allow automatic detection later\n        print(\"Warning: No bins found around 50/60 Hz for 'line' detection; falling back to automatic detection.\")\n        self.config['noisefreqs'] = []\n        return None\n    spectra_chunk = self.pxx_raw_log[idx, :]\n    # Flatten across channels and pick global maximum within the search band\n    flat_idx = np.argmax(spectra_chunk)\n    row_idx = np.unravel_index(flat_idx, spectra_chunk.shape)[0]\n    noise_freq = self.f[idx][row_idx]\n    print(f\"'noisefreqs' parameter was set to 'line', found line noise candidate at {noise_freq:.2f} Hz!\")\n    self.config['noisefreqs'] = []\n    self.config['minfreq'] = noise_freq - self.config['detectionWinsize'] / 2\n    self.config['maxfreq'] = noise_freq + self.config['detectionWinsize'] / 2\n    return noise_freq\n</code></pre>"},{"location":"api/utilities/#pyzaplineplus.core.PyZaplinePlus.detect_noise_frequencies","title":"<code>detect_noise_frequencies()</code>","text":"<p>Automatically detect noise frequencies in the data.</p> Source code in <code>pyzaplineplus/core.py</code> <pre><code>def detect_noise_frequencies(self):\n    \"\"\"\n    Automatically detect noise frequencies in the data.\n    \"\"\"\n    noise_freqs = []\n    threshs=[]\n    current_minfreq = self.config['minfreq']\n    self.config['automaticFreqDetection'] = True\n    while True:\n        noisefreq, _, _, thresh = find_next_noisefreq(\n            self.pxx_raw_log,\n            self.f,\n            current_minfreq,\n            self.config['coarseFreqDetectPowerDiff'],\n            self.config['detectionWinsize'],\n            self.config['maxfreq'],\n            self.config['coarseFreqDetectLowerPowerDiff'],\n            verbose=False\n        )\n\n        if noisefreq is None:\n            break\n\n        noise_freqs.append(noisefreq)\n        threshs.append(thresh)\n        current_minfreq = noisefreq + self.config['detectionWinsize'] / 2\n\n        if current_minfreq &gt;= self.config['maxfreq']:\n            break\n    self.config['thresh'] = threshs\n    return noise_freqs\n</code></pre>"},{"location":"api/utilities/#pyzaplineplus.core.PyZaplinePlus.finalize_inputs","title":"<code>finalize_inputs()</code>","text":"<p>Finalize and prepare inputs for the Zapline-plus algorithm.</p> Source code in <code>pyzaplineplus/core.py</code> <pre><code>def finalize_inputs(self):\n    \"\"\"\n    Finalize and prepare inputs for the Zapline-plus algorithm.\n    \"\"\"\n    # Check and adjust sampling rate\n    if self.sampling_rate &gt; 500:\n        print(\"WARNING: It is recommended to downsample the data to around 250Hz to 500Hz before applying Zapline-plus!\")\n        print(f\"Current sampling rate is {self.sampling_rate}. Results may be suboptimal!\")\n\n    # Transpose data if necessary\n    self.transpose_data = self.data.shape[1] &gt; self.data.shape[0]\n    if self.transpose_data:\n        self.data = self.data.T\n\n    # Adjust window size for spectrum calculation to mirror MATLAB behavior\n    # MATLAB ensures at least 8 segments for pwelch by setting the window to ~1/8 of data length\n    if self.config['winSizeCompleteSpectrum'] * self.sampling_rate &gt; (self.data.shape[0] / 8):\n        new_win = int(np.floor((self.data.shape[0] / self.sampling_rate) / 8))\n        self.config['winSizeCompleteSpectrum'] = max(new_win, 1)\n        print('Data set is short. Adjusted window size for whole data set spectrum calculation to be 1/8 of the length!')\n\n    # Set nkeep\n    if self.config['nkeep'] == 0:\n        self.config['nkeep'] = self.data.shape[1]\n\n    # Track the minimum allowed fixedNremove to support adaptive updates\n    if 'baseFixedNremove' not in self.config:\n        self.config['baseFixedNremove'] = int(self.config.get('fixedNremove', 1))\n\n    # Detect flat channels\n    self.flat_channels = self.detect_flat_channels()\n\n    # Compute initial spectrum\n    self.pxx_raw_log, self.f = self.compute_spectrum(self.data)\n\n    # Handle 'line' noise frequency\n    if self.config['noisefreqs'] == 'line':\n        self.detect_line_noise()\n\n\n    # Automatic noise frequency detection\n    if not self.config['noisefreqs']:\n        self.config['noisefreqs'] = self.detect_noise_frequencies()\n</code></pre>"},{"location":"api/utilities/#pyzaplineplus.core.PyZaplinePlus.fixed_chunk_detection","title":"<code>fixed_chunk_detection()</code>","text":"<p>Split the data into fixed-length chunks.</p> Source code in <code>pyzaplineplus/core.py</code> <pre><code>def fixed_chunk_detection(self):\n    \"\"\"\n    Split the data into fixed-length chunks.\n    \"\"\"\n    chunk_length_samples = int(self.config['chunkLength'] * self.sampling_rate)\n    chunk_indices = [0]\n    while chunk_indices[-1] + chunk_length_samples &lt; len(self.data):\n        chunk_indices.append(chunk_indices[-1] + chunk_length_samples)\n    chunk_indices.append(len(self.data))\n    return chunk_indices\n</code></pre>"},{"location":"api/utilities/#pyzaplineplus.core.PyZaplinePlus.iterative_outlier_removal","title":"<code>iterative_outlier_removal(data_vector, sd_level=3)</code>","text":"<p>Remove outliers in a vector based on an iterative sigma threshold approach.</p> Source code in <code>pyzaplineplus/core.py</code> <pre><code>def iterative_outlier_removal(self, data_vector, sd_level=3):\n    \"\"\"\n    Remove outliers in a vector based on an iterative sigma threshold approach.\n    \"\"\"\n    threshold_old: float = float(np.max(data_vector))\n    threshold: float = float(np.mean(data_vector) + sd_level * np.std(data_vector))\n    n_remove: int = 0\n\n    while threshold &lt; threshold_old:\n        flagged_points = data_vector &gt; threshold\n        data_vector = data_vector[~flagged_points]\n        n_remove += np.sum(flagged_points)\n        threshold_old = threshold\n        threshold = float(np.mean(data_vector) + sd_level * np.std(data_vector))\n\n    return n_remove, threshold\n</code></pre>"},{"location":"api/utilities/#pyzaplineplus.core.PyZaplinePlus.normalize_topcs","title":"<code>normalize_topcs(eigenvalues, topcs, threshold=None)</code>","text":"<p>Normalize and select top principal components based on a threshold.</p> <p>Parameters:     eigenvalues (np.ndarray): 1D array of eigenvalues.     topcs (np.ndarray): 2D array of top principal components (channels x PCs).     threshold (float, optional): Threshold value for selecting eigenvalues.</p> <p>Returns:     topcs_normalized (np.ndarray): Normalized top principal components.     eigenvalues_selected (np.ndarray): Selected eigenvalues after thresholding.</p> Source code in <code>pyzaplineplus/core.py</code> <pre><code>def normalize_topcs(self, eigenvalues, topcs, threshold=None):\n    \"\"\"\n    Normalize and select top principal components based on a threshold.\n\n    Parameters:\n        eigenvalues (np.ndarray): 1D array of eigenvalues.\n        topcs (np.ndarray): 2D array of top principal components (channels x PCs).\n        threshold (float, optional): Threshold value for selecting eigenvalues.\n\n    Returns:\n        topcs_normalized (np.ndarray): Normalized top principal components.\n        eigenvalues_selected (np.ndarray): Selected eigenvalues after thresholding.\n    \"\"\"\n    if threshold is not None and threshold &gt; 0:\n        # Ensure eigenvalues is 1D\n        if eigenvalues.ndim &gt; 1:\n            eigenvalues = np.diag(eigenvalues)\n\n        # Compute the ratio and create a boolean mask\n        ratio = eigenvalues / np.max(eigenvalues)\n        mask = ratio &gt; threshold  # Boolean array\n\n        # Debug statements (optional)\n        # print(f\"Eigenvalues: {eigenvalues}\")\n        # print(f\"Max Eigenvalue: {np.max(eigenvalues)}\")\n        # print(f\"Threshold: {threshold}\")\n        # print(f\"Ratio: {ratio}\")\n        # print(f\"Mask: {mask}\")\n\n        # Select indices where the condition is True\n        idx_thresh = np.where(mask)[0]\n\n        # Debug statement (optional)\n        # print(f\"Selected indices: {idx_thresh}\")\n\n        # Select columns in topcs based on idx_thresh\n        topcs_selected = topcs[:,idx_thresh]\n\n        # Select corresponding eigenvalues\n        eigenvalues_selected = eigenvalues[idx_thresh]\n\n        return topcs_selected, eigenvalues_selected\n    else:\n        # If no threshold is provided, return the original arrays\n        return topcs, eigenvalues\n</code></pre>"},{"location":"api/utilities/#pyzaplineplus.core.PyZaplinePlus.nt_bias_fft","title":"<code>nt_bias_fft(x, freq, nfft)</code>","text":"<p>Compute covariance matrices with and without filter bias using FFT.</p> <p>Parameters: - x (np.ndarray): Data matrix.     - 2D: (n_samples, n_channels)     - 3D: (n_samples, n_channels, n_trials) - freq (np.ndarray): Normalized frequencies to retain.     - 1D array: Individual frequencies.     - 2D array: Frequency bands with two rows (start and end frequencies). - nfft (int): FFT size.</p> <p>Returns: - c0 (np.ndarray): Unbiased covariance matrix. - c1 (np.ndarray): Biased covariance matrix after applying the frequency filter.</p> Source code in <code>pyzaplineplus/core.py</code> <pre><code>def nt_bias_fft(self, x: np.ndarray, freq: np.ndarray, nfft: int) -&gt; tuple:\n    \"\"\"\n    Compute covariance matrices with and without filter bias using FFT.\n\n    Parameters:\n    - x (np.ndarray): Data matrix.\n        - 2D: (n_samples, n_channels)\n        - 3D: (n_samples, n_channels, n_trials)\n    - freq (np.ndarray): Normalized frequencies to retain.\n        - 1D array: Individual frequencies.\n        - 2D array: Frequency bands with two rows (start and end frequencies).\n    - nfft (int): FFT size.\n\n    Returns:\n    - c0 (np.ndarray): Unbiased covariance matrix.\n    - c1 (np.ndarray): Biased covariance matrix after applying the frequency filter.\n    \"\"\"\n    from scipy.fft import fft\n    from scipy.signal import windows\n\n\n    # Input Validation\n    if np.max(freq) &gt; 0.5:\n        raise ValueError(\"Frequencies should be &lt;= 0.5\")\n    if nfft &gt; x.shape[0]:\n        raise ValueError(\"nfft too large\")\n\n    # Initialize Filter\n    filt: np.ndarray = np.zeros(nfft // 2 + 1)\n\n    if freq.ndim == 1:\n        for k in range(freq.shape[0]):\n            idx = int(np.floor(freq[k] * nfft + 0.5))\n            if idx &gt;= len(filt):\n                raise ValueError(f\"Frequency index {idx} out of bounds for filter of length {len(filt)}.\")\n            filt[idx] = 1\n    elif freq.shape[0] == 2:\n        for k in range(freq.shape[1]):\n            start_idx = int(np.floor(freq[0, k] * nfft + 0.5))\n            end_idx = int(np.floor(freq[1, k] * nfft + 0.5)) + 1\n            if start_idx &gt;= len(filt) or end_idx &gt; len(filt):\n                raise ValueError(f\"Frequency slice [{start_idx}:{end_idx}] out of bounds for filter of length {len(filt)}.\")\n            filt[start_idx:end_idx] = 1\n    else:\n        raise ValueError(\"freq should have one or two rows\")\n\n    # Symmetrize the Filter\n    # Build full-spectrum filter. Optionally force positive-bin-only for dev parity experiments.\n    if self.config.get('dss_positive_only', False) and freq.ndim == 1 and freq.size &gt; 0:\n        # Positive-bin-only mask: keep only exact positive bin indices, no mirrored negative bins\n        filt_full = np.zeros(nfft, dtype=float)\n        for k in range(freq.shape[0]):\n            idx = int(np.floor(freq[k] * nfft + 0.5))\n            filt_full[idx] = 1.0\n    else:\n        filt_full = np.concatenate([filt, np.flip(filt[1:-1])])\n\n    # Hann Window (symmetric to match MATLAB hanning(nfft))\n    w = windows.hann(nfft, sym=True)\n\n    # Handle 2D and 3D Data\n    if x.ndim == 2:\n        n_samples, n_channels = x.shape\n        n_trials = 1\n        x = x[:, :, np.newaxis]  # Convert to 3D for uniform processing\n    elif x.ndim == 3:\n        n_samples, n_channels, n_trials = x.shape\n    else:\n        raise ValueError(\"Input data `x` must be 2D or 3D.\")\n\n    # Compute c0: Unbiased Covariance Matrix\n    if n_trials &gt; 1:\n        x_reshaped = x.reshape(n_samples, n_channels * n_trials)\n    else:\n        x_reshaped = x.reshape(n_samples, n_channels)\n    c0,_= self.nt_cov(x_reshaped)\n\n    # Ensure c0 is 2D\n    if c0.ndim == 0:\n        c0 = np.array([[c0]])\n    elif c0.ndim == 1:\n        c0 = np.atleast_2d(c0)\n\n    # Initialize c1: Biased Covariance Matrix\n    c1 = np.zeros_like(c0)\n\n    # Calculate frame start indices (50% overlap). For MATLAB parity, restrict to\n    # full windows excluding the trailing partial pair so that frames = 2*floor(m/nfft)-3\n    # which matches observed behavior in zapline-plus' nt_bias_fft usage.\n    if self.config.get('dss_strict_frames', True):\n        windows_full = max(0, int(np.floor(n_samples / nfft) - 1))\n        max_start = int(max(0, windows_full * nfft))\n        starts = list(range(0, max_start + 1, nfft // 2))\n    else:\n        nframes = int(np.ceil((n_samples - nfft / 2) / (nfft / 2)))\n        starts = [int(min(k * (nfft // 2), n_samples - nfft)) for k in range(nframes)]\n\n    for trial in range(n_trials):\n        for idx in starts:\n            z = x[idx:idx + nfft, :, trial]  # (nfft, n_channels)\n            z = self.nt_vecmult(z,w)  # Apply Hann window\n            Z = fft(z, axis=0)\n            Z = self.nt_vecmult(Z, filt_full)  # Apply filter\n            cov_matrix = np.real(np.dot(Z.conj().T, Z))  # (n_channels, n_channels)\n            c1 += cov_matrix\n\n    # Optional window-energy normalization (dev parity control)\n    if self.config.get('dss_divide_by_sumw2', False):\n        from numpy import sum as npsum\n        denom = float(npsum(w**2))\n        if denom &gt; 0:\n            c1 = c1 / denom\n\n    return c0, c1\n</code></pre>"},{"location":"api/utilities/#pyzaplineplus.core.PyZaplinePlus.nt_cov","title":"<code>nt_cov(x, shifts=None, w=None)</code>","text":"<p>Calculate time-shifted covariance of the data.</p> <p>Parameters:     x (Union[np.ndarray, List[np.ndarray]]): Data matrix or list of matrices.         - If numeric:             - 1D: (n_samples,)             - 2D: (n_samples, n_channels)             - 3D: (n_samples, n_channels, n_trials)         - If list: list of 2D numpy arrays (cell array equivalent).     shifts (Union[List[int], np.ndarray]): Array-like of non-negative integer shifts.     w (Optional[Union[np.ndarray, List[np.ndarray]]]): Weights (optional).         - If numeric:             - 1D array for 1D or 2D <code>x</code>.             - 2D array for 3D <code>x</code>.         - If list: list of weight matrices corresponding to each cell.</p> <p>Returns:     Tuple[np.ndarray, float]: Covariance matrix and total weight.         - c: covariance matrix (numpy.ndarray) with shape (n_channels * nshifts, n_channels * nshifts).         - tw: total weight (float).</p> Source code in <code>pyzaplineplus/core.py</code> <pre><code>def nt_cov(\n    self,\n    x: Union[np.ndarray, List[np.ndarray]],\n    shifts: Optional[Union[List[int], np.ndarray]] = None,\n    w: Optional[Union[np.ndarray, List[np.ndarray]]] = None\n) -&gt; Tuple[np.ndarray, float]:\n    \"\"\"\n    Calculate time-shifted covariance of the data.\n\n    Parameters:\n        x (Union[np.ndarray, List[np.ndarray]]): Data matrix or list of matrices.\n            - If numeric:\n                - 1D: (n_samples,)\n                - 2D: (n_samples, n_channels)\n                - 3D: (n_samples, n_channels, n_trials)\n            - If list: list of 2D numpy arrays (cell array equivalent).\n        shifts (Union[List[int], np.ndarray]): Array-like of non-negative integer shifts.\n        w (Optional[Union[np.ndarray, List[np.ndarray]]]): Weights (optional).\n            - If numeric:\n                - 1D array for 1D or 2D `x`.\n                - 2D array for 3D `x`.\n            - If list: list of weight matrices corresponding to each cell.\n\n    Returns:\n        Tuple[np.ndarray, float]: Covariance matrix and total weight.\n            - c: covariance matrix (numpy.ndarray) with shape (n_channels * nshifts, n_channels * nshifts).\n            - tw: total weight (float).\n    \"\"\"\n    # Convert shifts to a NumPy array and flatten to 1D\n    if shifts is None:\n        shifts = np.array([0])\n    else:\n        shifts = np.asarray(shifts).flatten()\n    shifts = cast(np.ndarray, shifts)\n    if np.any(shifts &lt; 0):\n        raise ValueError(\"Shifts must be non-negative integers.\")\n    nshifts = len(shifts)\n\n    # Validate input data is not empty\n    if x is None or (isinstance(x, np.ndarray) and x.size == 0) or (isinstance(x, list) and len(x) == 0):\n        raise ValueError(\"Input data `x` is empty.\")\n\n    # Initialize covariance matrix and total weight\n    c = None\n    tw = 0.0\n\n    # Determine if input is a list (cell array) or numpy array\n    if isinstance(x, list):\n        # Handle list input (cell array equivalent)\n        if w is not None and not isinstance(w, list):\n            raise ValueError(\"Weights `w` must be a list if `x` is a list (cell array).\")\n\n        # Number of cells/trials not required explicitly here\n        # Determine number of channels\n        if len(x) == 0:\n            raise ValueError(\"Input list `x` is empty.\")\n        first_shape = x[0].shape\n        if first_shape == ():\n            n_channels = 1\n        elif x[0].ndim == 1:\n            n_channels = 1\n        elif x[0].ndim == 2:\n            n_channels = x[0].shape[1]\n        else:\n            raise ValueError(f\"Data in cell {0} has unsupported number of dimensions: {x[0].ndim}\")\n\n        c = np.zeros((n_channels * nshifts, n_channels * nshifts), dtype=np.float64)\n\n        for idx, data in enumerate(x):\n            # Validate data dimensions\n            if not isinstance(data, np.ndarray):\n                raise TypeError(f\"Element {idx} of input list `x` is not a numpy.ndarray.\")\n            if data.ndim != 2:\n                raise ValueError(f\"Data in cell {idx} must be 2D, got {data.ndim}D.\")\n            n_samples, n_channels_current = data.shape\n            if n_channels_current != n_channels:\n                raise ValueError(f\"All cells must have the same number of channels. Cell {idx} has {n_channels_current} channels, expected {n_channels}.\")\n\n            # Handle weights\n            if w is not None:\n                weight = w[idx]\n                if not isinstance(weight, np.ndarray):\n                    raise TypeError(f\"Weight for cell {idx} must be a numpy.ndarray.\")\n                if weight.size == 0:\n                    raise ValueError(f\"Weight for cell {idx} is empty.\")\n                if weight.ndim == 1:\n                    weight = weight[:, np.newaxis]  # Shape: (n_samples, 1)\n                elif weight.ndim == 2 and weight.shape[1] == 1:\n                    pass  # Shape is already (n_samples, 1)\n                else:\n                    raise ValueError(f\"Weight for cell {idx} must be 1D or 2D with a single column. Got shape {weight.shape}.\")\n            # Apply shifts\n            if not np.all(shifts == 0):\n                xx = self.nt_multishift(data, shifts)  # Shape: (n_samples * nshifts, n_channels)\n                if w is not None:\n                    ww = self.nt_multishift(weight, shifts)  # Shape: (n_samples * nshifts, 1)\n                    # Take the minimum weight across shifts for each sample\n                    ww_min = np.min(ww, axis=1, keepdims=True)  # Shape: (n_samples * nshifts, 1)\n                else:\n                    ww_min = np.ones((xx.shape[0], 1), dtype=np.float64)\n            else:\n                xx = data.copy()  # Shape: (n_samples, n_channels)\n                if w is not None:\n                    ww_min = weight.copy()  # Shape: (n_samples, 1)\n                else:\n                    ww_min = np.ones((xx.shape[0], 1), dtype=np.float64)\n\n            # Multiply each row by its corresponding weight\n            if w is not None:\n                xx = self.nt_vecmult(xx, ww_min)  # Shape: (time_shifted x channels)\n\n            # Accumulate covariance\n            c += np.dot(xx.T, xx)  # Shape: (n_channels * nshifts, n_channels * nshifts)\n\n            # Accumulate total weight\n            if w is not None:\n                if not np.all(shifts == 0):\n                    tw += np.sum(ww_min)\n                else:\n                    tw += np.sum(weight)\n            else:\n                tw += xx.shape[0]  # Number of samples\n\n    elif isinstance(x, np.ndarray):\n        # Handle NumPy array input\n        data = x.copy()\n        # original_shape not used; keep minimal state\n\n        # Determine data dimensionality\n        if data.ndim == 1:\n            data = data[:, np.newaxis]  # Shape: (n_samples, 1)\n            n_channels = 1\n            n_trials = 1\n        elif data.ndim == 2:\n            n_channels = data.shape[1]\n            n_trials = 1\n        elif data.ndim == 3:\n            n_channels = data.shape[1]\n            n_trials = data.shape[2]\n        else:\n            raise ValueError(f\"Input data has unsupported number of dimensions: {data.ndim}\")\n\n        # Preallocate covariance matrix\n        c = np.zeros((n_channels * nshifts, n_channels * nshifts), dtype=np.float64)\n\n        # Iterate over trials\n        for trial in range(n_trials):\n            if data.ndim == 3:\n                trial_data = data[:, :, trial]  # Shape: (n_samples, n_channels)\n                if w is not None:\n                    if not isinstance(w, np.ndarray):\n                        raise TypeError(\"Weights `w` must be a numpy.ndarray when `x` is a numpy.ndarray.\")\n                    trial_weight = w[:, trial]  # Shape: (n_samples,)\n            else:\n                trial_data = data.copy()  # Shape: (n_samples, n_channels)\n                if w is not None:\n                    if not isinstance(w, np.ndarray):\n                        raise TypeError(\"Weights `w` must be a numpy.ndarray when `x` is a numpy.ndarray.\")\n                    trial_weight = w.copy()  # Shape: (n_samples, 1)\n\n            # Apply shifts\n            if not np.all(shifts == 0):\n                xx = self.nt_multishift(trial_data, shifts)  # Shape: (n_samples * nshifts, n_channels)\n                if w is not None:\n                    ww = self.nt_multishift(trial_weight, shifts)  # Shape: (n_samples * nshifts, 1)\n                    # Take the minimum weight across shifts for each sample\n                    ww_min = np.min(ww, axis=1, keepdims=True)  # Shape: (n_samples * nshifts, 1)\n                else:\n                    ww_min = np.ones((xx.shape[0], 1), dtype=np.float64)\n            else:\n                xx = trial_data.copy()  # Shape: (n_samples, n_channels)\n                if w is not None:\n                    ww_min = trial_weight.copy()  # Shape: (n_samples, 1)\n                else:\n                    ww_min = np.ones((xx.shape[0], 1), dtype=np.float64)\n\n            # Multiply each row by its corresponding weight\n            if w is not None:\n                xx = self.nt_vecmult(xx, ww_min)  # Shape: (time_shifted x channels)\n\n            # Accumulate covariance\n            c += np.dot(xx.T, xx)  # Shape: (n_channels * nshifts, n_channels * nshifts)\n\n            # Accumulate total weight\n            if w is not None:\n                if not np.all(shifts == 0):\n                    tw += np.sum(ww_min)\n                else:\n                    tw += np.sum(trial_weight)\n            else:\n                tw += xx.shape[0]  # Number of samples\n\n    else:\n        raise TypeError(\"Input `x` must be a numpy.ndarray or a list of numpy.ndarray (cell array).\")\n\n    return c, tw\n</code></pre>"},{"location":"api/utilities/#pyzaplineplus.core.PyZaplinePlus.nt_demean","title":"<code>nt_demean(x, w=None)</code>","text":"<p>Remove weighted mean over columns.</p> <p>Parameters:     x (np.ndarray): Data array (time x channels x trials).     w (np.ndarray): Optional weights array (time x 1 x trials) or (time x channels x trials).</p> <p>Returns:     x_demeaned (np.ndarray): Demeaned data array.     mn (np.ndarray): Mean values that were removed.</p> Source code in <code>pyzaplineplus/core.py</code> <pre><code>def nt_demean(self, x, w=None):\n    \"\"\"\n    Remove weighted mean over columns.\n\n    Parameters:\n        x (np.ndarray): Data array (time x channels x trials).\n        w (np.ndarray): Optional weights array (time x 1 x trials) or (time x channels x trials).\n\n    Returns:\n        x_demeaned (np.ndarray): Demeaned data array.\n        mn (np.ndarray): Mean values that were removed.\n    \"\"\"\n    added_trial_dim = False\n    if x.ndim == 2:\n        x = x[:, :, np.newaxis]\n        added_trial_dim = True\n        # Only expand w if provided\n        if w is not None and w.ndim == 2:\n            w = w[:, :, np.newaxis]\n\n    # Interpret index-form weights if provided and smaller than time dimension\n    if w is not None and w.size &lt; x.shape[0]:\n        w_indices = w.flatten()\n        if np.min(w_indices) &lt; 0 or np.max(w_indices) &gt;= x.shape[0]:\n            raise ValueError('w interpreted as indices but values are out of range')\n        w_full = np.zeros((x.shape[0], 1, x.shape[2]))\n        w_full[w_indices, :, :] = 1\n        w = w_full\n\n    # Broadcast weights across trials if needed\n    if w is not None and w.ndim == 3 and w.shape[2] != x.shape[2]:\n        if w.shape[2] == 1 and x.shape[2] != 1:\n            w = np.tile(w, (1, 1, x.shape[2]))\n        else:\n            raise ValueError('w should have same number of trials as x, or be singleton in that dimension')\n\n    m, n, o = x.shape\n    x_unfolded = x.reshape(m, -1)  # Unfold x to 2D array (time x (channels*trials))\n\n    if w is None:\n        # Unweighted mean\n        mn = np.mean(x_unfolded, axis=0, keepdims=True)\n        x_demeaned = x_unfolded - mn\n    else:\n        # Ensure weights have shape (time, channels*trials)\n        if w.ndim == 3:\n            w_unfolded = w.reshape(m, -1)\n        elif w.ndim == 2:\n            # If provided as (time, 1), tile across channels*trials\n            if w.shape[1] == 1:\n                w_unfolded = np.tile(w, (1, x_unfolded.shape[1]))\n            else:\n                if w.shape[1] != x_unfolded.shape[1]:\n                    raise ValueError('Weight matrix should match unfolded data width')\n                w_unfolded = w\n        else:\n            raise ValueError('Weight matrix has invalid dimensions')\n\n        if w_unfolded.shape[0] != x_unfolded.shape[0]:\n            raise ValueError('x and w should have the same number of time samples')\n\n        sum_w = np.sum(w_unfolded, axis=0, keepdims=True) + np.finfo(float).eps\n        mn = np.sum(x_unfolded * w_unfolded, axis=0, keepdims=True) / sum_w\n        x_demeaned = x_unfolded - mn\n\n    x_demeaned = x_demeaned.reshape(m, n, o)\n    mn = mn.reshape(1, n, o)\n    if added_trial_dim:\n        x_demeaned = x_demeaned.squeeze(-1)  # Remove the last dimension if singleton\n        mn = mn.squeeze(-1)\n    return x_demeaned, mn\n</code></pre>"},{"location":"api/utilities/#pyzaplineplus.core.PyZaplinePlus.nt_dss0","title":"<code>nt_dss0(c0, c1, keep1=None, keep2=10 ** -9)</code>","text":"<p>Compute DSS from covariance matrices.</p> <p>Parameters: - c0: baseline covariance - c1: biased covariance - keep1: number of PCs to retain (default: all) - keep2: ignore PCs smaller than this threshold (default: 10^-9)</p> <p>Returns: - todss: matrix to convert data to normalized DSS components - pwr0: power per component (baseline) - pwr1: power per component (biased)</p> Source code in <code>pyzaplineplus/core.py</code> <pre><code>def nt_dss0(self, c0, c1, keep1=None, keep2=10**-9):\n    \"\"\"\n    Compute DSS from covariance matrices.\n\n    Parameters:\n    - c0: baseline covariance\n    - c1: biased covariance\n    - keep1: number of PCs to retain (default: all)\n    - keep2: ignore PCs smaller than this threshold (default: 10^-9)\n\n    Returns:\n    - todss: matrix to convert data to normalized DSS components\n    - pwr0: power per component (baseline)\n    - pwr1: power per component (biased)\n    \"\"\"\n    if c0.shape != c1.shape:\n        raise ValueError(\"c0 and c1 should have the same size\")\n    if c0.shape[0] != c0.shape[1]:\n        raise ValueError(\"c0 should be square\")\n    if np.any(np.isnan(c0)) or np.any(np.isnan(c1)):\n        raise ValueError(\"NaN in covariance matrices\")\n    if np.any(np.isinf(c0)) or np.any(np.isinf(c1)):\n        raise ValueError(\"INF in covariance matrices\")\n\n    # PCA and whitening matrix from the unbiased covariance\n    topcs1, evs1 = self.nt_pcarot(c0, keep1, keep2)\n    # Match MATLAB nt_dss0.m: take absolute eigenvalues before whitening\n    evs1 = np.abs(evs1)\n\n    # Truncate PCA series if needed (thresholding already handled inside nt_pcarot)\n    if keep1 is not None:\n        topcs1 = topcs1[:, :keep1]\n        evs1 = evs1[:keep1]\n\n    # Apply PCA and whitening to the biased covariance\n    evs1_sqrt=1.0 / np.sqrt(evs1)\n    N = np.diag(evs1_sqrt)\n    c2 = N.T @ topcs1.T @ c1 @ topcs1 @ N\n\n    # Matrix to convert PCA-whitened data to DSS\n    topcs2, evs2 = self.nt_pcarot(c2, keep1, keep2)\n\n    # DSS matrix (raw data to normalized DSS)\n    todss = topcs1 @ N @ topcs2\n    N2 = np.diag(todss.T @ c0 @ todss)\n    todss = todss @ np.diag(1.0 / np.sqrt(N2))  # Adjust so that components are normalized\n\n    # Power per DSS component (match MATLAB's nt_dss0.m formulation)\n    # pwr0 = sqrt(sum((c0' * todss).^2)), pwr1 = sqrt(sum((c1' * todss).^2))\n    P0 = c0.T @ todss\n    P1 = c1.T @ todss\n    pwr0 = np.sqrt(np.sum(P0 ** 2, axis=0))\n    pwr1 = np.sqrt(np.sum(P1 ** 2, axis=0))\n\n    return todss, pwr0, pwr1\n</code></pre>"},{"location":"api/utilities/#pyzaplineplus.core.PyZaplinePlus.nt_fold","title":"<code>nt_fold(x, epochsize)</code>","text":"<p>Converts a 2D matrix (time*trial x channel) back into a 3D matrix (time x channel x trial).</p> <p>Parameters:     x (np.ndarray): Input data. Should be 2D.     epochsize (int): Number of samples per trial.</p> <p>Returns:     y (np.ndarray): Folded data.</p> Source code in <code>pyzaplineplus/core.py</code> <pre><code>def nt_fold(self,x, epochsize):\n    \"\"\"\n    Converts a 2D matrix (time*trial x channel) back into a 3D matrix (time x channel x trial).\n\n    Parameters:\n        x (np.ndarray): Input data. Should be 2D.\n        epochsize (int): Number of samples per trial.\n\n    Returns:\n        y (np.ndarray): Folded data.\n    \"\"\"\n    if x.size == 0:\n        return np.array([])\n    else:\n        if x.shape[0] / epochsize &gt; 1:\n            trials = int(x.shape[0] / epochsize)\n            y = np.transpose(np.reshape(x, (epochsize, trials, x.shape[1])), (0, 2, 1))\n        else:\n            y = x\n    return y\n</code></pre>"},{"location":"api/utilities/#pyzaplineplus.core.PyZaplinePlus.nt_mmat","title":"<code>nt_mmat(x, m)</code>","text":"<p>Matrix multiplication (with convolution).</p> <p>Parameters:     x (np.ndarray): Input data (can be 2D or multi-dimensional).     m (np.ndarray): Matrix to apply.                     - If 2D: Right multiply x by m.                     - If 3D: Perform convolution-like operation along time.</p> <p>Returns:     y (np.ndarray): Result after applying m to x.</p> Source code in <code>pyzaplineplus/core.py</code> <pre><code>def nt_mmat(self, x, m):\n    \"\"\"\n    Matrix multiplication (with convolution).\n\n    Parameters:\n        x (np.ndarray): Input data (can be 2D or multi-dimensional).\n        m (np.ndarray): Matrix to apply.\n                        - If 2D: Right multiply x by m.\n                        - If 3D: Perform convolution-like operation along time.\n\n    Returns:\n        y (np.ndarray): Result after applying m to x.\n    \"\"\"\n    # Handle the case where x is a list (similar to cell arrays in MATLAB)\n    if isinstance(x, list):\n        return [self.nt_mmat(xi, m) for xi in x]\n\n    # Handle multi-dimensional x beyond 3D\n    if x.ndim &gt; 3:\n        # Reshape x to 3D (time x channels x combined other dimensions)\n        original_shape = x.shape\n        time_dim = original_shape[0]\n        chan_dim = original_shape[1]\n        other_dims = original_shape[2:]\n        x = x.reshape(time_dim, chan_dim, -1)\n        y = self.nt_mmat(x, m)\n        # Reshape y back to original dimensions\n        y_shape = (y.shape[0], y.shape[1]) + other_dims\n        y = y.reshape(y_shape)\n        return y\n\n    # If m is 2D, perform simple matrix multiplication using\n    if m.ndim == 2:\n        y = self.nt_mmat0(x, m)\n\n        # Ensure y has the correct shape by removing any singleton dimensions\n        if y.ndim == 3 and y.shape[2] == 1:\n            y = y.squeeze(-1)\n        elif y.ndim == 2:\n            pass  # Already correct\n        else:\n            # Handle unexpected dimensions\n            y = np.squeeze(y)\n\n        return y\n\n    else:\n        # Convolution-like operation when m is 3D\n        n_rows, n_cols, n_lags = m.shape\n\n        if x.ndim == 2:\n            x = x[:, :, np.newaxis]  # Add trial dimension\n        n_samples, n_chans, n_trials = x.shape\n\n        if n_chans != n_rows:\n            raise ValueError(\"Number of channels in x must match number of rows in m.\")\n\n        # Initialize output array\n        y = np.zeros((n_samples + n_lags - 1, n_cols, n_trials))\n\n        # Perform convolution-like operation\n        for trial in range(n_trials):\n            x_trial = x[:, :, trial]  # Shape: (n_samples, n_chans)\n            y_trial = np.zeros((n_samples + n_lags - 1, n_cols))\n\n            # Unfold x_trial to 2D\n            x_unfolded = x_trial  # Shape: (n_samples, n_chans)\n\n            for lag in range(n_lags):\n                m_lag = m[:, :, lag]  # Shape: (n_rows, n_cols)\n                # Shift x_trial by lag\n                x_shifted = np.zeros((n_samples + n_lags - 1, n_chans))\n                x_shifted[lag:lag + n_samples, :] = x_unfolded\n\n                # Multiply and accumulate\n                y_partial = x_shifted @ m_lag  # Shape: (n_samples + n_lags - 1, n_cols)\n                y_trial += y_partial\n\n            y[:, :, trial] = y_trial\n\n        # If trials dimension was added artificially, remove it\n        if n_trials == 1:\n            y = y.squeeze(-1)  # Remove the last dimension if singleton\n\n        return y\n</code></pre>"},{"location":"api/utilities/#pyzaplineplus.core.PyZaplinePlus.nt_mmat0","title":"<code>nt_mmat0(x, m)</code>","text":"<p>Performs matrix multiplication after unfolding x, then folds the result back.</p> <p>Parameters:     x (np.ndarray): Input data. Can be 2D or 3D.     m (np.ndarray): Matrix to multiply with. Should be 2D.</p> <p>Returns:     y (np.ndarray): Result after multiplication and folding.</p> Source code in <code>pyzaplineplus/core.py</code> <pre><code>def nt_mmat0(self,x, m):\n    \"\"\"\n    Performs matrix multiplication after unfolding x, then folds the result back.\n\n    Parameters:\n        x (np.ndarray): Input data. Can be 2D or 3D.\n        m (np.ndarray): Matrix to multiply with. Should be 2D.\n\n    Returns:\n        y (np.ndarray): Result after multiplication and folding.\n    \"\"\"\n    unfolded_x = self.nt_unfold(x)\n    multiplied = unfolded_x @ m\n    epochsize = x.shape[0]   # Assuming epochsize is the first dimension\n    folded_y = self.nt_fold(multiplied, epochsize)\n    return folded_y\n</code></pre>"},{"location":"api/utilities/#pyzaplineplus.core.PyZaplinePlus.nt_multishift","title":"<code>nt_multishift(x, shifts)</code>","text":"<p>Apply multiple shifts to a matrix.</p> <p>Parameters: x (numpy.ndarray): Input data to shift. Shape can be 1D, 2D, or 3D.                     - 1D: (samples,)                     - 2D: (samples, channels)                     - 3D: (samples, channels, trials) shifts (array-like): Array of non-negative integer shifts.</p> <p>Returns: numpy.ndarray: Shifted data with increased channel dimension.             - 1D input becomes 2D: (samples_shifted, shifts.size)             - 2D input becomes 3D: (samples_shifted, channels * shifts.size, trials)</p> Source code in <code>pyzaplineplus/core.py</code> <pre><code>def nt_multishift(self,x, shifts):\n    \"\"\"\n    Apply multiple shifts to a matrix.\n\n    Parameters:\n    x (numpy.ndarray): Input data to shift. Shape can be 1D, 2D, or 3D.\n                        - 1D: (samples,)\n                        - 2D: (samples, channels)\n                        - 3D: (samples, channels, trials)\n    shifts (array-like): Array of non-negative integer shifts.\n\n    Returns:\n    numpy.ndarray: Shifted data with increased channel dimension.\n                - 1D input becomes 2D: (samples_shifted, shifts.size)\n                - 2D input becomes 3D: (samples_shifted, channels * shifts.size, trials)\n    \"\"\"\n    x = np.asarray(x)\n    shifts = np.asarray(shifts).flatten()\n    nshifts = shifts.size\n\n    # Input validation\n    if np.any(shifts &lt; 0):\n        raise ValueError('Shifts should be non-negative')\n    if x.shape[0] &lt; np.max(shifts):\n        raise ValueError('Shifts should be no larger than the number of time samples in x')\n\n    # Handle different input dimensions by expanding to 3D\n    if x.ndim == 1:\n        x = x[:, np.newaxis, np.newaxis]  # (samples, 1, 1)\n    elif x.ndim == 2:\n        x = x[:, :, np.newaxis]  # (samples, channels, 1)\n    elif x.ndim &gt; 3:\n        raise ValueError('Input data has more than 3 dimensions, which is not supported.')\n\n    m, n, o = x.shape  # samples x channels x trials\n\n    # If only one shift and it's zero, return the original data\n    if nshifts == 1 and shifts[0] == 0:\n        return x.squeeze(axis=-1)\n\n    max_shift: int = int(np.max(shifts))\n    N: int = m - max_shift  # Number of samples after shifting\n\n    # Initialize output array\n    z = np.empty((N, n * nshifts, o), dtype=x.dtype)\n\n    for trial in range(o):\n        for channel in range(n):\n            y = x[:, channel, trial]  # (samples,)\n            for s_idx, shift in enumerate(shifts):\n                if shift == 0:\n                    shifted_y = y[:N]\n                else:\n                    shifted_y = y[shift:shift + N]\n                # Place the shifted data in the correct position\n                z[:, channel * nshifts + s_idx, trial] = shifted_y\n\n    return z.squeeze(axis=-1)\n</code></pre>"},{"location":"api/utilities/#pyzaplineplus.core.PyZaplinePlus.nt_normcol","title":"<code>nt_normcol(x, w=None)</code>","text":"<p>Normalize each column so its weighted mean square is 1.</p> <p>Parameters:     x (np.ndarray): Data array (time x channels x trials).     w (np.ndarray): Optional weights array with same dimensions as x or (time x 1 x trials).</p> <p>Returns:     y (np.ndarray): Normalized data array.     norms (np.ndarray): Vector of norms used for normalization.</p> Source code in <code>pyzaplineplus/core.py</code> <pre><code>def nt_normcol(self, x, w=None):\n    \"\"\"\n    Normalize each column so its weighted mean square is 1.\n\n    Parameters:\n        x (np.ndarray): Data array (time x channels x trials).\n        w (np.ndarray): Optional weights array with same dimensions as x or (time x 1 x trials).\n\n    Returns:\n        y (np.ndarray): Normalized data array.\n        norms (np.ndarray): Vector of norms used for normalization.\n    \"\"\"\n    if x.size == 0:\n        raise ValueError('Empty x')\n\n    if isinstance(x, list):\n        raise NotImplementedError('Weights not supported for list inputs')\n\n    if x.ndim == 4:\n        # Apply normcol to each \"book\" (4th dimension)\n        m, n, o, p = x.shape\n        y = np.zeros_like(x)\n        N = np.zeros(n)\n        for k in range(p):\n            y[:, :, :, k], NN = self.nt_normcol(x[:, :, :, k])\n            N += NN ** 2\n        return y, np.sqrt(N)\n\n    if x.ndim == 3:\n        # Unfold data to 2D\n        m, n, o = x.shape\n        x_unfolded = x.reshape(m * o, n)\n        if w is None:\n            y_unfolded, N = self.nt_normcol(x_unfolded)\n        else:\n            if w.shape[0] != m:\n                raise ValueError('Weight matrix should have same number of time samples as data')\n            if w.ndim == 2 and w.shape[1] == 1:\n                w = np.tile(w, (1, n, o))\n            w_unfolded = w.reshape(m * o, n)\n            y_unfolded, N = self.nt_normcol(x_unfolded, w_unfolded)\n        y = y_unfolded.reshape(m, n, o)\n\n        norms = np.sqrt(N)\n        return y, norms\n\n    elif x.ndim == 2:\n        # 2D data\n        m, n = x.shape\n        if w is None:\n            # No weight\n            N = np.sum(x ** 2, axis=0) / m\n            N_inv_sqrt = np.where(N == 0, 0, 1.0 / np.sqrt(N))\n            y = x * N_inv_sqrt\n        else:\n            if w.shape[0] != x.shape[0]:\n                raise ValueError('Weight matrix should have same number of time samples as data')\n            if w.ndim == 1 or (w.ndim == 2 and w.shape[1] == 1):\n                w = np.tile(w.reshape(-1, 1), (1, n))\n            if w.shape != x.shape:\n                raise ValueError('Weight should have same shape as data')\n            sum_w = np.sum(w, axis=0)\n            N = np.sum((x ** 2) * w, axis=0) / (sum_w + np.finfo(float).eps)\n            N_inv_sqrt = np.where(N == 0, 0, 1.0 / np.sqrt(N))\n            y = x * N_inv_sqrt\n        norms = np.sqrt(N)\n        return y, norms\n    else:\n        raise ValueError('Input data must be 2D or 3D')\n</code></pre>"},{"location":"api/utilities/#pyzaplineplus.core.PyZaplinePlus.nt_pca","title":"<code>nt_pca(x, shifts=None, nkeep=None, threshold=0, w=None)</code>","text":"<p>Apply PCA with time shifts and retain a specified number of components.</p> <p>Parameters: - x: data matrix (n_samples, n_channels) or list of arrays for cell-like data - shifts: array of shifts to apply (default: [0]) - nkeep: number of components to keep (default: all) - threshold: discard PCs with eigenvalues below this (default: 0) - w: weights (optional)     - If x is numeric: w can be 1D (n_samples,) or 2D (n_samples, n_channels)     - If x is a list: w should be a list of arrays matching x's structure</p> <p>Returns: - z: principal components     - If x is numeric: numpy.ndarray of shape (numel(idx), PCs, trials)     - If x is a list: list of numpy.ndarrays, each of shape (numel(idx), PCs) - idx: indices of x that map to z</p> Source code in <code>pyzaplineplus/core.py</code> <pre><code>def nt_pca(self, x, shifts=None, nkeep=None, threshold=0, w=None):\n    \"\"\"\n    Apply PCA with time shifts and retain a specified number of components.\n\n    Parameters:\n    - x: data matrix (n_samples, n_channels) or list of arrays for cell-like data\n    - shifts: array of shifts to apply (default: [0])\n    - nkeep: number of components to keep (default: all)\n    - threshold: discard PCs with eigenvalues below this (default: 0)\n    - w: weights (optional)\n        - If x is numeric: w can be 1D (n_samples,) or 2D (n_samples, n_channels)\n        - If x is a list: w should be a list of arrays matching x's structure\n\n    Returns:\n    - z: principal components\n        - If x is numeric: numpy.ndarray of shape (numel(idx), PCs, trials)\n        - If x is a list: list of numpy.ndarrays, each of shape (numel(idx), PCs)\n    - idx: indices of x that map to z\n    \"\"\"\n\n    # Ensure shifts is a numpy array\n    if shifts is None:\n        shifts = np.array([0])\n    else:\n        shifts = np.array(shifts).flatten()\n        if len(shifts) == 0:\n            shifts = np.array([0])\n    if np.any(shifts &lt; 0):\n        raise ValueError(\"All shifts must be non-negative.\")\n\n    # Adjust shifts to make them non-negative\n    min_shift: int = int(np.min(shifts))\n    offset = max(0, -min_shift)\n    shifts = shifts + offset\n    idx = offset + np.arange(x.shape[0] - max(shifts))  # x[idx] maps to z\n    # Determine if x is numeric or list (cell-like)\n    if isinstance(x, list):\n        o = len(x)\n        if o == 0:\n            raise ValueError(\"Input list 'x' is empty.\")\n        m, n = x[0].shape\n        if w is not None and not isinstance(w, list):\n            raise ValueError(\"Weights 'w' must be a list when 'x' is a list.\")\n        tw = 0\n        # Compute covariance\n        c, tw = self.nt_cov(x, shifts, w)\n    elif isinstance(x, np.ndarray):\n        if x.ndim not in [2, 3]:\n            raise ValueError(\"Input 'x' must be a 2D or 3D numpy.ndarray or a list of 2D arrays.\")\n        m, n = x.shape[:2]\n        o = x.shape[2] if x.ndim == 3 else 1\n        c, tw = self.nt_cov(x, shifts, w)\n    else:\n        raise TypeError(\"Input 'x' must be a numpy.ndarray or a list of numpy.ndarrays.\")\n\n    # Perform PCA\n    topcs, evs = self.nt_pcarot(c, nkeep, threshold)\n\n    # Apply PCA matrix to time-shifted data\n    if isinstance(x, list):\n        z = []\n        for k in range(o):\n            shifted = self.nt_multishift(x[k], shifts)  # Shape: (numel(idx), n * nshifts)\n            # Project onto PCA components\n            z_k = np.dot(shifted, topcs)\n            z.append(z_k)\n    elif isinstance(x, np.ndarray):\n        if x.ndim == 2:\n            shifted = self.nt_multishift(x, shifts)  # Shape: (numel(idx), n * nshifts)\n            z = shifted @ topcs # Shape: (numel(idx), PCs)\n        elif x.ndim == 3:\n            z = np.zeros((len(idx), topcs.shape[1], o))\n            for k in range(o):\n                shifted = self.nt_multishift(x[:, :, k], shifts)  # Shape: (numel(idx), n * nshifts)\n                shifted = shifted.reshape(-1, 1)  # Shape becomes (18000, 1)\n                z[:, :, k] = np.dot(shifted, topcs)  # Shape: (numel(idx), PCs)\n    else:\n        # This case should have been handled earlier\n        z = None\n\n    return z, idx\n</code></pre>"},{"location":"api/utilities/#pyzaplineplus.core.PyZaplinePlus.nt_pcarot","title":"<code>nt_pcarot(cov, nkeep=None, threshold=None, N=None)</code>","text":"<p>Calculate PCA rotation matrix from covariance matrix.</p> <p>Parameters: - cov (numpy.ndarray): Covariance matrix (symmetric, positive semi-definite). - nkeep (int, optional): Number of principal components to keep. - threshold (float, optional): Discard components with eigenvalues below this fraction of the largest eigenvalue. - N (int, optional): Number of top eigenvalues and eigenvectors to compute.</p> <p>Returns: - topcs (numpy.ndarray): PCA rotation matrix (eigenvectors), shape (n_features, n_components). - eigenvalues (numpy.ndarray): PCA eigenvalues, shape (n_components,).</p> Source code in <code>pyzaplineplus/core.py</code> <pre><code>def nt_pcarot(self, cov, nkeep=None, threshold=None, N=None):\n    \"\"\"\n    Calculate PCA rotation matrix from covariance matrix.\n\n    Parameters:\n    - cov (numpy.ndarray): Covariance matrix (symmetric, positive semi-definite).\n    - nkeep (int, optional): Number of principal components to keep.\n    - threshold (float, optional): Discard components with eigenvalues below this fraction of the largest eigenvalue.\n    - N (int, optional): Number of top eigenvalues and eigenvectors to compute.\n\n    Returns:\n    - topcs (numpy.ndarray): PCA rotation matrix (eigenvectors), shape (n_features, n_components).\n    - eigenvalues (numpy.ndarray): PCA eigenvalues, shape (n_components,).\n    \"\"\"\n    from scipy.sparse.linalg import eigsh\n    from scipy.linalg import eigh\n\n    # Validate covariance matrix\n    if not isinstance(cov, np.ndarray):\n        raise TypeError(\"Covariance matrix 'cov' must be a numpy.ndarray.\")\n    if cov.ndim != 2 or cov.shape[0] != cov.shape[1]:\n        raise ValueError(\"Covariance matrix 'cov' must be a square (2D) array.\")\n\n    n_features = cov.shape[0]\n\n    # Compute eigenvalues and eigenvectors\n    if N is not None:\n        if not isinstance(N, int) or N &lt;= 0:\n            raise ValueError(\"'N' must be a positive integer.\")\n        N = min(N, n_features)\n        eigenvalues_all, eigenvectors_all = eigsh(cov, k=N, which='LM')  # 'LM' selects largest magnitude eigenvalues\n        # Ensure real parts\n        eigenvalues_all = np.real(eigenvalues_all)\n        eigenvectors_all = np.real(eigenvectors_all)\n    else:\n        eigenvalues_all, eigenvectors_all = eigh(cov)\n        # Ensure real parts\n        eigenvalues_all = np.real(eigenvalues_all)\n        eigenvectors_all = np.real(eigenvectors_all)\n        # Reverse to descending order\n        eigenvalues_all = eigenvalues_all[::-1]\n        eigenvectors_all = eigenvectors_all[:, ::-1]\n        # Covariance is positive semidefinite; keep as-is for true MATLAB parity\n        # positive_idx = eigenvalues_all &gt; 0\n        # eigenvalues_all = eigenvalues_all[positive_idx]\n        # eigenvectors_all = eigenvectors_all[:, positive_idx]\n\n    # Define a small tolerance to handle numerical precision issues (optional)\n\n\n    # Select top N eigenvalues and eigenvectors if N is specified and not already done\n    if N is None:\n        eigenvalues = eigenvalues_all\n        eigenvectors = eigenvectors_all\n    else:\n        eigenvalues = eigenvalues_all\n        eigenvectors = eigenvectors_all\n\n    # Apply threshold\n    if threshold is not None:\n        if eigenvalues[0] == 0:\n            raise ValueError(\"The largest eigenvalue is zero; cannot apply threshold.\")\n        valid_indices = np.where(eigenvalues / eigenvalues[0] &gt; threshold)[0]\n        eigenvalues = eigenvalues[valid_indices]\n        eigenvectors = eigenvectors[:, valid_indices]\n\n    # Apply nkeep\n    if nkeep is not None:\n        if not isinstance(nkeep, int) or nkeep &lt;= 0:\n            raise ValueError(\"'nkeep' must be a positive integer.\")\n        nkeep = min(nkeep, eigenvectors.shape[1])\n        eigenvalues = eigenvalues[:nkeep]\n        eigenvectors = eigenvectors[:, :nkeep]\n\n    topcs = eigenvectors\n    return topcs, eigenvalues\n</code></pre>"},{"location":"api/utilities/#pyzaplineplus.core.PyZaplinePlus.nt_regcov","title":"<code>nt_regcov(cxy, cyy, keep=None, threshold=0)</code>","text":"<p>Compute regression matrix from cross-covariance matrices.</p> <p>Parameters:     cxy (np.ndarray): Cross-covariance matrix between data and regressor.     cyy (np.ndarray): Covariance matrix of regressor.     keep (int): Number of regressor PCs to keep (default: all).     threshold (float): Eigenvalue threshold for discarding regressor PCs (default: 0).</p> <p>Returns:     r (np.ndarray): Regression matrix to apply to regressor to best model data.</p> Source code in <code>pyzaplineplus/core.py</code> <pre><code>def nt_regcov(self, cxy, cyy, keep=None, threshold=0):\n    \"\"\"\n    Compute regression matrix from cross-covariance matrices.\n\n    Parameters:\n        cxy (np.ndarray): Cross-covariance matrix between data and regressor.\n        cyy (np.ndarray): Covariance matrix of regressor.\n        keep (int): Number of regressor PCs to keep (default: all).\n        threshold (float): Eigenvalue threshold for discarding regressor PCs (default: 0).\n\n    Returns:\n        r (np.ndarray): Regression matrix to apply to regressor to best model data.\n    \"\"\"\n    # PCA of regressor covariance matrix\n    topcs,eigenvalues = self.nt_pcarot(cyy)\n\n\n    # Discard negligible regressor PCs\n    if keep is not None:\n        # Keep at most `keep` components, not more\n        keep = min(keep, topcs.shape[1])\n        topcs = topcs[:, :keep]\n        eigenvalues = eigenvalues[:keep]\n\n    # if threshold is not None and threshold &gt; 0:\n    #     idx_thresh = np.where(eigenvalues / np.max(eigenvalues) &gt; threshold)[0]\n    #     topcs = topcs[idx_thresh]\n    #     eigenvalues = eigenvalues[idx_thresh]\n    topcs, eigenvalues= self.normalize_topcs(eigenvalues, topcs, threshold)\n\n    # Cross-covariance between data and regressor PCs\n    cxy = cxy.T  # Transpose cxy to match dimensions\n    r = topcs.T @ cxy\n\n    # Projection matrix from regressor PCs\n    r = self.nt_vecmult(r,1 / eigenvalues)\n\n    # Projection matrix from regressors\n    r = topcs @ r\n\n    return r\n</code></pre>"},{"location":"api/utilities/#pyzaplineplus.core.PyZaplinePlus.nt_smooth","title":"<code>nt_smooth(x, T, n_iterations=1, nodelayflag=False)</code>","text":"<p>Smooth the data by convolution with a square window.</p> <p>Parameters: x (numpy.ndarray): The input data to smooth. Shape: (samples, channels) or (samples, channels, ...) T (float): The window size (can be fractional). n_iterations (int): Number of iterations of smoothing (default is 1). nodelayflag (bool): If True, compensate for delay introduced by filtering.</p> <p>Returns: numpy.ndarray: Smoothed data with the same shape as input.</p> Source code in <code>pyzaplineplus/core.py</code> <pre><code>def nt_smooth(self, x, T, n_iterations=1, nodelayflag=False):\n    \"\"\"\n    Smooth the data by convolution with a square window.\n\n    Parameters:\n    x (numpy.ndarray): The input data to smooth. Shape: (samples, channels) or (samples, channels, ...)\n    T (float): The window size (can be fractional).\n    n_iterations (int): Number of iterations of smoothing (default is 1).\n    nodelayflag (bool): If True, compensate for delay introduced by filtering.\n\n    Returns:\n    numpy.ndarray: Smoothed data with the same shape as input.\n    \"\"\"\n    from scipy.signal import lfilter\n\n    # Ensure x is at least 2D\n    if x.ndim &lt; 2:\n        x = x[:, np.newaxis]\n\n    # Split T into integer and fractional parts\n    integ = int(np.floor(T))\n    frac = T - integ\n\n    # If the window size exceeds data length, replace data with mean\n    if integ &gt;= x.shape[0]:\n        x = np.tile(np.mean(x, axis=0), (x.shape[0], 1))\n        return x\n\n    # Remove onset step (similar to MATLAB code)\n    mn = np.mean(x[:integ + 1, :], axis=0)\n    x = x - mn\n\n    if n_iterations == 1 and frac == 0:\n        # Faster convolution using cumulative sum (similar to MATLAB)\n        cumsum = np.cumsum(x, axis=0)\n        x[integ:, :] = (cumsum[integ:, :] - cumsum[:-integ, :]) / T\n    else:\n        # Construct the initial filter kernel B\n        B = np.concatenate((np.ones(integ), [frac])) / T\n\n        # Iteratively convolve B with [ones(integ), frac] / T for n_iterations-1 times\n        for _ in range(n_iterations - 1):\n            B = np.convolve(B, np.concatenate((np.ones(integ), [frac]))) / T\n\n        # Apply the filter using causal filtering (similar to MATLAB's filter)\n        # For multi-dimensional data, apply filter along the first axis (samples)\n        for channel in range(x.shape[1]):\n            x[:, channel] = lfilter(B, 1, x[:, channel])\n\n    # Restore the mean value that was subtracted earlier\n    x = x + mn\n\n    # Delay compensation if nodelayflag is set to True\n    if nodelayflag:\n        shift = int(round(T / 2 * n_iterations))\n        if shift &gt; 0:\n            # Shift the data forward and pad the end with zeros\n            padding = np.zeros((shift, x.shape[1]))\n            x = np.vstack((x[shift:, :], padding))\n        else:\n            pass  # No shift needed\n\n    return x\n</code></pre>"},{"location":"api/utilities/#pyzaplineplus.core.PyZaplinePlus.nt_spect_plot","title":"<code>nt_spect_plot(data, nfft, fs, return_data=False)</code>","text":"<p>Compute and optionally plot the power spectrum of the data.</p> Source code in <code>pyzaplineplus/core.py</code> <pre><code>def nt_spect_plot(self, data, nfft, fs, return_data=False):\n    \"\"\"\n    Compute and optionally plot the power spectrum of the data.\n    \"\"\"\n    f, pxx = _welch_hamming_periodic(data, fs=fs, nperseg=nfft, axis=0)\n    if return_data:\n        return pxx, f\n    else:\n        plt.figure()\n        plt.semilogy(f, np.mean(pxx, axis=1))\n        plt.xlabel('Frequency (Hz)')\n        plt.ylabel('Power Spectral Density')\n        plt.title('Power Spectrum')\n        plt.grid(True)\n        plt.show()\n</code></pre>"},{"location":"api/utilities/#pyzaplineplus.core.PyZaplinePlus.nt_tsr","title":"<code>nt_tsr(x, ref, shifts=None, wx=None, wref=None, keep=None, thresh=1e-20)</code>","text":"<p>Perform time-shift regression (TSPCA) to denoise data.</p> <p>Parameters:     x (np.ndarray): Data to denoise (time x channels x trials).     ref (np.ndarray): Reference data (time x channels x trials).     shifts (np.ndarray): Array of shifts to apply to ref (default: [0]).     wx (np.ndarray): Weights to apply to x (time x 1 x trials).     wref (np.ndarray): Weights to apply to ref (time x 1 x trials).     keep (int): Number of shifted-ref PCs to retain (default: all).     thresh (float): Threshold to ignore small shifted-ref PCs (default: 1e-20).</p> <p>Returns:     y (np.ndarray): Denoised data.     idx (np.ndarray): Indices where x(idx) is aligned with y.     w (np.ndarray): Weights applied by tsr.</p> Source code in <code>pyzaplineplus/core.py</code> <pre><code>def nt_tsr(self, x, ref, shifts=None, wx=None, wref=None, keep=None, thresh=1e-20):\n    \"\"\"\n    Perform time-shift regression (TSPCA) to denoise data.\n\n    Parameters:\n        x (np.ndarray): Data to denoise (time x channels x trials).\n        ref (np.ndarray): Reference data (time x channels x trials).\n        shifts (np.ndarray): Array of shifts to apply to ref (default: [0]).\n        wx (np.ndarray): Weights to apply to x (time x 1 x trials).\n        wref (np.ndarray): Weights to apply to ref (time x 1 x trials).\n        keep (int): Number of shifted-ref PCs to retain (default: all).\n        thresh (float): Threshold to ignore small shifted-ref PCs (default: 1e-20).\n\n    Returns:\n        y (np.ndarray): Denoised data.\n        idx (np.ndarray): Indices where x(idx) is aligned with y.\n        w (np.ndarray): Weights applied by tsr.\n    \"\"\"\n    # Handle default arguments\n    if shifts is None:\n        shifts = np.array([0])\n    else:\n        shifts = np.asarray(shifts)\n\n    # x = np.atleast_3d(x)  # Shape: (time, channels, trials) or (time, channels, 1)\n    # ref = np.atleast_3d(ref)\n\n    if wx is not None and wx.ndim == 2:\n        wx = wx[:, np.newaxis, :]\n        wx = np.atleast_3d(wx)\n    if wref is not None and wref.ndim == 2:\n        wref = wref[:, np.newaxis, :]\n        wref = np.atleast_3d(wref)\n\n    # Ensure x and ref are at least 3D\n\n    # Check argument values for sanity\n    if x.shape[0] != ref.shape[0]:\n        raise ValueError('x and ref should have the same number of time samples')\n    if x.ndim&gt;=3:\n        if x.shape[2] != ref.shape[2]:\n            raise ValueError('x and ref should have the same number of trials')\n    if wx is not None and (x.shape[0] != wx.shape[0] or x.shape[2] != wx.shape[2]):\n        raise ValueError('x and wx should have matching dimensions')\n    if wref is not None and (ref.shape[0] != wref.shape[0] or ref.shape[2] != wref.shape[2]):\n        raise ValueError('ref and wref should have matching dimensions')\n    if max(shifts) - min(0, min(shifts)) &gt;= x.shape[0]:\n        raise ValueError('x has too few samples to support the given shifts')\n    if wx is not None and wx.shape[1] != 1:\n        raise ValueError('wx should have shape (time, 1, trials)')\n    if wref is not None and wref.shape[1] != 1:\n        raise ValueError('wref should have shape (time, 1, trials)')\n    if wx is not None and np.sum(wx) == 0:\n        raise ValueError('weights on x are all zero!')\n    if wref is not None and np.sum(wref) == 0:\n        raise ValueError('weights on ref are all zero!')\n    if shifts.size &gt; 1000:\n        raise ValueError(f'Number of shifts ({shifts.size}) is too large (if OK, adjust the code to allow it)')\n\n    # Adjust x and ref to ensure that shifts are non-negative\n    offset1 = max(0, -min(shifts))\n    idx = np.arange(offset1, x.shape[0])\n    x = x[idx, :]  # Truncate x\n    if wx is not None:\n        wx = wx[idx, :]\n    shifts = shifts + offset1  # Shifts are now non-negative\n\n    # Adjust size of x\n    offset2 = max(0, max(shifts))\n    idx_ref = np.arange(0, ref.shape[0] - offset2)\n    x = x[:len(idx_ref), :]  # Part of x that overlaps with time-shifted refs\n    if wx is not None:\n        wx = wx[:len(idx_ref), :]\n    if x.ndim == 3:\n        mx, nx, ox = x.shape\n        mref, nref, oref = ref.shape\n    elif x.ndim == 2:\n        mx, nx = x.shape\n        mref, nref = ref.shape\n    else:\n        raise ValueError('x should be 2D or 3D')\n\n\n    # Consolidate weights into a single weight matrix\n    w = np.zeros((mx, 1))\n    if wx is None and wref is None:\n        w[:] = 1\n    elif wref is None:\n        w = wx\n    elif wx is None:\n        wr = wref[:, :]\n        wr_shifted = self.nt_multishift(wr, shifts)\n        w[:, :] = np.min(wr_shifted, axis=1, keepdims=True)\n    else:\n\n        wr = wref[:, :]\n        wr_shifted = self.nt_multishift(wr, shifts)\n        w_min = np.min(wr_shifted, axis=1, keepdims=True)\n        w[:, :] = np.minimum(w_min, wx[:w_min.shape[0], :])\n    wx = w\n    wref = np.zeros((mref, 1))\n    wref[idx, :] = w\n\n    # Remove weighted means\n    x_demeaned, _ = self.nt_demean(x, wx)\n    ref_demeaned, _ = self.nt_demean(ref, wref)\n\n    # Equalize power of ref channels, then equalize power of ref PCs\n    ref_normalized, _ = self.nt_normcol(ref_demeaned, wref)\n    ref_pca, _ = self.nt_pca(ref_normalized, threshold=1e-6)\n    ref_normalized_pca, _ = self.nt_normcol(ref_pca, wref)\n    ref = ref_normalized_pca\n\n    # Covariances and cross-covariance with time-shifted refs\n    cref, twcref = self.nt_cov(ref, shifts, wref)\n    cxref, twcxref = self.nt_xcov(x, ref, shifts, wx)\n\n    # Regression matrix of x on time-shifted refs\n    r = self.nt_regcov(cxref / twcxref, cref / twcref, keep=keep, threshold=thresh)\n\n    # Clean x by removing regression on time-shifted refs\n    y = np.zeros_like(x)\n\n    ref_shifted = self.nt_multishift(ref[:, :], shifts)\n    z = ref_shifted @ r\n    y = x[:z.shape[0], :] - z\n\n    y_demeaned, _ = self.nt_demean(y, wx)  # Multishift(ref) is not necessarily zero mean\n\n    # idx for alignment\n    idx_output = np.arange(offset1, offset1 + y.shape[0])\n    w = wref\n\n    # Return outputs\n    return y_demeaned, idx_output, w\n</code></pre>"},{"location":"api/utilities/#pyzaplineplus.core.PyZaplinePlus.nt_unfold","title":"<code>nt_unfold(x)</code>","text":"<p>Converts a 3D matrix (time x channel x trial) into a 2D matrix (time*trial x channel).</p> <p>Parameters:     x (np.ndarray): Input data. Can be 2D or 3D.</p> <p>Returns:     y (np.ndarray): Unfolded data.</p> Source code in <code>pyzaplineplus/core.py</code> <pre><code>def nt_unfold(self,x):\n    \"\"\"\n    Converts a 3D matrix (time x channel x trial) into a 2D matrix (time*trial x channel).\n\n    Parameters:\n        x (np.ndarray): Input data. Can be 2D or 3D.\n\n    Returns:\n        y (np.ndarray): Unfolded data.\n    \"\"\"\n    if x.size == 0:\n        return np.array([])\n    else:\n        if x.ndim == 3:\n            m, n, p = x.shape\n            if p &gt; 1:\n                y = np.reshape(np.transpose(x, (0, 2, 1)), (m * p, n))\n            else:\n                y = x\n        else:\n            y = x\n    return y\n</code></pre>"},{"location":"api/utilities/#pyzaplineplus.core.PyZaplinePlus.nt_vecmult","title":"<code>nt_vecmult(xx, ww)</code>","text":"<p>Multiply each row of 'xx' by the corresponding weight in 'ww'.</p> <p>Parameters:     xx (np.ndarray): Data array (time_shifted x channels).     ww (np.ndarray): Weights array (time_shifted x 1).</p> <p>Returns:     weighted_xx (np.ndarray): Weighted data (time_shifted x channels).</p> Source code in <code>pyzaplineplus/core.py</code> <pre><code>def nt_vecmult(self, xx: np.ndarray, ww: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Multiply each row of 'xx' by the corresponding weight in 'ww'.\n\n    Parameters:\n        xx (np.ndarray): Data array (time_shifted x channels).\n        ww (np.ndarray): Weights array (time_shifted x 1).\n\n    Returns:\n        weighted_xx (np.ndarray): Weighted data (time_shifted x channels).\n    \"\"\"\n    # Ensure that 'ww' is a column vector\n    if ww.ndim == 1:\n        ww = ww[:, np.newaxis]\n    elif ww.ndim == 2 and ww.shape[1] != 1:\n        raise ValueError(\"Weights 'ww' must have a single column or be a 1D array.\")\n\n    # Element-wise multiplication with broadcasting\n    weighted_xx = xx * ww  # Shape: (time_shifted x channels)\n    return weighted_xx\n</code></pre>"},{"location":"api/utilities/#pyzaplineplus.core.PyZaplinePlus.nt_xcov","title":"<code>nt_xcov(x, y, shifts=None, w=None)</code>","text":"<p>Compute the cross-covariance of x and time-shifted y.</p> <p>Parameters:     x (Union[np.ndarray, List[np.ndarray]]): Data array x (time x channels x trials) or list of 2D arrays.     y (Union[np.ndarray, List[np.ndarray]]): Data array y (time x channels x trials) or list of 2D arrays.     shifts (Optional[Union[List[int], np.ndarray]]): Array of non-negative integer time shifts (default: [0]).     w (Optional[Union[np.ndarray, List[np.ndarray]]]): Optional weights array (time x 1 x trials) or (time x channels x trials).</p> <p>Returns:     Tuple[np.ndarray, float]: Cross-covariance matrix and total weight.         - c: cross-covariance matrix.         - tw: total weight.</p> Source code in <code>pyzaplineplus/core.py</code> <pre><code>def nt_xcov(\n    self,\n    x: Union[np.ndarray, List[np.ndarray]],\n    y: Union[np.ndarray, List[np.ndarray]],\n    shifts: Optional[Union[List[int], np.ndarray]] = None,\n    w: Optional[Union[np.ndarray, List[np.ndarray]]] = None\n) -&gt; Tuple[np.ndarray, float]:\n    \"\"\"\n    Compute the cross-covariance of x and time-shifted y.\n\n    Parameters:\n        x (Union[np.ndarray, List[np.ndarray]]): Data array x (time x channels x trials) or list of 2D arrays.\n        y (Union[np.ndarray, List[np.ndarray]]): Data array y (time x channels x trials) or list of 2D arrays.\n        shifts (Optional[Union[List[int], np.ndarray]]): Array of non-negative integer time shifts (default: [0]).\n        w (Optional[Union[np.ndarray, List[np.ndarray]]]): Optional weights array (time x 1 x trials) or (time x channels x trials).\n\n    Returns:\n        Tuple[np.ndarray, float]: Cross-covariance matrix and total weight.\n            - c: cross-covariance matrix.\n            - tw: total weight.\n    \"\"\"\n    # If numpy arrays are 2D, expand to 3D for consistent processing\n    if isinstance(x, np.ndarray) and isinstance(y, np.ndarray):\n        if x.ndim == 2 and y.ndim == 2:\n            x = x[:, :, np.newaxis]\n            y = y[:, :, np.newaxis]\n            if w is not None:\n                if not isinstance(w, np.ndarray):\n                    raise TypeError(\"Weights `w` must be a numpy.ndarray when `x`/`y` are numpy.ndarray types.\")\n                if w.ndim == 2:\n                    w = w[:, :, np.newaxis]\n    if shifts is None:\n        shifts = np.array([0])\n    else:\n        shifts = np.asarray(shifts).flatten()\n    shifts = cast(np.ndarray, shifts)\n\n    if np.any(shifts &lt; 0):\n        raise ValueError('Shifts must be non-negative integers')\n\n    # Validate dimensions\n    if isinstance(x, list) and isinstance(y, list):\n        if len(x) != len(y):\n            raise ValueError(\"Lists `x` and `y` must have the same length.\")\n        if w is not None and not isinstance(w, list):\n            raise ValueError(\"Weights `w` must be a list if `x` and `y` are lists.\")\n    elif isinstance(x, np.ndarray) and isinstance(y, np.ndarray):\n        if x.ndim != y.ndim:\n            raise ValueError(\"Arrays `x` and `y` must have the same number of dimensions.\")\n        if x.shape[0] != y.shape[0]:\n            raise ValueError(\"`x` and `y` must have the same number of time samples.\")\n        if x.ndim &gt; 2:\n            if x.shape[2] != y.shape[2]:\n                raise ValueError(\"`x` and `y` must have the same number of trials.\")\n            if w is not None:\n                if not isinstance(w, np.ndarray):\n                    raise TypeError(\"Weights `w` must be a numpy.ndarray when `x`/`y` are numpy.ndarray types.\")\n                if x.shape[2] != w.shape[2]:\n                    raise ValueError(\"`x` and `w` must have the same number of trials.\")\n    else:\n        raise TypeError(\"`x` and `y` must both be either lists or numpy.ndarray types.\")\n\n    nshifts = shifts.size\n\n    # Initialize covariance matrix and total weight\n    c = None\n    tw = 0.0\n\n    # Handle list inputs (equivalent to cell arrays in MATLAB)\n    if isinstance(x, list):\n        if w is not None and not isinstance(w, list):\n            raise ValueError(\"Weights `w` must be a list if `x` is a list (cell array).\")\n\n        o = len(x)  # Number of cells/trials\n        if o == 0:\n            raise ValueError(\"Input list `x` is empty.\")\n\n        # Determine number of channels from the first cell\n        # shapes of first elements inferred below when needed\n        if x[0].ndim == 1:\n            n_channels_x = 1\n        elif x[0].ndim == 2:\n            n_channels_x = x[0].shape[1]\n        else:\n            raise ValueError(f\"Data in cell 0 of `x` has unsupported number of dimensions: {x[0].ndim}\")\n\n        if y[0].ndim == 1:\n            n_channels_y = 1\n        elif y[0].ndim == 2:\n            n_channels_y = y[0].shape[1]\n        else:\n            raise ValueError(f\"Data in cell 0 of `y` has unsupported number of dimensions: {y[0].ndim}\")\n\n        # Initialize cross-covariance matrix\n        c = np.zeros((n_channels_x, n_channels_y * nshifts), dtype=np.float64)\n\n        for idx in range(o):\n            data_x = x[idx]\n            data_y = y[idx]\n\n            # Validate data dimensions\n            if not isinstance(data_x, np.ndarray) or not isinstance(data_y, np.ndarray):\n                raise TypeError(f\"Elements in lists `x` and `y` must be numpy.ndarray types. Found types: {type(data_x)}, {type(data_y)} at index {idx}.\")\n            if data_x.ndim != 2 or data_y.ndim != 2:\n                raise ValueError(f\"Data in lists `x` and `y` must be 2D arrays. Found dimensions: {data_x.ndim}, {data_y.ndim} at index {idx}.\")\n\n            if data_x.shape[1] != n_channels_x:\n                raise ValueError(f\"All cells in `x` must have {n_channels_x} channels. Found {data_x.shape[1]} at index {idx}.\")\n            if data_y.shape[1] != n_channels_y:\n                raise ValueError(f\"All cells in `y` must have {n_channels_y} channels. Found {data_y.shape[1]} at index {idx}.\")\n\n            # Handle weights\n            if w is not None:\n                weight = w[idx]\n                if not isinstance(weight, np.ndarray):\n                    raise TypeError(f\"Weight for cell {idx} must be a numpy.ndarray.\")\n                if weight.size == 0:\n                    raise ValueError(f\"Weight for cell {idx} is empty.\")\n                if weight.ndim == 1:\n                    weight = weight[:, np.newaxis]  # Shape: (n_samples, 1)\n                elif weight.ndim == 2 and weight.shape[1] == 1:\n                    pass  # Shape is already (n_samples, 1)\n                else:\n                    raise ValueError(f\"Weight for cell {idx} must be 1D or 2D with a single column. Got shape {weight.shape}.\")\n\n            # Apply shifts to y\n            y_shifted = self.nt_multishift(data_y, shifts)  # Shape: (n_samples_shifted, n_channels_y * nshifts)\n\n            # Truncate x to match the shifted y's time dimension\n            if not np.all(shifts == 0):\n                # Apply shifts to x if necessary (though MATLAB does not shift x in cross-covariance)\n                # Assuming x is not shifted, only y is\n                # Thus, truncate x to match the shifted y's number of samples\n                x_truncated = data_x[:y_shifted.shape[0], :]  # Shape: (n_samples_shifted, n_channels_x)\n            else:\n                x_truncated = data_x.copy()\n\n            # Handle weights: multiply x by w\n            if w is not None:\n                # Unfold x and w, multiply, fold back\n                x_weighted = self.nt_fold(\n                    self.nt_vecmult(\n                        self.nt_unfold(x_truncated),\n                        self.nt_unfold(weight[:y_shifted.shape[0], :]),\n                    ),\n                    data_x.shape[0],\n                )[: y_shifted.shape[0], :]  # Ensure matching time dimension\n                x_truncated = x_weighted  # Shape: (n_samples_shifted, n_channels_x)\n\n                # Update total weight\n                if not np.all(shifts == 0):\n                    # Take minimum weight across shifts\n                    ww = self.nt_multishift(weight[:y_shifted.shape[0], :], shifts)  # Shape: (n_samples_shifted * nshifts, 1)\n                    ww_min = np.min(ww, axis=1, keepdims=True)\n                    tw += np.sum(ww_min)\n                else:\n                    tw += np.sum(weight[:y_shifted.shape[0], :])\n            else:\n                tw += x_truncated.shape[0]  # Number of samples\n\n            # Accumulate cross-covariance\n            c += np.dot(x_truncated.T, y_shifted)  # Shape: (n_channels_x, n_channels_y * nshifts)\n\n        # Handle NumPy array inputs\n    elif isinstance(x, np.ndarray) and isinstance(y, np.ndarray):\n        # Validate dimensions\n        if x.ndim != y.ndim:\n            raise ValueError(\"Arrays `x` and `y` must have the same number of dimensions.\")\n        if x.shape[0] != y.shape[0]:\n            raise ValueError(\"`x` and `y` must have the same number of time samples.\")\n        if x.ndim &gt; 2:\n            if x.shape[2] != y.shape[2]:\n                raise ValueError(\"`x` and `y` must have the same number of trials.\")\n            if w is not None:\n                if not isinstance(w, np.ndarray):\n                    raise TypeError(\"Weights `w` must be a numpy.ndarray if `x` and `y` are numpy.ndarray types.\")\n                if w.shape[0] != x.shape[0] or w.shape[2] != x.shape[2]:\n                    raise ValueError(\"`w` must have the same number of time samples and trials as `x`.\")\n\n        mx, nx, ox = x.shape\n        my, ny, oy = y.shape\n\n        # Determine number of channels\n        n_channels_x = nx\n        n_channels_y = ny\n\n        # Initialize cross-covariance matrix\n        c = np.zeros((n_channels_x, n_channels_y * nshifts), dtype=np.float64)\n\n        for trial in range(ox):\n            data_x = x[:, :, trial]  # Shape: (n_samples, n_channels_x)\n            data_y = y[:, :, trial]  # Shape: (n_samples, n_channels_y)\n\n            # Apply shifts to y\n            y_shifted= self.nt_multishift(data_y, shifts)  # Shape: (n_samples_shifted, n_channels_y * nshifts)\n\n            # Truncate x to match the shifted y's time dimension\n            if not np.all(shifts == 0):\n                # Assuming y is shifted, x is not; truncate x accordingly\n                x_truncated = data_x[:y_shifted.shape[0], :]  # Shape: (n_samples_shifted, n_channels_x)\n            else:\n                x_truncated = data_x.copy()\n\n            if w is not None:\n                # Extract weights for this trial and truncate\n                if not isinstance(w, np.ndarray):\n                    raise TypeError(\"Weights `w` must be a numpy.ndarray when `x`/`y` are numpy.ndarray types.\")\n                trial_weight = w[:, :, trial]  # Shape: (n_samples, channels or 1)\n                # if w.ndim == 2:\n                #     # For 3D `x`, weights are 2D (time x trials)\n                #     trial_weight = trial_weight  # Shape: (n_samples, trials)\n                # elif w.ndim == 1:\n                #     # For 1D or 2D `x`, weights are 1D\n                #     trial_weight = trial_weight[:, np.newaxis]  # Shape: (n_samples, 1)\n                # else:\n                #     raise ValueError(f\"Unsupported weight dimensionality: {w.ndim}\")\n\n                # Unfold x and w, multiply, fold back\n                x_weighted = self.nt_fold(\n                    self.nt_vecmult(\n                        self.nt_unfold(x_truncated),\n                        self.nt_unfold(trial_weight[:y_shifted.shape[0], :]),\n                    ),\n                    mx,\n                )[: y_shifted.shape[0], :]  # Ensure matching time dimension\n\n                x_truncated = x_weighted  # Shape: (n_samples_shifted, n_channels_x)\n\n                if not np.all(shifts == 0):\n                    # Take minimum weight across shifts\n                    ww = self.nt_multishift(trial_weight[:y_shifted.shape[0], :], shifts)  # Shape: (n_samples_shifted * nshifts, 1)\n                    ww_min = np.min(ww, axis=1, keepdims=True)\n                    tw += np.sum(ww_min)\n                else:\n                    tw += np.sum(trial_weight[:y_shifted.shape[0], :])\n            else:\n                tw += x_truncated.shape[0]  # Number of samples\n\n            # Accumulate cross-covariance\n            c += np.dot(x_truncated.T, y_shifted)  # Shape: (n_channels_x, n_channels_y * nshifts)\n\n    else:\n        raise TypeError(\"`x` and `y` must both be either lists or numpy.ndarray types.\")\n\n    return c, tw\n</code></pre>"},{"location":"api/utilities/#pyzaplineplus.core._welch_hamming_periodic","title":"<code>_welch_hamming_periodic(x, fs, nperseg, axis=0)</code>","text":"<p>Welch PSD with Hamming window (periodic form) and 50% overlap, matching MATLAB pwelch default used by zapline-plus.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array</code> <p>Input data.</p> required <code>fs</code> <code>float</code> <p>Sampling frequency.</p> required <code>nperseg</code> <code>int</code> <p>Segment length for Welch.</p> required <code>axis</code> <code>int</code> <p>Axis over which the PSD is computed.</p> <code>0</code> <p>Returns:</p> Type Description <code>f, pxx : ndarray</code> <p>Frequency bins and power spectral density.</p> Source code in <code>pyzaplineplus/core.py</code> <pre><code>def _welch_hamming_periodic(x, fs, nperseg, axis=0):\n    \"\"\"\n    Welch PSD with Hamming window (periodic form) and 50% overlap,\n    matching MATLAB pwelch default used by zapline-plus.\n\n    Parameters\n    ----------\n    x : array\n        Input data.\n    fs : float\n        Sampling frequency.\n    nperseg : int\n        Segment length for Welch.\n    axis : int\n        Axis over which the PSD is computed.\n\n    Returns\n    -------\n    f, pxx : ndarray\n        Frequency bins and power spectral density.\n    \"\"\"\n    # periodic Hamming: sym=False\n    win = signal.windows.hamming(nperseg, sym=False)\n    f, pxx = signal.welch(\n        x,\n        fs=fs,\n        window=win,\n        noverlap=nperseg // 2,\n        nperseg=nperseg,\n        axis=axis,\n        detrend=False,\n    )\n    return f, pxx\n</code></pre>"},{"location":"developer-guide/contributing/","title":"Contributing","text":"<p>See the repository\u2019s <code>CONTRIBUTING.md</code> for full guidelines. Quick start:</p> <pre><code>git clone https://github.com/SinaEsmaeili/PyZaplinePlus\ncd PyZaplinePlus\npython -m venv .venv\nsource .venv/bin/activate  # Windows: .venv\\Scripts\\activate\npip install -e \".[dev]\"\npytest -q\n</code></pre>"},{"location":"developer-guide/development/","title":"Development Setup","text":"<p>1) Create a virtual environment and install dev deps:</p> <pre><code>python -m venv .venv\nsource .venv/bin/activate\npip install -e \".[dev]\"\n</code></pre> <p>2) Lint and format:</p> <pre><code>ruff check .\nblack .\n</code></pre> <p>3) Run tests with coverage:</p> <pre><code>pytest --cov=pyzaplineplus --cov-report=term-missing\n</code></pre>"},{"location":"developer-guide/release/","title":"Release Process","text":"<p>See the root <code>RELEASING.md</code> for the authoritative steps.</p> <p>Summary: - Bump version in <code>pyproject.toml</code> and <code>pyzaplineplus/_version.py</code> - Update <code>CHANGELOG.md</code> - Run tests + coverage and build docs - Build distribution and check with Twine - Tag a GitHub release (vX.Y.Z) to trigger publish</p>"},{"location":"user-guide/examples/","title":"Examples","text":""},{"location":"user-guide/examples/#mne-eegbci-demo","title":"MNE EEGBCI demo","text":"<p>Run the included demo to detect and remove line noise from an EEGBCI segment:</p> <pre><code>python examples/mne_eegbci_demo.py\n</code></pre> <p>This generates a diagnostic figure under <code>figures/zapline_results.png</code>.</p>"},{"location":"user-guide/examples/#synthetic-data","title":"Synthetic data","text":"<pre><code>import numpy as np\nfrom pyzaplineplus import zapline_plus\n\nfs = 500\nt = np.arange(0, 30, 1/fs)\neeg = np.random.randn(t.size, 64) * 10\neeg += 8*np.sin(2*np.pi*50*t)[:, None]\n\nclean, cfg, analytics, plots = zapline_plus(eeg, fs, noisefreqs='line', plotResults=True)\n</code></pre>"},{"location":"user-guide/installation/","title":"Installation","text":""},{"location":"user-guide/installation/#from-pypi-recommended","title":"From PyPI (recommended)","text":"<pre><code>pip install pyzaplineplus\n</code></pre>"},{"location":"user-guide/installation/#from-source","title":"From source","text":"<pre><code>git clone https://github.com/snesmaeili/PyZapline_plus.git\ncd PyZapline_plus\npip install -e \".[dev]\"\n</code></pre>"},{"location":"user-guide/installation/#optional-extras","title":"Optional extras","text":"<pre><code># MNE integration\npip install pyzaplineplus[mne]\n\n# Development\npip install pyzaplineplus[dev]\n</code></pre>"},{"location":"user-guide/installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.8+</li> <li>NumPy, SciPy, scikit-learn, Matplotlib</li> </ul>"},{"location":"user-guide/mne-integration/","title":"MNE Integration","text":"<p>PyZaplinePlus provides an optional adapter to clean <code>mne.io.Raw</code> objects directly, preserving channel metadata and annotations.</p>"},{"location":"user-guide/mne-integration/#basic-usage","title":"Basic usage","text":"<pre><code>import mne\nfrom pyzaplineplus import apply_zapline_to_raw\n\nraw = mne.io.read_raw_fif(\"my_raw.fif\", preload=True)\nraw_clean, config, analytics, figs = apply_zapline_to_raw(\n    raw,\n    picks=None,           # default: EEG-only\n    copy=True,            # return a modified copy\n    line_freqs=\"line\",   # auto-detect 50/60 Hz\n    plotResults=False,    # optional plotting\n    adaptiveNremove=True, # MATLAB parity default\n)\n\n# Save if desired\nraw_clean.save(\"my_raw_cleaned.fif\", overwrite=True)\n</code></pre>"},{"location":"user-guide/mne-integration/#key-options","title":"Key options","text":"<ul> <li>picks: list of channel names to process. Defaults to EEG-only (<code>mne.pick_types(eeg=True, ...)</code>).</li> <li>copy: True to return a modified copy; False to operate in-place.</li> <li>line_freqs: sequence of frequencies (e.g. <code>[50, 100]</code>), \"line\" for 50/60 autodetect, or <code>None</code> to rely on automatic detection within bounds.</li> <li>dtype: the adapter preserves the underlying <code>Raw</code> dtype (e.g. float32) when writing back.</li> </ul>"},{"location":"user-guide/mne-integration/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>preload: <code>raw</code> must be <code>preload=True</code>.</li> <li>channel types: default <code>picks=None</code> processes EEG channels only. Pass explicit names to include other types.</li> <li>events/annotations: these are preserved; only the selected channels\u2019 sample data are modified.</li> <li>no <code>mne</code> installed: install extra dependencies with <code>pip install pyzaplineplus[mne]</code>.</li> </ul>"},{"location":"user-guide/quickstart/","title":"Quick Start","text":"<pre><code>import numpy as np\nfrom pyzaplineplus import zapline_plus\n\n# Simulated EEG (time x channels)\nfs = 1000\nt = np.arange(0, 10, 1/fs)\neeg = np.random.randn(t.size, 64) * 10\neeg += 5*np.sin(2*np.pi*50*t)[:, None]\n\nclean, config, analytics, plots = zapline_plus(eeg, fs, plotResults=True)\n</code></pre> <p>See also <code>examples/mne_eegbci_demo.py</code> for an MNE dataset example.</p>"}]}